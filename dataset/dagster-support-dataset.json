[{"question": "Hello everyone, I'm primarily an R developer who has taken an interest in dagster and data engineering. I have a basic understanding of Python and have been following the Dagster tutorials, but I've been getting hung up on establishing an efficient development workflow. Making the switch from using the RStudio IDE to VS Code has been more painful than I anticipated. Even basic things like viewing the contents of a data frame, or jumping inside of a function to test its internals line-by-line are throwing me for a loop. I'm investing more time into learning about VS Code's Python debugger, but I wanted to poll this community to see if there are any other best practices that I should be aware of when developing pipelines with dagster. If there are any other R developers here who made the same transition, then I'd love to connect and learn more about your journey. Thank you in advance!", "replies": ["Not sure if these series of blogs might help you in your transition. <https://dagster.io/blog/python-packages-primer-1|https://dagster.io/blog/python-packages-primer-1>\n<https://calmcode.io/setup/introduction.html|https://calmcode.io/setup/introduction.html>", "Thanks for sharing these. They have been really helpful so far. I'm hoping that they'll add a new section that provides an overview of development/debugging best practices soon!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "You might find the series of blogs on the Dagster website and Calmcode.io helpful in your transition from R to Python and Dagster, as they provide a primer on Python packages and setup.", "dagster_empl_first_target": "Not sure if these series of blogs might help you in your transition. <https://dagster.io/blog/python-packages-primer-1|https://dagster.io/blog/python-packages-primer-1>\n<https://calmcode.io/setup/introduction.html|https://calmcode.io/setup/introduction.html>", "dagster_empl_last_target": "Thanks for sharing these. They have been really helpful so far. I'm hoping that they'll add a new section that provides an overview of development/debugging best practices soon!"}, {"question": "Hi team, I find the \"reading partitioned asset\" from a partitioned job behaves differently from the document mentioned. <https://docs.dagster.io/concepts/ops-jobs-graphs/graphs#loading-an-asset-as-an-input> . It tries to load all partitions instead of a single partition in the job. So I have partitioned asset \"daily_market_feather_path\" defined using decorator  `@graph_asset(partitions_def=weekday_partition_def)`  with this customised partition def `\n```weekday_partition_def = TimeWindowPartitionsDefinition(\n    start=\"2023-05-22\", end_offset=1, fmt=\"%Y-%m-%d\", cron_schedule=\"0 0 * * 1-5\"\n)```\n`\nI have my parititioned job defined as this decorate `@job(partitions_def=weekday_partition_def)` with the same partition def. Within the job, I called `<http://daily_market_feather_path.to|daily_market_feather_path.to>_source_asset()`. When I run this job, I see from the log it tries to load all 28 partitions. Since I don't have all partition fully filled, it failed on some paritition (no file find). Am I doing something wrong here?", "replies": ["Hi Jimmy, would you mind creating a minimal reproduction in code? Something like this:\n\n```weekday_partition_def = TimeWindowPartitionsDefinition(\n    start=\"2023-05-22\", end_offset=1, fmt=\"%Y-%m-%d\", cron_schedule=\"0 0 * * 1-5\"\n)\n\n@asset(partitions_def=weekday_partition_def)\ndef foo():\n    ...\n\n# ... other assets, job defs\n\ndefs = Definitions(\n    assets=[foo, ...]\n    jobs=[...]\n)```\n", "Hi <@U01GTMVMGQH> of course. here you go. I encounter same issue for the following code:\n```weekday_partition_def = TimeWindowPartitionsDefinition(\n    start=\"2023-05-22\", end_offset=1, fmt=\"%Y-%m-%d\", cron_schedule=\"0 0 * * 1-5\"\n)\n\n@asset(partitions_def=weekday_partition_def)\ndef foo():\n    return 1\n\n@op\ndef op1(data):\n    return data + 1\n\n@job(partitions_def=weekday_partition_def)\ndef job1():\n    res = op1(foo.to_source_asset())\n\ndefs = Definitions(\n    assets=[foo],\n    jobs=[job1]\n)```", "Hi Jimmy, In trying to repro I believe I have encountered a more general bug with asset-op interoperability. I will get back to you here when I hear back from some other engineers on our team.", "Hey Jimmy, I was able to replicate this and determined that we already have an issue open: <https://github.com/dagster-io/dagster/issues/13357>\n\nWe\u2019ll try to fix this ASAP"], "is_dagster_empl": [true, false, true, true], "gpt4_replies_target": "The user is experiencing an issue where the \"reading partitioned asset\" from a partitioned job in Dagster is loading all partitions instead of a single one, contrary to the documentation. The team has acknowledged this as a more general bug with asset-op interoperability and confirmed that there is an open issue on this matter. They are working on fixing it.", "dagster_empl_first_target": "Hi Jimmy, would you mind creating a minimal reproduction in code? Something like this:\n\n```weekday_partition_def = TimeWindowPartitionsDefinition(\n    start=\"2023-05-22\", end_offset=1, fmt=\"%Y-%m-%d\", cron_schedule=\"0 0 * * 1-5\"\n)\n\n@asset(partitions_def=weekday_partition_def)\ndef foo():\n    ...\n\n# ... other assets, job defs\n\ndefs = Definitions(\n    assets=[foo, ...]\n    jobs=[...]\n)```\n", "dagster_empl_last_target": "Hey Jimmy, I was able to replicate this and determined that we already have an issue open: <https://github.com/dagster-io/dagster/issues/13357>\n\nWe\u2019ll try to fix this ASAP"}, {"question": "Hello Dagster Community. I am facing an issue with the dagster-airbyte package I have installed and am using in my code. I have spent hours debugging and trying multiple fixes but it just isn't working.  The package is clearly installed on my virtual environmen,  however when I go to use it in my code the package just can't be found. Has anyone else had to face this issue? Any help would be highly appreciated as this is stopping me from running my code. I have checked there is not conflict between the packages either. Here is a screenshot of my code file (it is just test code at the minute):", "replies": ["Hi Divyesh, I don\u2019t think this is likely to be anything specific to `dagster-airbyte`. Your screenshot is showing errors in your editor, but can you provide the exact error you are getting when trying to run code?", "Correct it is showing errors in the editor this is because it is unable to find the dagster-airbyte package. Here is the exact error I am getting when running the code :\n\nError loading repository location airbyte_dagster.py:NameError: name 'define_asset_job' is not defined.\n\nI have fixed this as I was missing the additional import after from 'dagster' package for the define_asset_job function. However my dagster_airbyte package issue has not been solved.", "Are you getting errors regarding `dagster-airbyte` at runtime? Or just in your editor? Those are different environments-- what is shown in your editor is a matter of editor configuration.", "<@U01GTMVMGQH> turns out it is actually an IDE issue. My code is running . The error is still showing in my IDE. WOW. That got me :see_no_evil:", "Thanks for your help :slightly_smiling_face: . I have noted this down for next time."], "is_dagster_empl": [true, false, true, false, false], "gpt4_replies_target": "The user was facing an issue with the dagster-airbyte package not being found in their code, but it turned out to be an IDE issue rather than a problem with the package itself, as the code was running correctly.", "dagster_empl_first_target": "Hi Divyesh, I don\u2019t think this is likely to be anything specific to `dagster-airbyte`. Your screenshot is showing errors in your editor, but can you provide the exact error you are getting when trying to run code?", "dagster_empl_last_target": "Are you getting errors regarding `dagster-airbyte` at runtime? Or just in your editor? Those are different environments-- what is shown in your editor is a matter of editor configuration."}, {"question": "Is there a command line way to set log level when we execute a job like `dagster job execute --log-level info ...`  ?", "replies": ["Hi Bala, There is no \u201cdirect\u201d way-- you have to specify it in the run config like here: <https://docs.dagster.io/concepts/logging/loggers#configuring-built-in-loggers>\n\nAnd then you can do `dagster job execute --config /path/to/config.yaml ...`\n\nIf you think a CLI option would be useful, please open an issue."], "is_dagster_empl": [true], "gpt4_replies_target": "There is no direct command line way to set log level when executing a job, instead you have to specify it in the run config and then execute the job with the config file.", "dagster_empl_first_target": "Hi Bala, There is no \u201cdirect\u201d way-- you have to specify it in the run config like here: <https://docs.dagster.io/concepts/logging/loggers#configuring-built-in-loggers>\n\nAnd then you can do `dagster job execute --config /path/to/config.yaml ...`\n\nIf you think a CLI option would be useful, please open an issue.", "dagster_empl_last_target": "Hi Bala, There is no \u201cdirect\u201d way-- you have to specify it in the run config like here: <https://docs.dagster.io/concepts/logging/loggers#configuring-built-in-loggers>\n\nAnd then you can do `dagster job execute --config /path/to/config.yaml ...`\n\nIf you think a CLI option would be useful, please open an issue."}, {"question": "Hi, I am running a small pymc model inside dagster. PyMC uses fastprogress to print out a progress bar and stats during sampling. This is not captured in the dagster logs. Is there a way to display that in the dagit frontend? under the hood they are using fastprogress", "replies": ["Hi Matthias, we do not support streaming a progress bar from a process atm. While in theory one might be able to use our <https://docs.dagster.io/concepts/logging/python-logging|python logging integration> to forward the writes to dagster, I\u2019m pretty sure this would just result in a ton of different log messages like `#`, `##`, `###`, `####`\u2026, because a progress bar typically overwrites the same line in terminal during execution, but there is no such concept in dagster logs.\n\nIn order to support this I think we would need to provide a de novo API and UI element, and I don\u2019t think we\u2019re going to do that in the near term.\n\nOne alternative might be to use dagster\u2019s python logging integration to write a subset of percentage updates to the log, like \u201c10% complete\u201d, \u201c20% complete\u201d, etc. I don\u2019t know anything about PyMC or fastprogress, but is it possible to configure it to emit such messages?", "Thanks, I was misssing all output. The progressbar is not that important. However in my AWS deployment I seem to have a logging configuration in place that captures more logs. So this is fine for me. Thanks"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Dagster currently does not support streaming a progress bar from a process. However, you can use Dagster's python logging integration to write a subset of percentage updates to the log, like \u201c10% complete\u201d, \u201c20% complete\u201d, etc.", "dagster_empl_first_target": "Hi Matthias, we do not support streaming a progress bar from a process atm. While in theory one might be able to use our <https://docs.dagster.io/concepts/logging/python-logging|python logging integration> to forward the writes to dagster, I\u2019m pretty sure this would just result in a ton of different log messages like `#`, `##`, `###`, `####`\u2026, because a progress bar typically overwrites the same line in terminal during execution, but there is no such concept in dagster logs.\n\nIn order to support this I think we would need to provide a de novo API and UI element, and I don\u2019t think we\u2019re going to do that in the near term.\n\nOne alternative might be to use dagster\u2019s python logging integration to write a subset of percentage updates to the log, like \u201c10% complete\u201d, \u201c20% complete\u201d, etc. I don\u2019t know anything about PyMC or fastprogress, but is it possible to configure it to emit such messages?", "dagster_empl_last_target": "Hi Matthias, we do not support streaming a progress bar from a process atm. While in theory one might be able to use our <https://docs.dagster.io/concepts/logging/python-logging|python logging integration> to forward the writes to dagster, I\u2019m pretty sure this would just result in a ton of different log messages like `#`, `##`, `###`, `####`\u2026, because a progress bar typically overwrites the same line in terminal during execution, but there is no such concept in dagster logs.\n\nIn order to support this I think we would need to provide a de novo API and UI element, and I don\u2019t think we\u2019re going to do that in the near term.\n\nOne alternative might be to use dagster\u2019s python logging integration to write a subset of percentage updates to the log, like \u201c10% complete\u201d, \u201c20% complete\u201d, etc. I don\u2019t know anything about PyMC or fastprogress, but is it possible to configure it to emit such messages?"}, {"question": "My asset returns Output(df, metadata={...})\nHow do I access this metadata from my io-manager? I am able to find a bunch of requests from last year for this functionality but can't find any resolutions", "replies": ["Hi Brittany, This is not currently accessible from the IO manager-- I just looked up the issues you mention and can confirm they haven\u2019t been fixed. I\u2019m going to take on solving this.\n\nIn the meantime, this depends on the nature of the metadata but you might be able to put it on the `AssetOut` if you want to access it in the IO manager."], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, metadata is not accessible from the IO manager in Dagster, but you might be able to put it on the `AssetOut` if you want to access it in the IO manager.", "dagster_empl_first_target": "Hi Brittany, This is not currently accessible from the IO manager-- I just looked up the issues you mention and can confirm they haven\u2019t been fixed. I\u2019m going to take on solving this.\n\nIn the meantime, this depends on the nature of the metadata but you might be able to put it on the `AssetOut` if you want to access it in the IO manager.", "dagster_empl_last_target": "Hi Brittany, This is not currently accessible from the IO manager-- I just looked up the issues you mention and can confirm they haven\u2019t been fixed. I\u2019m going to take on solving this.\n\nIn the meantime, this depends on the nature of the metadata but you might be able to put it on the `AssetOut` if you want to access it in the IO manager."}, {"question": "What's the right way to test IO managers that handle time window partitions?", "replies": ["This is a bug we\u2019re aware of-- until it gets fixed, I \u2019d recommend just executing the op in testing as part of a job. That will ensure the `step_context` is present. Something like this:\n\n```@asset(partitions_def=..., io_manager_key=\"foo\")\ndef my_asset(context):\n    ...\n\ndefs = Definitions(\n    assets=[my_asset],\n    jobs=[define_asset_job(\"my_job\")],\n    resources={\"foo\": &lt;io manager&gt;},\n)\n\ndefs.get_job_def(\"my_job\").execute_in_process()```", "Heads up I created an issue for this: <https://github.com/dagster-io/dagster/issues/15051>", "Thanks! I'll follow it so I know when I can swap over"], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "To test IO managers that handle time window partitions, you can execute the operation in testing as part of a job to ensure the `step_context` is present. However, there is a known bug related to this, and an issue has been created on GitHub to track its progress.", "dagster_empl_first_target": "This is a bug we\u2019re aware of-- until it gets fixed, I \u2019d recommend just executing the op in testing as part of a job. That will ensure the `step_context` is present. Something like this:\n\n```@asset(partitions_def=..., io_manager_key=\"foo\")\ndef my_asset(context):\n    ...\n\ndefs = Definitions(\n    assets=[my_asset],\n    jobs=[define_asset_job(\"my_job\")],\n    resources={\"foo\": &lt;io manager&gt;},\n)\n\ndefs.get_job_def(\"my_job\").execute_in_process()```", "dagster_empl_last_target": "Heads up I created an issue for this: <https://github.com/dagster-io/dagster/issues/15051>"}, {"question": "is there an easy way to disable a schedule if it fails n times in a row?", "replies": ["Hi Drew, There currently is not.  If you\u2019d find this useful you should submit an issue: <https://github.com/dagster-io/dagster/issues>"], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, there is no easy way to disable a schedule if it fails n times in a row in Dagster.", "dagster_empl_first_target": "Hi Drew, There currently is not.  If you\u2019d find this useful you should submit an issue: <https://github.com/dagster-io/dagster/issues>", "dagster_empl_last_target": "Hi Drew, There currently is not.  If you\u2019d find this useful you should submit an issue: <https://github.com/dagster-io/dagster/issues>"}, {"question": "Is there a way to set the order in which backfills of Statically partitioned assets are run? Seems like the default is in sorted ascending order. Is there a setting we could change to run it in descending order?", "replies": ["Hi <@U04DU27K8UA>! it's not currently possible to supply a custom sort key, although looking at the code I would expect the partitions should be processed in the order that they're defined in the StaticPartitionsDefinition (or maybe the reverse order) -- is it possible that this is what's happening?", "Ah yeah that\u2019s exactly what\u2019s happening. They\u2019re defined in ascending order in our definition\u2026thanks for the sanity check!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, it's not possible to set a custom sort key for backfills of statically partitioned assets in Dagster, they are processed in the order defined in the StaticPartitionsDefinition.", "dagster_empl_first_target": "Hi <@U04DU27K8UA>! it's not currently possible to supply a custom sort key, although looking at the code I would expect the partitions should be processed in the order that they're defined in the StaticPartitionsDefinition (or maybe the reverse order) -- is it possible that this is what's happening?", "dagster_empl_last_target": "Hi <@U04DU27K8UA>! it's not currently possible to supply a custom sort key, although looking at the code I would expect the partitions should be processed in the order that they're defined in the StaticPartitionsDefinition (or maybe the reverse order) -- is it possible that this is what's happening?"}, {"question": "Is it possible for a job to materialize an asset? For instance, I want a job to execute (that is not an asset, just does work based on a config) and then save an offset int to be used by the job the next time it runs.", "replies": ["Hi Aaron, Any op can `yield AssetMaterialization(...)`. This will generate an `AssetMaterialization` event, and you can put whatever you want in the metadata (like a path to where the offset is stored, or perhaps the value itself). However, you will have to handle the actual persistence of the value yourself (unless you encode it directly in the materialization metadata).\n\nBut you might not even want to represent this value as an asset-- dagster doesn\u2019t need to know about it, you could just read/write it from some path you either hardcode or pass in via config."], "is_dagster_empl": [true], "gpt4_replies_target": "Yes, a job can materialize an asset by yielding `AssetMaterialization(...)`. However, you will need to handle the actual persistence of the value yourself, unless you encode it directly in the materialization metadata.", "dagster_empl_first_target": "Hi Aaron, Any op can `yield AssetMaterialization(...)`. This will generate an `AssetMaterialization` event, and you can put whatever you want in the metadata (like a path to where the offset is stored, or perhaps the value itself). However, you will have to handle the actual persistence of the value yourself (unless you encode it directly in the materialization metadata).\n\nBut you might not even want to represent this value as an asset-- dagster doesn\u2019t need to know about it, you could just read/write it from some path you either hardcode or pass in via config.", "dagster_empl_last_target": "Hi Aaron, Any op can `yield AssetMaterialization(...)`. This will generate an `AssetMaterialization` event, and you can put whatever you want in the metadata (like a path to where the offset is stored, or perhaps the value itself). However, you will have to handle the actual persistence of the value yourself (unless you encode it directly in the materialization metadata).\n\nBut you might not even want to represent this value as an asset-- dagster doesn\u2019t need to know about it, you could just read/write it from some path you either hardcode or pass in via config."}, {"question": "question about syncing a partitioned asset from postgres to snowflake:\n1. i have a `postgres.events` asset with a daily partition. it's fully materialized\n2. i have a `snowflake.events` asset with a daily partition. these depend on `postgres.events`, and they need to get materialized, but *in date asc* order\n3. i was thinking of writing a job that iterates through the `postgres.events` partitions, enqueues the corresponding `snowflake.events` partition, but it has to be done sequentially, and it needs to abort upon failure (because i want to insert the data in order)\nany tips on how to accomplish #3 this? or am i thinking about it wrong? thank you!", "replies": ["Is there a particular reason why you'd like the inserts to be done sequentially? If you're clustering your Snowflake table by event time, Snowflake should automatically re-cluster on its own with the insert.\n\nDepending on the scale of your data, I'd recommend not reading the postgres data into memory in order to write it to Snowflake. If it's too big to fit into memory, you might want to consider using Dagster to orchestrate:\n\u2022 dumping the data out of Postgres into an S3 bucket (or something similar)\n\u2022 having Snowflake recognize that bucket as an external stage\n\u2022 then running a query to move the data from the stage directly into Snowflake.\nAnd assuming you use the date/partition key as how you segment and store the data, the ingests should stay in step together when moving out of postgres and into snowflake.", "hey Tim, thanks for the quick reply!\n\n&gt; Is there a particular reason why you'd like the inserts to be done sequentially? \nyea i was wondering about this too. i found <https://community.snowflake.com/s/question/0D50Z00009a59J3SAI/does-order-by-matter-when-ingesting-a-lot-of-data-from-stageis-the-order-used-in-deciding-the-storage-clustering|this answer on the Snowflake forum> from a Snowflake employee saying order _does_ matter :thinking_face:\n\nbut i guess if i decide to specify the `cluster by` myself, then i'm back in control\n\n&gt; Depending on the scale of your data, I'd recommend not reading the postgres data into memory\nyea sorry i ommitted that in my original post, but the `postgres.events` is actually *postres data* but hosted on *s3 in parquet* format. i guess i should rename the asset to `s3.events`? :thinking_face:\n\nthat's also a point of confusion for me. should these be three separate assts? they're all the same data, but hosted in different places (postgres -&gt; s3 parquet -&gt; snowflake)", "Oh yeah, they _are_ right, but if the data is at the scale where you have to worry about your ingest, I think it's a fair time to start hi-jacking their partitioning strategy and managing the clustering on your own.\n\nI like framing this as 3 assets:\n\u2022 `postgres_events` - is \"optional\", but it's a<https://docs.dagster.io/_apidocs/assets#dagster.SourceAsset| source asset>, since I assume your product is what populates this, not orchestrated by Dagster.\n    \u25e6 I'm gonna ack that this asset you can likely skip if you don't want this level of observability or touch declarative scheduling this early on.\n\u2022 `s3_events`  - the exporting and dumping of postgres data into S3\n\u2022 `snowflake_events`  - the query from the stage\nThe reason why I'd split this into multiple assets is so if any of those steps fail, you can re-run from there rather than having to do it all over again.\n\nex. You likely won't get any errors when dumping from Postgres to S3, but I'd expect the copy into Snowflake step to fail a bit because the schema for `events` is likely prone to change. By splitting the assets, you can let the fail happen, patch up your query for copying into snowflake, and then re-run just that asset materialization, rather than both assets.", "awesome, thank you so much for the great support! :pray:"], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "To sync a partitioned asset from Postgres to Snowflake, consider orchestrating the process in three steps: dumping the data from Postgres into an S3 bucket, having Snowflake recognize that bucket as an external stage, and then running a query to move the data from the stage directly into Snowflake. This process can be framed as three assets: `postgres_events`, `s3_events`, and `snowflake_events`. This way, if any step fails, you can re-run from there rather than having to do it all over again.", "dagster_empl_first_target": "Is there a particular reason why you'd like the inserts to be done sequentially? If you're clustering your Snowflake table by event time, Snowflake should automatically re-cluster on its own with the insert.\n\nDepending on the scale of your data, I'd recommend not reading the postgres data into memory in order to write it to Snowflake. If it's too big to fit into memory, you might want to consider using Dagster to orchestrate:\n\u2022 dumping the data out of Postgres into an S3 bucket (or something similar)\n\u2022 having Snowflake recognize that bucket as an external stage\n\u2022 then running a query to move the data from the stage directly into Snowflake.\nAnd assuming you use the date/partition key as how you segment and store the data, the ingests should stay in step together when moving out of postgres and into snowflake.", "dagster_empl_last_target": "Oh yeah, they _are_ right, but if the data is at the scale where you have to worry about your ingest, I think it's a fair time to start hi-jacking their partitioning strategy and managing the clustering on your own.\n\nI like framing this as 3 assets:\n\u2022 `postgres_events` - is \"optional\", but it's a<https://docs.dagster.io/_apidocs/assets#dagster.SourceAsset| source asset>, since I assume your product is what populates this, not orchestrated by Dagster.\n    \u25e6 I'm gonna ack that this asset you can likely skip if you don't want this level of observability or touch declarative scheduling this early on.\n\u2022 `s3_events`  - the exporting and dumping of postgres data into S3\n\u2022 `snowflake_events`  - the query from the stage\nThe reason why I'd split this into multiple assets is so if any of those steps fail, you can re-run from there rather than having to do it all over again.\n\nex. You likely won't get any errors when dumping from Postgres to S3, but I'd expect the copy into Snowflake step to fail a bit because the schema for `events` is likely prone to change. By splitting the assets, you can let the fail happen, patch up your query for copying into snowflake, and then re-run just that asset materialization, rather than both assets."}, {"question": "Hello! This might be a very simple question about shutting down queued runs in dagster. I was running a dagster asset over a bunch of partitions and around 2000 was queued. I wanted to shut it down in the middle of the run. Is there a fast way to do so other than shutting the server down? (and could shutting the server down cause some unintended issues?) So far the UI only lets me terminate 25 runs at a time. Thanks!", "replies": ["Hey Kevin, Yes the backfills page is the correct place to do this."], "is_dagster_empl": [true], "gpt4_replies_target": "Yes, you can shut down queued runs in the middle of a run from the backfills page in dagster.", "dagster_empl_first_target": "Hey Kevin, Yes the backfills page is the correct place to do this.", "dagster_empl_last_target": "Hey Kevin, Yes the backfills page is the correct place to do this."}, {"question": "Hi friends, I seem to be having some trouble launching jobs in ECS\u2026", "replies": ["I\u2019m using EcsRunLauncher as my run_launcher class, but I get an error in Dagit that \u201cimage\u201d is not a str when a job is dequeuing. ", "It appears the DAGSTER_CURRENT_IMAGE environment variable needs to be set in the usercode ECS container. It should be the ECR URL pointing to the same image the usercode task is running :)"], "is_dagster_empl": [false, false], "gpt4_replies_target": "You need to set the DAGSTER_CURRENT_IMAGE environment variable in the usercode ECS container to the ECR URL pointing to the same image the usercode task is running to resolve the error.", "dagster_empl_first_target": "I\u2019m using EcsRunLauncher as my run_launcher class, but I get an error in Dagit that \u201cimage\u201d is not a str when a job is dequeuing. ", "dagster_empl_last_target": "It appears the DAGSTER_CURRENT_IMAGE environment variable needs to be set in the usercode ECS container. It should be the ECR URL pointing to the same image the usercode task is running :)"}, {"question": "Happy new year folks! :tada:\n\nIs there a way currently to automatically trigger a dbt run as soon as it\u2019s upstream assets are materialized?\n\nI have a scenario where there are:\n\u2022 Multiple airbyte connection (with multiple assets produced per connection) \n\u2022 Multiple dbt projects that consume from airbyte assets \nI want a sensor to automatically trigger a dbt project as soon as it\u2019s upstream assets have all been materialized.", "replies": ["Happy New Year!\n1. Create multiple jobs, one for each set of dbt models you want to trigger. This could be an entire project, or even a subset of a project. (We have 20+ jobs using the same project, but different models or same parameterized models with different vars)\n2. Create <https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#multi-asset-sensors-|multi-asset sensors> to trigger those jobs, based on the appropriate asset selection\nYou can also create a combined asset job for airbyte + dbt, without sensors. We use sensors for triggering dbt tests/doc generation, but the model runs are handled as a combined asset job with airbyte assets (dagster will handle the dependencies, as long as they are correctly represented in your asset graph).", "Thanks Adam! That does certainly help!\n\nHave you encountered scenarios with dbt cross-project dependencies?\n\nIn the screenshot below, I have `project2.mart_gh_join` that reads from `project1.mart_gh_stargazer` and `project2.mart_gh_cumulative`.\n\nBut since `project1.mart_gh_stargazer` is from a separate project, it gets treated as a separate \u201cempty\u201d node rather than pointing to the actual node.", "They show up as different asset keys. The one defined as a dbt source appears as `postgres/mart_gh_stargazer`, and the one defined as a model in project 1 appears as `postgres1/mart/mart_gh_stargazer`", "You\u2019ll need to get the to be the same asset key. (There may be other ways, but that\u2019s what I\u2019m familiar with.) A few things that might help you with that are the `source_key_prefix` and `node_info_to_asset_key` arguments to <https://docs.dagster.io/_apidocs/libraries/dagster-dbt#dagster_dbt.load_assets_from_dbt_manifest|https://docs.dagster.io/_apidocs/libraries/dagster-dbt#dagster_dbt.load_assets_from_dbt_manifest>\n\nI suspect the latter is going to be what you want to modify. The default mapping function is defined here for reference: <https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt/asset_defs.py#L133|https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-dbt/dagster_dbt/asset_defs.py#L133>", "Thanks Adam - your suggestions helped with the troubleshooting. I was able to get it to work.\n\nThe solution was to make sure that I set my sources name to `mart` as that's what dagster uses as part of the asset key.\n\n```version: 2\nsources:\n- name: mart\n  database: postgres\n  schema: public\n  tables:\n    - name: mart_gh_stargazer\n      identifier: _mart_gh_stargazer\n      description: Given stars on GitHub repositories```"], "is_dagster_empl": [false, false, false, false, false], "gpt4_replies_target": "To automatically trigger a dbt run when its upstream assets are materialized, you can create multiple jobs for each set of dbt models you want to trigger and then create multi-asset sensors to trigger those jobs based on the appropriate asset selection. If there are cross-project dependencies, ensure the asset keys are the same by using the `source_key_prefix` and `node_info_to_asset_key` arguments.", "dagster_empl_first_target": "Happy New Year!\n1. Create multiple jobs, one for each set of dbt models you want to trigger. This could be an entire project, or even a subset of a project. (We have 20+ jobs using the same project, but different models or same parameterized models with different vars)\n2. Create <https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#multi-asset-sensors-|multi-asset sensors> to trigger those jobs, based on the appropriate asset selection\nYou can also create a combined asset job for airbyte + dbt, without sensors. We use sensors for triggering dbt tests/doc generation, but the model runs are handled as a combined asset job with airbyte assets (dagster will handle the dependencies, as long as they are correctly represented in your asset graph).", "dagster_empl_last_target": "Thanks Adam - your suggestions helped with the troubleshooting. I was able to get it to work.\n\nThe solution was to make sure that I set my sources name to `mart` as that's what dagster uses as part of the asset key.\n\n```version: 2\nsources:\n- name: mart\n  database: postgres\n  schema: public\n  tables:\n    - name: mart_gh_stargazer\n      identifier: _mart_gh_stargazer\n      description: Given stars on GitHub repositories```"}, {"question": "Hi everyone, we currently operate the open source deployment using helm.\nWe have an issue with the `code location server`  (gRPC server):\n\nOur jobs and assets are built dynamically based on some configuration (provided by an API).\nNow what we want, is to rebuilt the job/asset graph on a reload.\nHowever we did not get it to work with `dagster api grpc --python-file repo.py`\nFrom what we reverse engineered, is that the  `repo.py` is only evaluated on startup, but not on a reload.\n\nrepo.py:\n```from dagster import (repository)\n\nprint(\"_______ Outside __________\")\n\n@repository(name=\"repo\")\ndef dagster():\n    print(\"_______ Inside the repository function__________\")\n    return build_dynamic_repository()```\nWhen I run this locally using `dagster api grpc --python-file repo.py`\nI get\n```_______ Outside __________\n_______ Inside the repository function__________\ndagster.code_server - INFO - Started Dagster code server ```\nBut when I hit `reload` in dagit (or graphQL) it never prints neither `Outside` nor `Inside`\n\nAny pointers on how I could set this up differently? Moving to definitions would not help if the whole python-file is never re-evaluated on reload.\nThe only option right now is to restart the grpc server which is not feasable for us", "replies": ["Hey Manuel here - it's a bit buried, but there's a `RepositoryData` option that you can use here that lets you return an object with a different caching setup for this case: <https://docs.dagster.io/_apidocs/repositories#dagster.repository>", "Can you share more about why restarting the grpc server isn't feasible?", "We currently built our system that a user has an UI with very restricted settings that we use to generate the repository from.\n\nWhenever a configuration changes we want to reload the repository as assets / schedules changed.\n\nAs we are running on k8s, restarting the grpc server would mean we would have to kill the user-code-deployment pod on every small onChangeNotification originating from our users", "Thanks a lot, I was actually browsing the page a lot but still missed the RepositoryData. I will try to get it running with your suggestion, thank you", "Will something like this be supported in later versions (as we are moving away from `@repository`?\n\nAlso this method expects the legacy pipeline definitions, do you have any pointers in the documentation on how to get from \u201cassets, jobs, schedules\u201d to PipelineDefinitions?", "Note for others reading this:\n`@repository` instantiates a `CachingRepositoryData` which can be found in repository_definition.py", "Yes, no plans to remove that functionality (or repository) for the forseeable future", "You can return jobs from get_all_pipelines (it's a bit confusing)\n\nLooking at the public definitions on RepositoryData may be the easiest way: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/repository_definition.py#L267-L491>", "<@U016C4E5CP8> have there been any updates to this? We are on the new versions using code locations and facing the issue of having to restart the gRPC server which is too much..", "We have assets, jobs and sensors..", "We are usingthe following <@U04EZSM3M8C>\n```class NonCachingRepositoryData(RepositoryData):\n    __underlying_repository_data: RepositoryData\n\n    def _reload_underlying_repository_data(self):\n        self.__underlying_repository_data = CachingRepositoryData.from_list(assets, jobs, sensors)\n\n    def get_all_jobs(self) -&gt; Sequence[JobDefinition]:\n        return self.__underlying_repository_data.get_all_jobs()\n\n    def get_all_pipelines(self) -&gt; Sequence[PipelineDefinition]:\n        # We have to reload here\n        # this is a technical detail, a reload triggers this method first, can be changed in future releases!\n        self._reload_underlying_repository_data()\n        return self.__underlying_repository_data.get_all_pipelines()\n\n    def get_all_schedules(self) -&gt; Sequence[ScheduleDefinition]:\n        return self.__underlying_repository_data.get_all_schedules()\n\n    def get_all_sensors(self) -&gt; Sequence[SensorDefinition]:\n        return self.__underlying_repository_data.get_all_sensors()\n\n    def get_all_partition_sets(self) -&gt; Sequence[PartitionSetDefinition]:\n        return self.__underlying_repository_data.get_all_partition_sets()```"], "is_dagster_empl": [true, true, false, false, false, false, true, true, false, false, false], "gpt4_replies_target": "To dynamically rebuild the job/asset graph on a reload, you can use the `RepositoryData` option which allows you to return an object with a different caching setup. However, this method expects the legacy pipeline definitions, so you might need to look into how to convert from \u201cassets, jobs, schedules\u201d to PipelineDefinitions.", "dagster_empl_first_target": "Hey Manuel here - it's a bit buried, but there's a `RepositoryData` option that you can use here that lets you return an object with a different caching setup for this case: <https://docs.dagster.io/_apidocs/repositories#dagster.repository>", "dagster_empl_last_target": "You can return jobs from get_all_pipelines (it's a bit confusing)\n\nLooking at the public definitions on RepositoryData may be the easiest way: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/repository_definition.py#L267-L491>"}, {"question": "In a docker deployment using separate user code containers, what does the \"reload location\" button at the lower-left of the dagit UI actually do? Does it restart the user code container?", "replies": ["It does not restart the user code container - it reloads dagit's copy of the data (which often isn't very useful since its unlikely to have changed unless the user code container has restarted)", "Thanks, Daniel. Is there any way to have this type of deployment detect changes to the user code so that it can be refreshed in the UI without a `docker-compose down` and `docker-compose up`?", "there isn't currently a way to do it without restarting the code server in some way - but this comes up frequently and we're looking into ways to support this better", "Thanks! FWIW, the documentation is misleading in this regard, I think.", "I would definitely believe that - is there a specific page or paragraph you're thinking of?", "I think some of it has to do with legacy discussions of using @repository vs. the module structure. It's also b/c of the \"reload location\" buttons on the dagit UI. I've set up containerized modules as code locations, based on following the documentation, so I expected \"reload location\" to detect that those changed. Now that I know that it doesn't, I can move on to other work instead of trying to debug my setup. :slightly_smiling_face: This page is pretty clear that the user code container needs to be rebuilt and restarted if code is updated, though: <https://docs.dagster.io/deployment/guides/docker#multi-container-docker-deployment>\n\nFor now, I'll probably just modify my CI pipeline to restart the relevant container(s). A more specific paragraph outlining recommended way to handle reloading user code containers would be nice to have in the docker deployment section of the documentation. It's somewhat of a must-have feature for setups with multiple people managing work on dagster.", "I think a possible end state here would be dagster more directly managing the user code containers instead of relying on docker compose to do it - e.g. the daemon could spin up a container for each code location on startup, and when you hit reload it could deploy a new one for you", "That would be good. Our environment is very cloistered, so I have to rely entirely on internal tools for my work. We have an internal gitlab instance that we can use for CI. My use case involves 2-3 teams who will each maintain a repository of user code (1 per team, initially - more later, perhaps) for dagster. In a perfect world, they would have dev/prod branches in their git repo and when they pushed to a prod branch, the CI system could deploy the code to the dagster server and dagster/daemon/dagit would notice the change and optionally prompt the user to reload the code location through the UI. (Optionally, it could automatically reload.) For now, I'll write a bash script that the CI can execute that will pull the git repos to the server and then restart the user code containers", "Thanks for the information! :slightly_smiling_face:"], "is_dagster_empl": [true, false, true, false, true, false, true, false, false], "gpt4_replies_target": "The \"reload location\" button in the dagit UI does not restart the user code container, but instead reloads dagit's copy of the data. Currently, there isn't a way to detect changes to the user code without restarting the code server, but improvements are being considered.", "dagster_empl_first_target": "It does not restart the user code container - it reloads dagit's copy of the data (which often isn't very useful since its unlikely to have changed unless the user code container has restarted)", "dagster_empl_last_target": "I think a possible end state here would be dagster more directly managing the user code containers instead of relying on docker compose to do it - e.g. the daemon could spin up a container for each code location on startup, and when you hit reload it could deploy a new one for you"}, {"question": "Is there a work around to getting the results from a dagstermill asset running in the pipeline? I am running into this:\n\n```def yield_result(self, value, output_name=\"result\"):\n \n    &lt;snip&gt;\n \n    # dagstermill assets do not support yielding additional results within the notebook:\n    if len(step_context.job_def.asset_layer.asset_keys) &gt; 0:\n      raise DagstermillError(\n        \"dagstermill assets do not currently support dagstermill.yield_result\"\n      )```", "replies": ["this is maybe a bit hacky but you could manually write your result to storage, then you could make no-op asset with the name set so that it corresponds with the location where you store the result in the notebook. This asset would take the notebook asset as a `non_argument_deps` . then i think you could use the asset corresponding to the result in other assets and it would fetch the correct value from storage.\n\nbig caveat - i\u2019ve never tried this and i dont know if you\u2019ll run into typing issues (returning None from the asset but expecting it to be a typed value in a downstream asset may be an issue) or other major issues. If you want to give it a try i recommend writing up a super simple notebook + asset graph to test it out", "Thanks <@U030M2AL48M> I was thinking the same thing.  Thanks for confirming that solution."], "is_dagster_empl": [true, false], "gpt4_replies_target": "A possible workaround to get results from a dagstermill asset running in the pipeline could be to manually write your result to storage, then create a no-op asset with the name set to correspond with the location where you store the result in the notebook. This asset would take the notebook asset as a `non_argument_deps`. However, this solution might have potential issues with typing or other major issues.", "dagster_empl_first_target": "this is maybe a bit hacky but you could manually write your result to storage, then you could make no-op asset with the name set so that it corresponds with the location where you store the result in the notebook. This asset would take the notebook asset as a `non_argument_deps` . then i think you could use the asset corresponding to the result in other assets and it would fetch the correct value from storage.\n\nbig caveat - i\u2019ve never tried this and i dont know if you\u2019ll run into typing issues (returning None from the asset but expecting it to be a typed value in a downstream asset may be an issue) or other major issues. If you want to give it a try i recommend writing up a super simple notebook + asset graph to test it out", "dagster_empl_last_target": "this is maybe a bit hacky but you could manually write your result to storage, then you could make no-op asset with the name set so that it corresponds with the location where you store the result in the notebook. This asset would take the notebook asset as a `non_argument_deps` . then i think you could use the asset corresponding to the result in other assets and it would fetch the correct value from storage.\n\nbig caveat - i\u2019ve never tried this and i dont know if you\u2019ll run into typing issues (returning None from the asset but expecting it to be a typed value in a downstream asset may be an issue) or other major issues. If you want to give it a try i recommend writing up a super simple notebook + asset graph to test it out"}, {"question": "I'm trying to follow this section <https://docs.dagster.io/concepts/resources#accessing-resources-in-ops> for accessing resources in ops with the following code:\n```from dagster import job, op, Definitions\n\n@op(required_resource_keys={\"database\"})\ndef op_requires_resources(context):\n    database_resource = context.resources.database\n    <http://context.log.info|context.log.info>(database_resource)\n\n@job()\ndef demo_job():\n    op_requires_resources()\n\ndefs = Definitions(\n    jobs=[demo_job],\n    resources={\n        \"database\": \"test_resource\",\n    }\n)```\nhowever, I'm getting the following error:\n```dagster._core.errors.DagsterInvalidDefinitionError: resource with key 'database' required by op 'op_requires_resources' was not provided. Please provide a &lt;class 'dagster._core.definitions.resource_definition.ResourceDefinition'&gt; to key 'database', or change the required key to one of the following keys which points to an &lt;class 'dagster._core.definitions.resource_definition.ResourceDefinition'&gt;: ['io_manager']```\nwhat am I doing wrong here?", "replies": ["You need to define the resource e.g. by decorating a function\n&gt; @resource\n&gt; def test_resource(init_context):\n&gt; ...\n", "thanks <@U04DATSSK2T>", "<@U04DATSSK2T> how do i get the resources from:\n```defs = Definitions(\n    jobs=[demo_job],\n    resources={\n        \"database\": \"test_resource\",\n    }\n)```\nin:\n&gt; @resource\n&gt; def test_resource(init_context):\n", "Try replacing \"test_resource\" in your definitions with ResourceDefinition.hardcoded_resource(\"test_resource\").\nThen replace it with test_resource (object not string) and define a more complex resource in a function as I mentioned. This is also a ResourceDefinition.\n\nIn the Definitions you are linking a ResourceDefinition with a key in the resources dictionary. You then define a op which makes reference to the key. The purpose of this all is so that if you want to use different resources, you don't have to rewrite the op."], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "You need to define the resource by decorating a function with @resource. Then, replace \"test_resource\" in your definitions with ResourceDefinition.hardcoded_resource(\"test_resource\"). After that, replace it with test_resource (object not string) and define a more complex resource in a function. This will link a ResourceDefinition with a key in the resources dictionary, allowing you to use different resources without having to rewrite the op.", "dagster_empl_first_target": "You need to define the resource e.g. by decorating a function\n&gt; @resource\n&gt; def test_resource(init_context):\n&gt; ...\n", "dagster_empl_last_target": "Try replacing \"test_resource\" in your definitions with ResourceDefinition.hardcoded_resource(\"test_resource\").\nThen replace it with test_resource (object not string) and define a more complex resource in a function as I mentioned. This is also a ResourceDefinition.\n\nIn the Definitions you are linking a ResourceDefinition with a key in the resources dictionary. You then define a op which makes reference to the key. The purpose of this all is so that if you want to use different resources, you don't have to rewrite the op."}, {"question": "Hi, Is there a best practice for defining multiple software-defined-assets which share the same parameterised op but with different parameters? config_schema doesn't quite work because these wouldn't be expected to change at run time. It seems like an op factory is the best way forward, but I thought I'd check because it seems a bit like a sledgehammer to crack a nut. Can you define an op's config schema as part of calling it from another op, or is that only in jobs? Thanks!", "replies": ["Never mind, the configured api seems to be perfect for what I need. What a great piece of software!"], "is_dagster_empl": [false], "gpt4_replies_target": "The user found that the configured API in dagster is the perfect solution for defining multiple software-defined-assets which share the same parameterised op but with different parameters.", "dagster_empl_first_target": "Never mind, the configured api seems to be perfect for what I need. What a great piece of software!", "dagster_empl_last_target": "Never mind, the configured api seems to be perfect for what I need. What a great piece of software!"}, {"question": "Hi all! I am trying to set up an Airbyte Connection in Dagster with a Postgres source and a Snowflake Destination. I started with the assets_modern_data_stack template, which already shows a clear format for the Postgres Source config, but not a Snowflake Destination. When I tried to write a config of the same format for a Snowflake Destination:\n\n`from dagster_snowflake.utils import get_conn_string as get_conn_string_sf`\n\nI got the error:\n\n`ModuleNotFoundError: No module named 'dagster_snowflake.utils'`\n\nAfter reading through the dagster-snowflake documentation (<https://docs.dagster.io/_modules/dagster_snowflake/resources>) , it looked like I needed dagster_snowflake.resources instead.\n\nWhen I tried:\n\n`from dagster_snowflake.resources import get_connection as get_conn_string_sf`\n\nI got the error:\n\n`ImportError: cannot import name 'get_connection' from 'dagster_snowflake.resources' (/opt/homebrew/lib/python3.10/site-packages/dagster_snowflake/resources.py)`\n\nIs there something that I should be doing differently? Is there a good template for creating an Airbyte Snowflake destination? Thanks!", "replies": ["Disregard this thread. I am now trying the approach here, but running into similar issues: <https://dagster.slack.com/archives/C01U954MEER/p1675881639360499>"], "is_dagster_empl": [false], "gpt4_replies_target": "The user is experiencing issues with importing the correct modules for setting up a Snowflake Destination in Dagster and is now trying a different approach as per a thread in the Dagster Slack community.", "dagster_empl_first_target": "Disregard this thread. I am now trying the approach here, but running into similar issues: <https://dagster.slack.com/archives/C01U954MEER/p1675881639360499>", "dagster_empl_last_target": "Disregard this thread. I am now trying the approach here, but running into similar issues: <https://dagster.slack.com/archives/C01U954MEER/p1675881639360499>"}, {"question": "My team is having trouble implementing `tag_concurrency_limits` when deploying on *k8s*\nOur run coordinator includes the following:\n```run_coordinator:\n  module: dagster._core.run_coordinator\n  class: QueuedRunCoordinator\n  config:\n    max_concurrent_runs: 2\n    tag_concurrency_limits:\n    - key: catalog\n      limit: 4\n      value: iceberg```\nThe relevant `ops` are decorated with `@op(tags={\"catalog\": \"iceberg\"})`\nYet, more than 4 of these ops will process concurrently. The ops are being spawned by a `yield DynamicOutput(...)`. Is this a special case where I need to do something differently?\nWe'd like to limit the number of spawned ops that access one of our databases.", "replies": ["Hi Chris - those tag limits only apply to jobs/runs, not to individual ops. You can also apply tag limits to ops within a single run, but we don\u2019t yet have a way to globally limit the number of ops with a certain tag across all runs", "Or, actually - on k8s specifically there is a way to specify limits like this with celery: <https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm-advanced|https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm-advanced>", "<@U016C4E5CP8>, thanks for the help! I'll be working on this again in about 12 hours with someone else on our team who does the deploying."], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "The tag limits in Dagster only apply to jobs/runs, not to individual ops. However, on Kubernetes, you can specify limits with celery.", "dagster_empl_first_target": "Hi Chris - those tag limits only apply to jobs/runs, not to individual ops. You can also apply tag limits to ops within a single run, but we don\u2019t yet have a way to globally limit the number of ops with a certain tag across all runs", "dagster_empl_last_target": "Or, actually - on k8s specifically there is a way to specify limits like this with celery: <https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm-advanced|https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm-advanced>"}, {"question": "trying to use sensors and schedules to start kicking off materialization of certain assets and running certain jobs. Currently getting this error on every sensor/schedule:\n```Exception: 'network' cannot be used in 'container_kwargs'. Use the 'networks' config key instead.\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py\", line 335, in _dequeue_run\n    instance.run_launcher.launch_run(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/instance/__init__.py\", line 660, in run_launcher\n    launcher = cast(InstanceRef, self._ref).run_launcher\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/instance/ref.py\", line 491, in run_launcher\n    return self.run_launcher_data.rehydrate() if self.run_launcher_data else None\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_serdes/config_class.py\", line 101, in rehydrate\n    return klass.from_config_value(self, check.not_none(result.value))\n  File \"/usr/local/lib/python3.10/site-packages/dagster_docker/docker_run_launcher.py\", line 66, in from_config_value\n    return DockerRunLauncher(inst_data=inst_data, **config_value)\n  File \"/usr/local/lib/python3.10/site-packages/dagster_docker/docker_run_launcher.py\", line 41, in __init__\n    validate_docker_config(network, networks, container_kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/dagster_docker/utils.py\", line 47, in validate_docker_config\n    raise Exception(```\nmaterializing the assets manually through dagit works fine, it's only trying to schedule these jobs using sensors and schedules. We're using the DefaultRunCoordinator and DefaultRunLauncher. Am I missing some other sort of configuration here?", "replies": ["actually I just discovered we have a different configuration in the daemon, so that's likely the problem. Do we need the daemon run_launcher and run_coordinator to match what's in the dagit container?", "They don\u2019t necessarily have to be in sync, but we generally recommend it", "In part to avoid confusing situations like this where you\u2019re not sure what the configuration actually is", ":+1: I'll definitely get those in sync. I think we wants to use the DockerRunLauncher moving forward", "Schedules and sensors are always launched from the daemon but launching runs from dagit are launched from the dagit process (unless you\u2019re using the run queue, in which case they will also be launched from the daemon) - which I think matches what you\u2019re seeing", "so if you specify the QueuedRunCoordinator in the dagit config, it'll hand over all jobs from the dagit process to the daemon?", "That\u2019s right", "Thank you!"], "is_dagster_empl": [false, true, true, false, true, false, true, false], "gpt4_replies_target": "The error you're encountering is likely due to a mismatch in configuration between your daemon and dagit container. While they don't necessarily have to be in sync, it's generally recommended to avoid confusion. Schedules and sensors are always launched from the daemon, but launching runs from dagit are launched from the dagit process, unless you're using the run queue, in which case they will also be launched from the daemon. If you specify the QueuedRunCoordinator in the dagit config, it'll hand over all jobs from the dagit process to the daemon.", "dagster_empl_first_target": "They don\u2019t necessarily have to be in sync, but we generally recommend it", "dagster_empl_last_target": "That\u2019s right"}, {"question": "Curious how I should set my project up: Lets take a very simple example - I have 3 tasks (A, B, C) and C is dependent on B and B is dependent on A. In other words it looks like this A-&gt;B-&gt;C. In each of these tasks, Im querying an API and creating a table in my warehouse. I need to make sure that A finishes before B and B before C. Anyone have good code examples for this? Should I use ops or assets?", "replies": ["sorry for the late night ping <@U016C4E5CP8> but any chance you could offer guidance here (either tonight if youre around or early AM is fine) ? Im supposed to demo something tomorrow night and Im relying on dagster for orchestrating this.", "in my head, it would look something like this:\n```@asset\ndef asset1():\n    write to tableA\n\n@asset(wait_for_asset1_finish)\ndef asset2():\n    write to tableB\n\n@asset(wait_for_asset2_finish)\ndef asset3():\n    write to tableC\n\n\ntest_job = define_asset_job(name=\"testing\")\n\ndefs = Definitions(\n    assets=[asset1, asset2, asset3],\n    jobs=[test_job],\n)```", "in my head, it would look something like this:\n```@asset\ndef asset1():\n    write to tableA\n```\n```@asset\ndef asset2(asset1):\n    write to tableB\n```\n```@asset\ndef asset3(asset2):\n    write to tableC\n\n```\n```test_job = define_asset_job(name=\"testing\")\n```\n```defs = Definitions(\n    assets=[asset1, asset2, asset3],\n    jobs=[test_job],\n)```", "<https://docs.dagster.io/concepts/assets/software-defined-assets#assets-with-dependencies|https://docs.dagster.io/concepts/assets/software-defined-assets#assets-with-dependencies> has explanations and examples", "thanks!"], "is_dagster_empl": [false, false, true, true, false], "gpt4_replies_target": "You can set up your project using Dagster's assets. Define each task as an asset and specify dependencies by passing the dependent asset as an argument to the next asset. For example, `@asset def asset2(asset1): write to tableB`. You can then define a job that includes these assets. For more details and examples, refer to the Dagster documentation on assets with dependencies.", "dagster_empl_first_target": "in my head, it would look something like this:\n```@asset\ndef asset1():\n    write to tableA\n```\n```@asset\ndef asset2(asset1):\n    write to tableB\n```\n```@asset\ndef asset3(asset2):\n    write to tableC\n\n```\n```test_job = define_asset_job(name=\"testing\")\n```\n```defs = Definitions(\n    assets=[asset1, asset2, asset3],\n    jobs=[test_job],\n)```", "dagster_empl_last_target": "<https://docs.dagster.io/concepts/assets/software-defined-assets#assets-with-dependencies|https://docs.dagster.io/concepts/assets/software-defined-assets#assets-with-dependencies> has explanations and examples"}, {"question": "Hi,\n\nI have restarted my dagster instance on AWS, and I am getting the error:\nAttributeError: 'FieldInfo' object has no attribute 'discriminator'\n\nI haven't update any python version or dagster. Any ideas on what could be happening? Full error in the comments", "replies": ["docker run dagster_etl\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/dagster\", line 5, in &lt;module&gt;\n    from dagster.cli import main\n  File \"/usr/local/lib/python3.10/site-packages/dagster/__init__.py\", line 458, in &lt;module&gt;\n    from dagster._core.storage.fs_io_manager import (\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/storage/fs_io_manager.py\", line 129, in &lt;module&gt;\n    config_schema=<http://FilesystemIOManager.to|FilesystemIOManager.to>_config_schema(),\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_config/pythonic_config/__init__.py\", line 237, in to_config_schema\n    return DefinitionConfigSchema(infer_schema_from_config_class(cls))\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_config/pythonic_config/__init__.py\", line 1496, in infer_schema_from_config_class\n    fields[pydantic_field.alias] = _convert_pydantic_field(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_config/pythonic_config/__init__.py\", line 1242, in _convert_pydantic_field\n    if pydantic_field.field_info.discriminator:\nAttributeError: 'FieldInfo' object has no attribute 'discriminator'", "Solved my issue by downgrading dagster version. Theres probably something broken in the newest one"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The error seems to be related to a potential issue in the latest version of Dagster. Downgrading Dagster version has resolved the issue for the user.", "dagster_empl_first_target": "docker run dagster_etl\n\nTraceback (most recent call last):\n  File \"/usr/local/bin/dagster\", line 5, in &lt;module&gt;\n    from dagster.cli import main\n  File \"/usr/local/lib/python3.10/site-packages/dagster/__init__.py\", line 458, in &lt;module&gt;\n    from dagster._core.storage.fs_io_manager import (\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/storage/fs_io_manager.py\", line 129, in &lt;module&gt;\n    config_schema=<http://FilesystemIOManager.to|FilesystemIOManager.to>_config_schema(),\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_config/pythonic_config/__init__.py\", line 237, in to_config_schema\n    return DefinitionConfigSchema(infer_schema_from_config_class(cls))\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_config/pythonic_config/__init__.py\", line 1496, in infer_schema_from_config_class\n    fields[pydantic_field.alias] = _convert_pydantic_field(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_config/pythonic_config/__init__.py\", line 1242, in _convert_pydantic_field\n    if pydantic_field.field_info.discriminator:\nAttributeError: 'FieldInfo' object has no attribute 'discriminator'", "dagster_empl_last_target": "Solved my issue by downgrading dagster version. Theres probably something broken in the newest one"}, {"question": "Is it possible to have an asset-sensor that is listening to multiple assets?", "replies": ["Wait i just found `multi_asset_sensor` my bad"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, it is possible to have an asset-sensor that listens to multiple assets by using `multi_asset_sensor`.", "dagster_empl_first_target": "Wait i just found `multi_asset_sensor` my bad", "dagster_empl_last_target": "Wait i just found `multi_asset_sensor` my bad"}, {"question": "Hi all, I am trying to use IO Manager to load from and save to a PostgreSQL database.\n```import pandas as pd\n\nfrom dagster import IOManager, InputContext, OutputContext\n\nclass MyIOManager(IOManager):\n    def handle_output(self, context: OutputContext, obj: pd.DataFrame) -&gt; None:\n        # Push it into a PostgreSQL database\n        # Save its returning index to a file\n        pass\n\n    def load_input(self, context: InputContext) -&gt; pd.DataFrame:\n        # Read the index from the file\n        # Pull from PostgreSQL database using the index\n        pass```\nHowever, saving to a file and read from it does not seem right to me. Can someone shed some lights on me if there is a better way to do this?\n\nThank you in advance. :slightly_smiling_face:", "replies": ["Maybe you can take some inspiration from the Snowflake IO Manager: <https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/resources/snowflake_io_manager.py#L46>", "We have this setup:\n\u2022 Source assets from Postgres using a db_io (based on the snowflake)\n\u2022 downstream assets do something, \n    \u25e6 take source asset as input\n        \u25aa\ufe0e this triggers load_input in the db_io which contains the SQL SELECT\n    \u25e6 Write to file using fs_io\n\u2022 further downstream have storage of the result\n    \u25e6 Use a db_io again\n        \u25aa\ufe0e handle_output uses pandas to_sql to write the data back to PostGres\nSo, you need two IOmanagers, one for files and one for DB tables", "Hi <@U03RP112CAJ>, the example you shared seems to be using timeframe to load_input. For PostgreSQL, I believe unless we inject timestamp into the data, we cannot pull in this way. Now, to pull and push from the database, I am thinking that I can add a column to store the run_id. However, in your opinion, is there any other way which I do not need to modify the table structure?", "Hi <@U04CJTR1TFS>, my apologies for my confusing sentences. My problem is mainly with using IO Manager for storing the data I have processed into a PostgreSQL DB and *load back the exact data for downstream*. The loading part is the part where I am stucked. When writing my own IO Manager, I am unable to pass any information from handle_output to load_input, so I am unable to get any IDs to retrieve back the data from database in the load_input function. Hopefully, this can clear up my questions.", "Do I understand correctly that you don\u2019t have any date/time information in your table? The example passes the partition information into the select statement, thus filtering the query; it\u2019s nothing to do with the run id or run timestamp.", "<@U03RP112CAJ> Yes. The table which I am using do not have any date time information.\n\nI saw that the time_window is gotten from the partition information. After that, they use this time_windows to query from the db.\n\nSo I believe since my table has no date time information, I have to add some sort of information to track \"what have I processed\". Either I can add a DateTime column for my data or I am thinking to add a run_id column with context.get_identifier()?", "1. You can log your storage information (like the table name) as metadata with context.add_output_metadata in handle_output and read it in load_input. This metadata can point to the actual table. \n2. You can make your own table in postgres specifically for your IOManager and store the information there. \n(1) is basically a better (2) \n\n3. Have a deterministic function that extracts this information from the context (using asset_key, etc). You then can call it in both handle_output and load_input.", "It\u2019s not straightforward how to access the metadata in load_input. <https://github.com/dagster-io/dagster/issues/8521|This> should help. \n\nBy the way, <@U011CET83FG> , are there any plans to make this pattern more accessible? ", "<@U03JEQ2PRRV> (1) looks really promising! I will give it a try. Thank you. :slightly_smiling_face:"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "You can use context.add_output_metadata in handle_output to log your storage information and read it in load_input, or create a specific table in postgres for your IOManager to store the information. Alternatively, you can have a deterministic function that extracts this information from the context using asset_key, etc.", "dagster_empl_first_target": "Maybe you can take some inspiration from the Snowflake IO Manager: <https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/resources/snowflake_io_manager.py#L46>", "dagster_empl_last_target": "<@U03JEQ2PRRV> (1) looks really promising! I will give it a try. Thank you. :slightly_smiling_face:"}, {"question": "This message was deleted.", "replies": ["Have you looked at <https://docs.dagster.io/integrations/airbyte>? The change you'll need to make is to point host/port not at localhost, but at the host/port of the airbyte-webapp service. I'd recommend using internal cluster DNS. As an example, we use `airbyte-airbyte-webapp-svc.airbyte.svc.cluster.local`", "<@U03Q27RQY76> Yes, I have read the documentation,  I can connect to Airbyte using code . I just wanted to configure the helm values in such way that Airbyte would just use Dagster as an Orchestrator, so I could configure all source connector stuff from within Dagster"], "is_dagster_empl": [false, false], "gpt4_replies_target": "To configure Airbyte to use Dagster as an orchestrator, you need to point the host/port not at localhost, but at the host/port of the airbyte-webapp service, using internal cluster DNS, and configure all source connector stuff from within Dagster.", "dagster_empl_first_target": "Have you looked at <https://docs.dagster.io/integrations/airbyte>? The change you'll need to make is to point host/port not at localhost, but at the host/port of the airbyte-webapp service. I'd recommend using internal cluster DNS. As an example, we use `airbyte-airbyte-webapp-svc.airbyte.svc.cluster.local`", "dagster_empl_last_target": "<@U03Q27RQY76> Yes, I have read the documentation,  I can connect to Airbyte using code . I just wanted to configure the helm values in such way that Airbyte would just use Dagster as an Orchestrator, so I could configure all source connector stuff from within Dagster"}, {"question": "I have a similar question, is there a way to include those source asset in the same group?", "replies": ["<@U03S5FN5CMC> (cc <@U04Q39E7SKA>) - you just need to pass a custom `node_info_to_group_fn`- it can literally return a static string if you want everything in the same group", "Yes I am doing that but sources are not included into the group", "`load_assets_from_dbt_project` doesn't return any `SourceAsset` objects. if you want to put those source asset in a group, you'll need to create `SourceAsset` objects and assign the group to them", "Thanks But that it will not hide original source node right (gray square)? let me see if understand this: create \"sourceAsset\" and put the original source like upstream to this new one?", "Or I will have both not related", "Question once \"load_assets_from_dbt_project\" is executed I only have a  List[AssetsDefinition]. , should iterate over this and add the new \"SourceAsset\" ? what method of \"AssetDefinition\" can I use for that?", "backing up a bit, I'm not actually entirely sure where the gray squares are coming from.\n\nif you have a dbt project with a single model and a single dbt source, I'd expect it to look something like this, and model0 wouldn't apply to any group:", "are you explicitly defining assets that correspond to the dbt sources somewhere?", "Hi, I have a dbt project with 3 sources.  The \"Gray squares\" are just visual and depend of the option  you use to see the lineage.  EX:   if you click on the items using the left panel you will see them like your image, but if you use the option \"*View global asset lineage*\" you will see them like my example.\n\nThanks for your answer.  So sources can not  group.  Returning to your first answer of using an \"SourceAsset\".  Could you give a guide about how to do it?\n\nBecause the idea is load my repository by code. Let me explain my use case:\n\n1 - I have a service that is creating dbt projects on the fly and putting them into a single folder, So I don't know how many dbt projects could be  into this folder.\n2-  all the models and sources are unknow,  because all of them are create by other services\n\nSo I need to load all this in dagster and the \"nice to have\" will be have sources also include inside the group of models using \"node_info_to_group_fn\"", "&gt; Thanks for your answer. So sources can not group. Returning to your first answer of using an \"SourceAsset\". Could you give a guide about how to do it?\nSuppose one of your dbt sources has the key [\"marketing\", \"campaigns\"]\n\nYou can do\n\n```campaigns = SourceAsset(AssetKey([\"marketing\", \"campaigns\"]), group_name=\"whatever\")```", "Thanks works perfect"], "is_dagster_empl": [false, false, true, false, false, false, true, true, false, true, false], "gpt4_replies_target": "To include source assets in the same group, you need to create `SourceAsset` objects and assign the group to them. For example, if one of your dbt sources has the key [\"marketing\", \"campaigns\"], you can do `campaigns = SourceAsset(AssetKey([\"marketing\", \"campaigns\"]), group_name=\"whatever\")`.", "dagster_empl_first_target": "`load_assets_from_dbt_project` doesn't return any `SourceAsset` objects. if you want to put those source asset in a group, you'll need to create `SourceAsset` objects and assign the group to them", "dagster_empl_last_target": "&gt; Thanks for your answer. So sources can not group. Returning to your first answer of using an \"SourceAsset\". Could you give a guide about how to do it?\nSuppose one of your dbt sources has the key [\"marketing\", \"campaigns\"]\n\nYou can do\n\n```campaigns = SourceAsset(AssetKey([\"marketing\", \"campaigns\"]), group_name=\"whatever\")```"}, {"question": "Hi, how would I go about reading the tags of a job that ran previously, if for example I want to find out the run key of the previous run of said job, outside of a sensor execution context?", "replies": ["What I figured so far is that I can search for run records via\n```last_successful_run = context.instance.get_run_records(\n        filters=RunsFilter(\n            job_name=\"log_s3_dummy_job\",\n            statuses=[DagsterRunStatus.SUCCESS],\n        ),\n        order_by=\"create_timestamp\",)```", "But then I need to find out the run key or other tags for the runs in question", "Solved - I found the context.instance.get_runs that had all the info I needde", "glad you were able to figure this out!"], "is_dagster_empl": [false, false, false, true], "gpt4_replies_target": "You can retrieve the tags of a previously run job by using the `context.instance.get_runs` method, which provides all the information you need, including the run key and other tags.", "dagster_empl_first_target": "glad you were able to figure this out!", "dagster_empl_last_target": "glad you were able to figure this out!"}, {"question": "Is there a way to query the config of running and queued job runs? I'm setting up a sensor which can initiate job runs and would like the query to look something like\n```SELECT download_url from file_requests\nMINUS\nSELECT download_url from acquired_files\nMINUS\nSELECT download_url from in_process_and_queued_files```\nFor each record returned by this query I'd like to initiate a run request for a job (`job_A`) that requires `download_url` as config. `job_A` updates the `acquired_files` table but, depending on the file size and number of runs already queued, I'm concerned the `acquired_files` table will not always be updated in time for the next sensor evaluation tick which could result in initiating another `RunRequest` for a `download_url` that is already either in progress or in queue. Is there a way to handle this besides increasing the sensor tick to a higher value?", "replies": ["You can disregard this. Seems like I can use the `run_key` arg when initiating the `RunRequest` to handle this situation."], "is_dagster_empl": [false], "gpt4_replies_target": "You can handle this situation by using the `run_key` argument when initiating the `RunRequest`, which will prevent initiating another `RunRequest` for a `download_url` that is already in progress or in queue.", "dagster_empl_first_target": "You can disregard this. Seems like I can use the `run_key` arg when initiating the `RunRequest` to handle this situation.", "dagster_empl_last_target": "You can disregard this. Seems like I can use the `run_key` arg when initiating the `RunRequest` to handle this situation."}, {"question": "Hi,\nI'm getting this error while trying to use load_assets_from_package_name, load_assets_from_package_module or load_assets_from_modules:\n\n Member of iterable mismatches type. Expected (&lt;class 'dagster._core.definitions.assets.AssetsDefinition'&gt;, &lt;class 'dagster._core.definitions.source_asset.SourceAsset'&gt;, &lt;class 'dagster._core.definitions.cacheable_assets.CacheableAssetsDefinition'&gt;). Got [&lt;dagster._core.definitions.assets.AssetsDefinition object at 0x000002A9C57DBB50&gt;, &lt;dagster._core.definitions.assets.AssetsDefinition object a\n\nWhat can it be?", "replies": ["Hi <@U03MXV86UNS>! Can you share how you're creating your Definitions object? It looks like you might be doing\n```defs = Definitions(\n    assets=[load_assets_from_package_module(...)],\n)```\ninstead of\n```defs = Definitions(\n    assets=load_assets_from_package_module(...),\n)```\n", "you're gooddamn right! (read this with the voice of Heisenberg)\n\nI had something like this\n\n```my_assets_from_package = load_assets...\nassets = [asset_1, asset2, my_assets_from_package]\n\ndefs = Definitions(\n     assets=assets\n)```", "thanks!!", ":blob_salute:"], "is_dagster_empl": [true, false, false, true], "gpt4_replies_target": "The error occurs because you're passing an array to the 'assets' parameter in your Definitions object, instead of directly passing the result of the 'load_assets_from_package_module' function.", "dagster_empl_first_target": "Hi <@U03MXV86UNS>! Can you share how you're creating your Definitions object? It looks like you might be doing\n```defs = Definitions(\n    assets=[load_assets_from_package_module(...)],\n)```\ninstead of\n```defs = Definitions(\n    assets=load_assets_from_package_module(...),\n)```\n", "dagster_empl_last_target": ":blob_salute:"}, {"question": "Hey team, We are trying to execute Dagster jobs on Dask cluster. Our Dask cluster is running in a different container than the source code. We are currently facing issue while running job as it is not able to access source code and it's dependencies.\nOur Dask configurations are:\n```  dask_scheduler:\n    image: <http://ghcr.io/dask/dask:2023.5.1|ghcr.io/dask/dask:2023.5.1>\n    hostname: dask-scheduler\n    ports:\n      - \"8786:8786\"\n      - \"8787:8787\"\n    command: [\"dask\", \"scheduler\"]\n    env_file:\n      - ./boilerplate/variables.env\n    networks:\n      - dask_net\n\n  dask_worker:\n    image: <http://ghcr.io/dask/dask:2023.5.1|ghcr.io/dask/dask:2023.5.1>\n    hostname: dask-worker\n    command: [\"dask\", \"worker\", \"<tcp://dask_scheduler:8786>\"]\n    env_file:\n      - ./boilerplate/variables.env\n    networks:\n      - dask_net\n    depends_on:\n      - dask_scheduler```\nApplication config:\n```  docker_boilerplate_code:\n    build:\n      context: .\n      dockerfile: ./boilerplate/Dockerfile_boilerplate\n    container_name: boilerplate\n    image: boilerplate\n    restart: always\n    env_file:\n      - ./boilerplate/variables.env\n    volumes:\n      - ./boilerplate:/usr/src/app/boilerplate\n    networks:\n      - dask_net\n    depends_on:\n      - dask_scheduler\n      - dask_worker```\nWe were trying to run the below op:\n```from dask.distributed import Client\n\n@op()\ndef partition_sum():\n    client = Client()\n    A = client.map(square, range(10))\n    B = client.map(neg, A)\n    total = client.submit(sum, B)\n    return total.result()```\nAm I missing out something or is there a better way to run jobs in dask worker.\nCan anyone assist me with this ?", "replies": ["Hey Himanshu, I\u2019m not that familiar with Dask so I might need to call in someone else-- but it\u2019s not clear to me from the above whether you are using <https://docs.dagster.io/deployment/guides/dask|dagster-dask>. Are you?", "Just checking in, have you resolved your issue?", "I am going to mark this as resolved, but feel free to respond here if you are still having problems."], "is_dagster_empl": [true, true, true], "gpt4_replies_target": "It's unclear from the information provided whether you are using dagster-dask, which could be causing the issue with executing Dagster jobs on your Dask cluster.", "dagster_empl_first_target": "Hey Himanshu, I\u2019m not that familiar with Dask so I might need to call in someone else-- but it\u2019s not clear to me from the above whether you are using <https://docs.dagster.io/deployment/guides/dask|dagster-dask>. Are you?", "dagster_empl_last_target": "I am going to mark this as resolved, but feel free to respond here if you are still having problems."}, {"question": "Hey Dagster team - we just updated from the trial version to the standard plan (yay!) on Dagster Cloud. However, the whole Dagster instance seems to be stuck / not-usable. It looks like the serverless deployment failed (it\u2019s been in this state for 3+ hours). Anyway we/you can kick it?", "replies": ["hey Kunal can share your organization id with me?", "Hi Kunal\u2026 I just restarted your agent.  Let me know if you see any persistent issues"], "is_dagster_empl": [true, true], "gpt4_replies_target": "The Dagster team has restarted your agent and requests you to share your organization id if the issue persists.", "dagster_empl_first_target": "hey Kunal can share your organization id with me?", "dagster_empl_last_target": "Hi Kunal\u2026 I just restarted your agent.  Let me know if you see any persistent issues"}, {"question": "Is there currently a way to have the auto-materialization daemon be running by default?", "replies": ["Hi <@U050TN4M924>! this is not currently possible, although once you turn it on for the first time locally I would expect it to stay on", "Cheers Owen! Yes, it does seem to reliably stay turned on at least :)"], "is_dagster_empl": [true, false], "gpt4_replies_target": "No, there isn't currently a way to have the auto-materialization daemon running by default, but once turned on for the first time, it should reliably stay on.", "dagster_empl_first_target": "Hi <@U050TN4M924>! this is not currently possible, although once you turn it on for the first time locally I would expect it to stay on", "dagster_empl_last_target": "Hi <@U050TN4M924>! this is not currently possible, although once you turn it on for the first time locally I would expect it to stay on"}, {"question": "I'm using dagster-gcp, and bigquery, and my numerical data is loading into bigquery as nulls. My pandas dataframe has the right numerical types, so do I need to specify the bigquery schema in some way? The example here <https://docs.dagster.io/integrations/bigquery/using-bigquery-with-dagster> does not. Should I not convert the data to numerical types?", "replies": ["oh - I think this is my issue: <https://github.com/googleapis/python-bigquery/issues/1566>", "yup, that was it! spaces in bigquery column names will load without error, but the data will be loaded as null!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue of numerical data loading as nulls in BigQuery when using dagster-gcp could be due to spaces in BigQuery column names; removing these spaces should resolve the issue.", "dagster_empl_first_target": "oh - I think this is my issue: <https://github.com/googleapis/python-bigquery/issues/1566>", "dagster_empl_last_target": "yup, that was it! spaces in bigquery column names will load without error, but the data will be loaded as null!"}, {"question": "How can I make an optional op config schema field?", "replies": ["You can use `is_required=False` or make it `Noneable`:\n\n```from dagster import Field, Noneable, op, OpExecutionContext\n\n@op(\n  config_schema={\"optional_field\": Field(str, is_required=False,\n                 \"noneable_field\": Field(Noneable(str), is_required=True, default_value=None)}\n)\ndef something(context: OpExecutionContext):\n    context.op_config[\"optional_field\"] # will throw KeyError if optional_field not included in run config\n    assert context.op_config[\"noneable_field\"] == None```", "Cool thanks! Didn't know I could do fields. Was just using standard typing and ended up settling on `Noneable`", "if you\u2019re using the newer Pythonic/Pydantic config system, you would do it like this\n\n```from pydantic import Field \nfrom dagster import Config \nfrom typing import Optional\n\nclass MyConfig(Config):\n     optional_field: Optional[str] = Field(default=None)```\nnote that here `Field` is imported from `pydantic` not `dagster`", "Cool. I have never used Pydantic. Is that the preferred way of making standard configs?", "Pydantic config was fully released in 1.3 and is now our recommended approach, but the `config_schema` dictionaries are still fully supported, so if that works better for your use case feel free to use it!", "What Zach wrote will totally work, just wanted to include the Pydantic method since there are two ways you could do this\n\nfwiw with Pydantic config you provide it to the asset like this\n\n```from pydantic import Field \nfrom dagster import Config, asset\nfrom typing import Optional\n\nclass MyConfig(Config):\n     optional_field: Optional[str] = Field(default=None)\n\n@asset\ndef my_asset(config: MyConfig):\n     if config.optional_field is not None:\n         ...```", "Is that a key word arg? If we wanted the context too, does order matter?", "yeah to pipe the config through correctly, the parameter has to be named `config` and have the right type annotation. You can use the context too!\n\n```@asset\ndef my_asset(context, config: MyConfig):\n     if config.optional_field is not None:\n         ...```\nwould be totally fine\n\nI don\u2019t think the order matters (worth verifying with a small sample asset though)", "relevant docs for ya <https://docs.dagster.io/concepts/configuration/config-schema>", "Thanks! will take a look", "<@U030M2AL48M> how would this look if I had this config for an op and wanted a job to pass through the config to the op?", "If you want to specify config at execution time, this is the relevant docs section <https://docs.dagster.io/concepts/configuration/config-schema#specifying-runtime-configuration>. If you want to hard-code some configuration you can do something like this\n```from pydantic import Field\nfrom dagster import Config, job, op, RunConfig\nfrom typing import Optional\n\nclass MyConfig(Config):\n     optional_field: Optional[str] = Field(default=None)\n\n@op\ndef my_op(config: MyConfig):\n     if config.optional_field is not None:\n          ...\n\n@job(\n    config=RunConfig(ops={\"op_name\": MyConfig(optional_field=\"foo\")})\n)\ndef my_job():\n     my_op()```", "If I don't know the run config values and want to use dagit to execute, do I need to specify the config, or just pass through the config in the launchpad?", "you can just specify the config in the launchpad <https://docs.dagster.io/concepts/configuration/config-schema#dagit>", "got it, thanks. I wasn't sure if I also had to specify in the code", "I am trying to materialize an asset from an op in a job. That asset has a config associated to it, but when I try reloading my code location I get a TypeError `asset_1() missing 1 required positional argument: 'config'` . I am using the `materialize()` function in my op", "`materialize` should generally just be used for testing assets (ie in unit tests or integration tests) and not called from within ops. What is the larger goal of materializing the asset within the op? there is likely another way to accomplish the goal", "We currently use SOLR as a cache that we write to from Dagster. In order to update the schema of this I originally created a job with ops to complete the schema updates. This leaves residual objects in our SOLR cache, that can be used as a backup. Eventually, after an independent data review, I want to cleanup those residual objects. I thought Dynamic Partitions would be good for this, because I can cleanup by partition_key with a downstream asset or job, while keeping those update objects separated. The way I create the partition_key is to use a job `run_config` that will make some api calls and get the partition key, and then materialize the asset with that partition_key", "ok, i think i see. My recommendation in this case would be to have the job that makes the partition key, and then maybe a run_status_sensor that materializes the assets when that job completes. It\u2019s a bit of indirection, but i think that should accomplish what you want", "How would the sensor get the partition key from job output?", "Now I am just thinking have the job add a dynamic partition key, then the user can just materialize that partition key manually. But that is kind of what I am doing now, just calling `materialize` instead of someone manually using the UI to materialize a partition. It just creates an ephemeral run, not sure of the implications that has, but I think an additional thread is held because the Op that calls materialize stays as running while the asset is materializing. That is the only real downside.  I could change it to async, but maybe for one-off uses this is ok", "the other thing I tried using was the graphql client, but I didn't see how to add a partition_key to a graphapi submit_job_execution"], "is_dagster_empl": [false, false, true, false, true, true, false, true, true, false, false, true, false, true, false, false, true, false, true, false, false, false], "gpt4_replies_target": "To make an optional op config schema field in Dagster, you can use `is_required=False` or make it `Noneable`. If you're using the newer Pythonic/Pydantic config system, you can use `Optional[str] = Field(default=None)`. The parameter has to be named `config` and have the right type annotation to pipe the config through correctly.", "dagster_empl_first_target": "if you\u2019re using the newer Pythonic/Pydantic config system, you would do it like this\n\n```from pydantic import Field \nfrom dagster import Config \nfrom typing import Optional\n\nclass MyConfig(Config):\n     optional_field: Optional[str] = Field(default=None)```\nnote that here `Field` is imported from `pydantic` not `dagster`", "dagster_empl_last_target": "ok, i think i see. My recommendation in this case would be to have the job that makes the partition key, and then maybe a run_status_sensor that materializes the assets when that job completes. It\u2019s a bit of indirection, but i think that should accomplish what you want"}, {"question": "Hi, is it possible to store different versions of the calculated asset output and configure a job to use an older output version for downstream assets? I've looked into the provided io managers but didn't find something suitable. In my use case I want to train a ml-model and archive the different model versions. Later, I want be able to use older model versions for prediction in my job.", "replies": ["You could probably do something smart with partitions :slightly_smiling_face:", "Hi Tobias, We don\u2019t have this kind of \u201casset history\u201d functionality baked in (yet), but I think that you might be able to use the experimental <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#dynamically-partitioned-assets|Dynamic partitions API> to achieve what you\u2019re after.", "Thanks, I will give it a try"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "While Dagster doesn't have built-in \"asset history\" functionality, you might be able to use the experimental Dynamic partitions API to store different versions of the calculated asset output and configure a job to use an older output version for downstream assets.", "dagster_empl_first_target": "Hi Tobias, We don\u2019t have this kind of \u201casset history\u201d functionality baked in (yet), but I think that you might be able to use the experimental <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#dynamically-partitioned-assets|Dynamic partitions API> to achieve what you\u2019re after.", "dagster_empl_last_target": "Hi Tobias, We don\u2019t have this kind of \u201casset history\u201d functionality baked in (yet), but I think that you might be able to use the experimental <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#dynamically-partitioned-assets|Dynamic partitions API> to achieve what you\u2019re after."}, {"question": "Hello, I have a task to do for a number of \"accounts\". Let's say, retrieve data A, data B, merge them, and write to a DB. Now what would be the idiomatic way to do this? Would it be a \"for\" loop in the affected nodes (iterating over the accounts) or is there a better way, allowing me to track the assets and metrics of the accounts separately?", "replies": ["What about using partitioned assets where each partition corresponds to an account?", "This is exactly what I need! I was looking more into the \"dynamic graph\" direction, but what you propose seems to be the best start for me. Thank you very much for the hint!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The idiomatic way to handle this task in Dagster would be to use partitioned assets, where each partition corresponds to an account, allowing you to track the assets and metrics of the accounts separately.", "dagster_empl_first_target": "What about using partitioned assets where each partition corresponds to an account?", "dagster_empl_last_target": "This is exactly what I need! I was looking more into the \"dynamic graph\" direction, but what you propose seems to be the best start for me. Thank you very much for the hint!"}, {"question": "Hi All!\nCan I have both monthly and daily partition assets for one job?\nI am getting this error:\n```Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: ```\n and how can i bind this  in define_asset_job.", "replies": ["Hi Shailesh, You can\u2019t have these in the same job, because a job needs a single partitions definition. But you can materialize differently partitioned assets with separate jobs and these assets can depend on each other via partition mappings."], "is_dagster_empl": [true], "gpt4_replies_target": "You cannot have both monthly and daily partition assets in the same job as a job requires a single partition definition. However, you can materialize differently partitioned assets with separate jobs and these assets can depend on each other via partition mappings.", "dagster_empl_first_target": "Hi Shailesh, You can\u2019t have these in the same job, because a job needs a single partitions definition. But you can materialize differently partitioned assets with separate jobs and these assets can depend on each other via partition mappings.", "dagster_empl_last_target": "Hi Shailesh, You can\u2019t have these in the same job, because a job needs a single partitions definition. But you can materialize differently partitioned assets with separate jobs and these assets can depend on each other via partition mappings."}, {"question": "Hi,\n\nI'm a beginner when it comes to this, so apologies if this question is a bit trivial/obvious - but I'm having a bit of trouble making the scheduler work. I won't copy the entire code below but essentially it's a football scraper -&gt; Twitter bot. And I essentially want it to run the first op at a certain time automatically to get today's fixtures, which will then feed into the subsequent operations.\n\n`@op(`\n        `out= {`\n                `\"data\" : DynamicOut(),` \n        `}`\n`)`\n`def get Stuff():`\n    `# Scrape stuff`\n    `for idx, row in enumerate(table_rows):`\n          `# Just feed in url here and say if available for certain date (today -&gt; then yield)`\n          `yield DynamicOutput((time, url), output_name = \"data\", mapping_key=f\"data_{idx}\")`\n`@op` \n`def do_Stuff(url):`\n    `# Just generate datasets then plots from url and time determines when we do so we got above`\n\n`@job`\n`def materialize_Stuff():`\n    `time, url = get_Info()`\n    `url.map(do_Stuff)`\n\n`basic_schedule = ScheduleDefinition(job=materialize_view, cron_schedule=\"0 9 * * *\")`\n\nI thought I could simply run the scheduler using that last line but seemingly not. I can see from the tutorials (<https://dagster-git-docs-tutorialscheduling-elementl.vercel.app/tutorial/scheduling-your-pipeline>) that they change the init file but not sure if that's relevant here (since using ops/jobs). So not really sure what to do -  any help would really be appreciated.\n\nThanks,\nJoe", "replies": ["Hi Joe, Defining a `ScheduleDefinition` or (any dagster definition) in Python is not enough by itself to execute anything-- you need to package your definitions into a `Definitions` object and run dagit against it (as in the tutorial).", "Ah, thanks Sean. Think I was getting a bit confused since it was for asset jobs and not sure what the equivalent would be. Presumably just adding this to the end of the code will suffice (works fine by the looks of it but just checking I'm not missing anything)\n`basic_schedule = ScheduleDefinition(job=materialize_view,` \n                  `cron_schedule=\"15 18 * * *\",                               )`\n\n`defs = Definitions(`\n    `schedules=[basic_schedule],`\n    `jobs=[materialize_view]`\n`)`                                                                                                                                                 Thanks again for your help"], "is_dagster_empl": [true, false], "gpt4_replies_target": "To schedule a job in Dagster, you need to package your definitions into a `Definitions` object and run dagit against it, as simply defining a `ScheduleDefinition` is not enough to execute anything.", "dagster_empl_first_target": "Hi Joe, Defining a `ScheduleDefinition` or (any dagster definition) in Python is not enough by itself to execute anything-- you need to package your definitions into a `Definitions` object and run dagit against it (as in the tutorial).", "dagster_empl_last_target": "Hi Joe, Defining a `ScheduleDefinition` or (any dagster definition) in Python is not enough by itself to execute anything-- you need to package your definitions into a `Definitions` object and run dagit against it (as in the tutorial)."}, {"question": "Hello!\n\nI tried to update our way of defining a <https://docs.dagster.io/concepts/code-locations#defining-code-locations|code location> using Definitions instead of repository. I have a module name `pipelines` with:\n\u2022 one asset (define as decorated function), \n\u2022 one job (defined through `define_asset_job`), \n\u2022 one sensor triggering the asset (defined as decorated function), \n\u2022 and an `__init__.py` at the module root containing the following code:\n```from dagster import Definitions\nfrom pipelines.core.resources.my_resource import my_resource\nfrom pipelines.my_folder.job import my_job\nfrom pipelines.my_folder.sensors import my_sensor\nfrom pipelines.my_folder.assets import my_asset\n\ndefs = Definitions(\n    assets=[my_asset],\n    schedules=[],\n    jobs=[my_job],\n    sensors=[my_sensor],\n    resources={'my_resource': my_resource}\n)```\nI then start `dagster dev -m my_module`\n\nEverything works correctly and seeing the log I can say that my sensor and job are being triggered. But there are numerous pages inside dagit that contains errors looking like the ones attached.\nDo you know what could I have done wrong ? Is it a known bug (I have seen another person mention the same a month ago) ?\n\nThank you by advance and have a nice day!", "replies": ["The error:", "Also I do see my module being loaded properly:", "For anyone passing by here, I found the issue. If you name your package `pipelines`, there\u2019s an error in the front end.\n\nDagit URLs look like this\n```<http://127.0.0.1:3000/locations/>&lt;module_name&gt;/jobs/&lt;job_name&gt;```\nIf your module is named `pipelines` , legacy code <https://github.com/dagster-io/dagster/blob/f043765958c8d2a00877eb48fe2441b3027b9452/js_modules/dagit/packages/core/src/workspace/WorkspacePipelineRoot.tsx#L33|here>, will try to find a \u201cpipeline\u201d called \u201cjobs\u201d which probably doesn\u2019t exists, which gives you this error.\nJust rename your package :slightly_smiling_face:", "<https://github.com/dagster-io/dagster/issues/13258>", "Thanks for finding this and filing an issue :pray:"], "is_dagster_empl": [false, false, false, false, true], "gpt4_replies_target": "The error you're experiencing is due to naming your package `pipelines`. There's a conflict with Dagit URLs and legacy code which causes the system to look for a \u201cpipeline\u201d called \u201cjobs\u201d that doesn't exist. To resolve this, simply rename your package.", "dagster_empl_first_target": "Thanks for finding this and filing an issue :pray:", "dagster_empl_last_target": "Thanks for finding this and filing an issue :pray:"}, {"question": "hi team - would anyone know if there have been any recent changes to how context.pdb.set_trace() works? on older versions of dagster, im _fairly_ confident i was able to properly set breakpoints in local dagit instances using that line before, but now i immediately get the following error:\n```The above exception was caused by the following exception:\nbdb.BdbQuit\n  File \"/home/david.tong/miniconda3/envs/dagster3/lib/python3.8/site-packages/dagster/_core/execution/plan/utils.py\", line 54, in op_execution_error_boundary\n    yield\n  File \"/home/david.tong/miniconda3/envs/dagster3/lib/python3.8/site-packages/dagster/_utils/__init__.py\", line 439, in iterate_with_context\n    next_output = next(iterator)\n  File \"/home/david.tong/miniconda3/envs/dagster3/lib/python3.8/site-packages/dagster/_core/execution/plan/compute_generator.py\", line 122, in _coerce_solid_compute_fn_to_iterator\n    result = invoke_compute_fn(\n  File \"/home/david.tong/miniconda3/envs/dagster3/lib/python3.8/site-packages/dagster/_core/execution/plan/compute_generator.py\", line 116, in invoke_compute_fn\n    return fn(context, **args_to_pass) if context_arg_provided else fn(**args_to_pass)\n  File \"/home/david.tong/repos/dagster_workflows/dagster_workflows/dagster_modules/some_job/ops/new_op.py\", line 25, in run_some_job_large\n    config = load_yaml(\"/home/david.tong/repos/dagster_workflows/nav_config.yaml\")\n  File \"/home/david.tong/repos/dagster_workflows/dagster_workflows/dagster_modules/some_job/ops/new_op.py\", line 25, in run_some_job_large\n    config = load_yaml(\"/home/david.tong/repos/dagster_workflows/nav_config.yaml\")\n  File \"/home/david.tong/miniconda3/envs/dagster3/lib/python3.8/bdb.py\", line 88, in trace_dispatch\n    return self.dispatch_line(frame)\n  File \"/home/david.tong/miniconda3/envs/dagster3/lib/python3.8/bdb.py\", line 113, in dispatch_line\n    if self.quitting: raise BdbQuit```\n", "replies": ["Hi David - I think I know the change that likely caused this: <https://github.com/dagster-io/dagster/pull/13099>\n\nThis is a tricky one because there are certain packages that hang if you try to import them and stdin isn't null - but that may now be interfering with pdb. We'll see if we can come up with a solution that satisfies both requirements", "got it - so for now, should we consider pdb non-functional until a fix is released, or might there be any workarounds?", "I would expect it to work in CLI calls like `dagster job execute` (or if you run the code in your own grpc server via `dagster api grpc`: <https://docs.dagster.io/concepts/code-locations/workspace-files#running-your-own-grpc-server>) I would also expect it to still work on the previous release (1.2.3)", "The problem only occurs when dagit or dagster dev is the one spinning up a subprocess", "nice, yea can confirm `context.pdb` w/ `dagster job execute` looks functional on 1.2.4. should do for now, thanks!", "Just a heads up that we believe we fixed this issue in 1.2.6 that went out this week, thanks for reporting it", "Still occurring for me, <@U016C4E5CP8>, with 1.2.6 on osx", "Ah yes, should have updated this thread - I think we have actually fixed this for real in <https://github.com/dagster-io/dagster/pull/13525> which is going out in the release today or tomorrow (1.2.7)", "thanks for the heads up!", "great branch name", "Hey guys. I just had the same problem. I'm wondering if this is still supported?\n\n<https://docs.dagster.io/_apidocs/utilities#dagster._utils.forked_pdb.ForkedPdb>", "It's still supported - should be working again like it was before after the 1.2.7 release that's rolling out right now", "I'm still encountering with 1.2.7 on OSX (Py 3.10.3)", "Can you share your repro steps / was it working on past versions?", "oh, hm, that change does not seem to have actually made it into 1.2.7! Sorry for the confusion, that was an oversight on our part", ":cry:", "In the meantime - \"dagster job execute\" should still work, as should running the code in your own grpc server with \"dagster api grpc\"", "that's alright", "Is there any way to specify ops from `dagster job execute`? (i'm re-running an expensive computation repeatedly before i can debug the downstream asset)", "`dagster asset materialize` should work too, right?", "Yeah, the problem only occurs with dagit / dagster dev", "By the way, are there any docs for using PyCharm's debugger with Dagster? I think there should be a way. I did it with Airflow once, but don't remember the details.", "That one may be worth a separate post - i'm not sure about docs, but i believe somebody on the team will know", "Yeah, I think it will by incredibly valuable.\nI'll spend some time trying to set this up next week", "<@U016C4E5CP8> I haven't tested this rigorously, but pdb does appear to be working on asset materializations originated from Dagit.", "I'm not sure if its intermittent, but either way I appreciate you and the fix", "Although I may have spoken too soon \u2013 the read prompt appeared in my terminal but i'm not sure if my input was evaluated and no output was printed.", "Yeah the issue was related to passing input between subprocesses - the fix is in master and will definitely 100% be in the next release"], "is_dagster_empl": [true, false, true, true, false, true, false, true, false, false, false, true, false, true, true, false, true, false, false, false, true, false, true, false, false, false, false, true], "gpt4_replies_target": "The issue with context.pdb.set_trace() in Dagster is known and a fix has been implemented in the master branch and will be included in the next release. In the meantime, it should still work with \"dagster job execute\" and running the code in your own grpc server with \"dagster api grpc\".", "dagster_empl_first_target": "Hi David - I think I know the change that likely caused this: <https://github.com/dagster-io/dagster/pull/13099>\n\nThis is a tricky one because there are certain packages that hang if you try to import them and stdin isn't null - but that may now be interfering with pdb. We'll see if we can come up with a solution that satisfies both requirements", "dagster_empl_last_target": "Yeah the issue was related to passing input between subprocesses - the fix is in master and will definitely 100% be in the next release"}, {"question": "I have one `@asset` that periodically loads data to BigQuery but I'd like to run it three times based on these inputs `[\"one\", \"two\", \"three\"]` - what's the best way to do this without duplicating code?", "replies": ["Sounds like maybe you should use a partitioned asset? <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#defining-partitioned-assets>", "I currently have an asset that fetches some data and uploads it to BigQuery using a `BigQueryDataframeIOManager` - it fetches daily data and also runs on a daily cron schedule.\n\nUsing exactly the same code inside this `@asset` function I'd also like to run it on an hourly and minutely cron schedule, aggregating data at the respective resolution.\n\nSo to summarise, in total I'm trying to update three BigQuery table:\n1. `bigquery_minutely_table` updated every minute and aggregated on a minute timeframe\n2. `bigquery_hourly_table` updated every hour and aggregated on an hourly timeframe\n3.  `bigquery_daily_table` updated every day and aggregated on a daily timeframe\nIs this possible?", "I'm still wrapping my head around your docs but it looks like I could define my timeframes inside a `config.yaml`  to pass in some <https://docs.dagster.io/concepts/configuration/config-schema#run-configuration|run configurations>.<https://docs.dagster.io/concepts/configuration/config-schema#run-configuration| >", "<@U01GTMVMGQH> would really appreciate your guidance as to how the above is best achieved - thanks!", "IIUC  I think you want three separate partitioned assets, since you\u2019re generating results at 3 different granularities, maybe something like this:\n\n```@asset(partitions_def=HourlyPartitionsDefinition...)\ndef hourly_asset():\n    shared_code()\n\n@asset(partitions_def=DailyPartitionsDefinition...)\ndef daily_asset():\n    shared_code()\n\n@asset(partitions_def=TimeWindowPartitionsDefinition(..., cron_schedule=\"* * * * *\", ...)\ndef minutely_asset():\n    shared_code()```", "This is really helpful, I'll revert if I have any Q's"], "is_dagster_empl": [true, false, false, false, true, false], "gpt4_replies_target": "You can achieve this by creating three separate partitioned assets for each granularity (minute, hour, day) using the `@asset` decorator with the respective `PartitionsDefinition`, and then call the shared code within each asset.", "dagster_empl_first_target": "Sounds like maybe you should use a partitioned asset? <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#defining-partitioned-assets>", "dagster_empl_last_target": "IIUC  I think you want three separate partitioned assets, since you\u2019re generating results at 3 different granularities, maybe something like this:\n\n```@asset(partitions_def=HourlyPartitionsDefinition...)\ndef hourly_asset():\n    shared_code()\n\n@asset(partitions_def=DailyPartitionsDefinition...)\ndef daily_asset():\n    shared_code()\n\n@asset(partitions_def=TimeWindowPartitionsDefinition(..., cron_schedule=\"* * * * *\", ...)\ndef minutely_asset():\n    shared_code()```"}, {"question": "is there, or should there be a channel for new users just learning dagster? (maybe even specifically new to data-orchestration in general) I see channels about migrating from other platforms, but what about folks like me, migrating from nothing? If I just haven't found a channel that already exists, I'll be glad to learn about it!", "replies": ["Hi Nate, We don\u2019t have a dedicated channel for that at this time, but it\u2019s something we will consider as we expand our developer education and outreach efforts. For now you can post in this channel for things you can\u2019t determine from the docs.", "thanks!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, there isn't a dedicated channel for new users just learning dagster, but you can post any queries in the existing channel if you can't find answers from the documentation.", "dagster_empl_first_target": "Hi Nate, We don\u2019t have a dedicated channel for that at this time, but it\u2019s something we will consider as we expand our developer education and outreach efforts. For now you can post in this channel for things you can\u2019t determine from the docs.", "dagster_empl_last_target": "Hi Nate, We don\u2019t have a dedicated channel for that at this time, but it\u2019s something we will consider as we expand our developer education and outreach efforts. For now you can post in this channel for things you can\u2019t determine from the docs."}, {"question": "Hi, I'm trying to turn a normal asset that's been a bottleneck into a partitioned asset to improve performance. The problem is this asset depends on multiple non-partitioned upstream assets. When I try to run the job, it's launching runs for each partition (as expected), but each of these runs seems to be independently generating all of the upstream assets and blowing up resource usage. What I want is each upstream asset to just be created once, then used by all of the partitions. Is there a way to achieve this behavior that I haven't been able to figure out, or should I be using a different pattern here?\n\nI've also considered creating a factory function to create individual assets for each partition. There's not too many partitions, so this seems like probably a fine solution, but I wanted to see if I could get the built-in asset partition functionality working as desired first.", "replies": ["Hi Zach, Each run is scoped to at most one partition. We are working to get asset memoization/staleness working with partitions, but for now your best bet is to materialize the non-partitioned assets in one run and then use a separate asset job to launch-partition-scoped runs (they should still be able to load the unpartitioned upstream inputs even if these assets are not included in the run)", "Ah ok, that makes sense, thanks!", "Thanks <@U01GTMVMGQH>! Is there a github issue or discussion we could contribute to?", "This discussion does not exactly match the topic, but it\u2019s a reasonable place to post any thoughts on staleness: <https://github.com/dagster-io/dagster/discussions/13102>\n\nFeel free to be as expansive as you like in your wishlist! Btw I am point-person on data versions and staleness.", "I\u2019ve tried to explain our use case and prior expectations about how it would work in this comment: <https://github.com/dagster-io/dagster/discussions/13102#discussioncomment-5548988>"], "is_dagster_empl": [true, false, false, true, false], "gpt4_replies_target": "To manage resource usage with partitioned assets that depend on non-partitioned upstream assets, you should materialize the non-partitioned assets in one run and then use a separate asset job to launch partition-scoped runs. This will allow each partition to load the unpartitioned upstream inputs without having to regenerate them each time.", "dagster_empl_first_target": "Hi Zach, Each run is scoped to at most one partition. We are working to get asset memoization/staleness working with partitions, but for now your best bet is to materialize the non-partitioned assets in one run and then use a separate asset job to launch-partition-scoped runs (they should still be able to load the unpartitioned upstream inputs even if these assets are not included in the run)", "dagster_empl_last_target": "This discussion does not exactly match the topic, but it\u2019s a reasonable place to post any thoughts on staleness: <https://github.com/dagster-io/dagster/discussions/13102>\n\nFeel free to be as expansive as you like in your wishlist! Btw I am point-person on data versions and staleness."}, {"question": "In the dagster free trial, is there a way to see what I _would_ be getting billed if I weren't in the trial period?", "replies": ["You can click on the cloud setting page under their profile icon, then click on Usage. This will give you a number to use with our list pricing for self serve.\n\nYou can also reach out to sales for more info.", "cool, thanks"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Yes, you can see what you would be getting billed by clicking on the cloud setting page under your profile icon, then clicking on Usage. You can also contact sales for more information.", "dagster_empl_first_target": "You can click on the cloud setting page under their profile icon, then click on Usage. This will give you a number to use with our list pricing for self serve.\n\nYou can also reach out to sales for more info.", "dagster_empl_last_target": "You can click on the cloud setting page under their profile icon, then click on Usage. This will give you a number to use with our list pricing for self serve.\n\nYou can also reach out to sales for more info."}, {"question": "Hey folks, is there way to programmatically delete a branch deployment on dagster cloud? Need to be able to clean up dagster cloud when a github PR is merged. Thanks in advance!", "replies": ["Hi Vivek, the GitHub Actions for branch deployments should mark the branch as \u201cclosed\u201d when a PR is merged/closed. That will hide it from the UI (&amp; branch deployment agent infra spins down automatically after a timeout). Are you interested in explicitly deleting? Archiving is our default to keep e.g. run history around", "Thanks for the reply Ben, based on what I can see, it looks like there a bug in hiding closed and merged branches. If it spins the agent, I think that\u2019s fine, just want to have a view where it\u2019s not spammy with old PRs", "Do the branches show up as closed or merged in the UI? It may be an error in marking them as closed/merged on the GH Actions end.", "Open branches will have this green icon", "I might have to update my deployment agents to the latest version", "right now if I toggle `Show closed and merged branches` off, I still see some that were merged.", "Do they have the merged/closed icon? Just trying to determine where the error might be happening (e.g. they aren\u2019t being marked merged/closed or they aren\u2019t being hidden)", "", "here\u2019s what I see now", "Hmm, yeah, it looks like we aren\u2019t marking them as closed. Are you using the GitHub Action to create branch deployments?", "yup using the action", "specifically <https://github.com/dagster-io/dagster-cloud-action/blob/main/actions/hybrid_branch_deploy/action.yml>", "If you can file a github issue for it, I can track the progress there, Thanks <@U0291UAGGCB>!", "I can do that! Do you see any failing github action runs for closed branches?", "So the way I have it now is 3 actions", "Set up tracking issue <https://github.com/dagster-io/dagster-cloud-action/issues/121|here>", "1 action for PR open", "1 action for Merge into main branch", "1 action for PR close", "this might just be a configuration issue on my end", "the PR close action is currently a stub", "because it was unclear how PR open or Merge into main branch would handle", "Ahh yeah that may be the case. The \u201ccreate or update branch\u201d action will tell Dagster that the PR is closed if the PR state is closed/merged", "ahhh", "where is the `create or update branch` action?", "I\u2019m trying to look for it", "Ah sorry, the hybrid branch deploy action. It calls `create or update branch` internally (it\u2019s a composite action)", "Here you can see it passes the PR status alongside the PR info to update the status in Dagster Cloud: <https://github.com/dagster-io/dagster-cloud-action/blob/main/actions/hybrid_branch_deploy/action.yml#LL69-L79C71>", "got it!", "so basically i can merge my PR close action with my PR open action?", "Yes, that should be fine. Under the hood the action will do different things based on the PR state", "e.g. here you\u2019ll see it only builds images for open PRs, a closed PR will skip this step and only update the state in the UI <https://github.com/dagster-io/dagster-cloud-action/blob/main/actions/hybrid_branch_deploy/action.yml#L57>", "ah perfect! thanks <@U0291UAGGCB>!", "<@U0291UAGGCB> all looks good, I was able to get it working! Thanks!", "Fantastic! Glad to hear"], "is_dagster_empl": [true, false, true, true, false, false, true, false, false, true, false, false, false, true, false, true, false, false, false, false, false, false, true, false, false, false, true, true, false, false, true, true, false, false, true], "gpt4_replies_target": "Dagster Cloud automatically marks a branch as \"closed\" when a PR is merged/closed, hiding it from the UI and spinning down the branch deployment agent infra after a timeout. However, if branches are not being marked as closed/merged correctly, it may be an error with the GitHub Actions. The \"create or update branch\" action within the hybrid branch deploy action should update the PR status in Dagster Cloud. Merging the PR close action with the PR open action should resolve the issue.", "dagster_empl_first_target": "Hi Vivek, the GitHub Actions for branch deployments should mark the branch as \u201cclosed\u201d when a PR is merged/closed. That will hide it from the UI (&amp; branch deployment agent infra spins down automatically after a timeout). Are you interested in explicitly deleting? Archiving is our default to keep e.g. run history around", "dagster_empl_last_target": "Fantastic! Glad to hear"}, {"question": "Hello,\n\nI am very new to dagster. I am coming from airflow background. I was wondering how I can read from a datawarehouse table and write output to another table? I am using postgress for datawarehouse. a simple example would be helpful", "replies": ["Hi Nidhin, Have you been through our tutorial? <https://docs.dagster.io/tutorial/introduction#tutorial-part-one-intro-to-dagster>\n\nThat will get you up to speed on Dagster\u2019s basic concepts.", "Also find these helpful too\nblog post: <https://dagster.io/blog/dagster-script-to-assets|https://dagster.io/blog/dagster-script-to-assets>\ncode example: <https://github.com/petehunt/dagster-script-to-assets|https://github.com/petehunt/dagster-script-to-assets>\nYT video: <https://youtu.be/sCUYBa-glh0|https://youtu.be/sCUYBa-glh0>"], "is_dagster_empl": [true, false], "gpt4_replies_target": "To read from a datawarehouse table and write output to another table in Dagster, you can refer to the Dagster tutorial on their official website, a blog post on their site, a code example on GitHub, and a YouTube video which provide detailed instructions and examples.", "dagster_empl_first_target": "Hi Nidhin, Have you been through our tutorial? <https://docs.dagster.io/tutorial/introduction#tutorial-part-one-intro-to-dagster>\n\nThat will get you up to speed on Dagster\u2019s basic concepts.", "dagster_empl_last_target": "Hi Nidhin, Have you been through our tutorial? <https://docs.dagster.io/tutorial/introduction#tutorial-part-one-intro-to-dagster>\n\nThat will get you up to speed on Dagster\u2019s basic concepts."}, {"question": "Hi all! Is there some nice way to model \"non-pure functions\" as assets? e.g. I have some non-pure code, that produces a \"batch\" of raw data every time it runs. Then from there we have various pure functions (to transform/combine the data). I care about all batches of data ever produced, not just the most recent one.\n\nWhat I _could_ do:\n\u2022 Model the non-pure function  as a `job`\n\u2022 Use an S3 sensor to create a dynamically partitioned `asset` for each output written to S3 by this job\n\u2022 Use `asset`s for the (pure) downstream tasks\nBut in this model I don't have a single place in dagit to see my data end-to-end. The jobs and their logs don't show up in the asset graph here. The link between the job and the assets is implicit via S3, not explicit, like with assets that depend on other assets.", "replies": ["Assuming I'm not entirely missing a feature, I guess what I'm looking for is one of:\n\n\u2022 UI changes so I can see my job and assets in a single lineage graph.\n\u2022 A special type of non-re-materializable asset, where every \"run\" just creates a new partition instead of re-creating the existing data.", "Hi Timo, I don\u2019t see why you couldn\u2019t use a partitioned asset at the root of your graph to represent a non-pure function.  I understand that\u2019s not quite the same as the \u201cspecial type of non-re-materializable asset\u201d you\u2019re inquiring about, but we don\u2019t currently have that in our API (though it\u2019s an interesting idea-- you might want to open a feature request).", "If my nonpure function is something like \"download top 100 hackernews stories\" won't that create big risk that a rematerialization of a partition \"overwrites\" data I collected in the past with new/different data? My goal is to only ever add new data.\n\nPerhaps there's some way I can make a partition know if it's been materialized in the past and exit immediately (or return the data it produced last time)?", "End result being is that if I create a new partition, then the initial materialization goes off and collects a batch of data. But any re-materializations of that partition are no-ops.", "Yes, that risk is there-- one thing you could do is inside the materialization function, query the instance for any previous materializations:\n\n```@asset\ndef my_asset(context):\n    if context.partition_key in context.instance.get_materialization_count_by_partition(AssetKey([\"my_asset\"]):\n    raise Exception()\n    ...```\nThat\u2019s a private API, but I can expand the public `context.get_asset_provenance` to work with partitions for this week, that will enable you to tell if it\u2019s been materialized too.", "Thanks for the suggestions! I will go play and see what I can do with these.\nI've also created a feature request here: <https://github.com/dagster-io/dagster/issues/13336>", "Somewhat unrelated: I've seen your name in a number of GitHub issues I've been following as well. Just wanted to say your community support efforts are noticed and appreciated. Thanks! :raised_hands::pray:", "Hacked something together which suits my needs for now. Sharing in case someone else finds this message: <https://github.com/dagster-io/dagster/issues/13336#issuecomment-1495217302>"], "is_dagster_empl": [false, true, false, false, true, false, false, false], "gpt4_replies_target": "You can model non-pure functions as assets by using a partitioned asset at the root of your graph. To avoid overwriting data, you can query the instance for any previous materializations inside the materialization function. However, this is a private API and there is currently no special type of non-re-materializable asset in the API.", "dagster_empl_first_target": "Hi Timo, I don\u2019t see why you couldn\u2019t use a partitioned asset at the root of your graph to represent a non-pure function.  I understand that\u2019s not quite the same as the \u201cspecial type of non-re-materializable asset\u201d you\u2019re inquiring about, but we don\u2019t currently have that in our API (though it\u2019s an interesting idea-- you might want to open a feature request).", "dagster_empl_last_target": "Yes, that risk is there-- one thing you could do is inside the materialization function, query the instance for any previous materializations:\n\n```@asset\ndef my_asset(context):\n    if context.partition_key in context.instance.get_materialization_count_by_partition(AssetKey([\"my_asset\"]):\n    raise Exception()\n    ...```\nThat\u2019s a private API, but I can expand the public `context.get_asset_provenance` to work with partitions for this week, that will enable you to tell if it\u2019s been materialized too."}, {"question": ":wave: Hello, team!  I am having trouble figuring out how to stop a dagster daemon.  I am getting this message:\n```Another SENSOR daemon is still sending heartbeats. You likely have multiple daemon processes running at once, which is not supported. Last heartbeat daemon id: e3323fa9-6255-4bc3-bf9e-3064ded7f370, Current daemon_id: f0a2b8e4-6fc0-4aae-8f8b-7af132f9fdcf```\nI can see the daemons that are running from dagit, but there is only one of each type (there is only one SENSOR daemon).  Is there a way I can kill the daemon using the id?", "replies": ["It resolved itself overnight somehow\u2026", "Hi Kate, Well that\u2019s good news! I\u2019m going to resolve this for now, but don\u2019t hesitate to post again if this issue re-arises."], "is_dagster_empl": [false, true], "gpt4_replies_target": "The issue with the dagster daemon resolved itself overnight, but if it reoccurs, the user is advised to post again for further assistance.", "dagster_empl_first_target": "Hi Kate, Well that\u2019s good news! I\u2019m going to resolve this for now, but don\u2019t hesitate to post again if this issue re-arises.", "dagster_empl_last_target": "Hi Kate, Well that\u2019s good news! I\u2019m going to resolve this for now, but don\u2019t hesitate to post again if this issue re-arises."}, {"question": "While working on an IO manager, I realized there are a bunch of important public methods on `OutputContext`, like `has_asset_key` and `has_asset_partitions`, which are not rendered in docs. I think this is possibly just because they are lacking docstrings. Is that probably the correct explanation? If so I an make a PR to add a lot of docstrings.", "replies": ["I went ahead with a PR <https://github.com/dagster-io/dagster/pull/13273>", "Hi Spencer, thanks for the PR and for bringing to our attention-- this is also related to a broader issue I can take a look at (these methods should be rendered even without docstrings if marked `@public`)"], "is_dagster_empl": [false, true], "gpt4_replies_target": "The user's assumption about the lack of docstrings causing important public methods on `OutputContext` not to render in docs could be correct, and they have proceeded with a PR to add docstrings. However, it's also noted that these methods should be rendered even without docstrings if marked `@public`, indicating a broader issue that needs to be addressed.", "dagster_empl_first_target": "Hi Spencer, thanks for the PR and for bringing to our attention-- this is also related to a broader issue I can take a look at (these methods should be rendered even without docstrings if marked `@public`)", "dagster_empl_last_target": "Hi Spencer, thanks for the PR and for bringing to our attention-- this is also related to a broader issue I can take a look at (these methods should be rendered even without docstrings if marked `@public`)"}, {"question": "I have an asset with about 800,000 partitions (Multi(TimeWindow,Dynamic)). This makes dagit take about a minute to render the materializations in last run block.", "replies": ["Hi Drew, Thanks for the report, we\u2019re aware of dagit performance issues with many partitions and are actively working towards improving this."], "is_dagster_empl": [true], "gpt4_replies_target": "The Dagster team is aware of the performance issues with Dagit when handling many partitions and is actively working on improvements.", "dagster_empl_first_target": "Hi Drew, Thanks for the report, we\u2019re aware of dagit performance issues with many partitions and are actively working towards improving this.", "dagster_empl_last_target": "Hi Drew, Thanks for the report, we\u2019re aware of dagit performance issues with many partitions and are actively working towards improving this."}, {"question": "Hey guys, I just update dagster and dagit from 1.1.21 to 1.2.3 and now I cannot see any logs for my runs. Neither for runs started before updating nor for runs started after updating. I double checked: with version 1.1.21 I can see all the logs.  Anyone else noticed this behavior? Am I missing something?\nThank you", "replies": ["yes i've seen this as well", "To the right of the filter is a levels option - there\u2019s a bug that it is unselecting the default options. Just select info and above and you\u2019ll see your logs again.", "Fix coming in the next release: <https://github.com/dagster-io/dagster/pull/13109|https://github.com/dagster-io/dagster/pull/13109>", "Could have found this on my own, but somehow I didn't. :disappointed: Thank you!"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "The issue with not seeing logs after updating Dagster and Dagit to version 1.2.3 is due to a bug that deselects the default options in the levels option next to the filter. You can see your logs again by selecting info and above. A fix for this issue is coming in the next release.", "dagster_empl_first_target": "yes i've seen this as well", "dagster_empl_last_target": "Could have found this on my own, but somehow I didn't. :disappointed: Thank you!"}, {"question": "Hi guys, How are you integrating dbt with dagster?\nas far as i know, dagster requires dbt project must be available to access no mater we load dbt source from path or manifest file. So i think there are some suitable approaches:\n+ Bundle dbt project into dagster user code project.\n+ Use gitsync to sync dbt project then mount to dagster deployment pod.\nAre you guys are deploying with above approaches or there is a better solution for that?\nThank you!", "replies": ["The former - we bundle our dbt project into our user code build. They are part of the same git monorepo"], "is_dagster_empl": [false], "gpt4_replies_target": "The preferred method of integrating dbt with dagster is to bundle the dbt project into the user code build, as they are part of the same git monorepo.", "dagster_empl_first_target": "The former - we bundle our dbt project into our user code build. They are part of the same git monorepo", "dagster_empl_last_target": "The former - we bundle our dbt project into our user code build. They are part of the same git monorepo"}, {"question": "Is there a way to have a sensor that checks the status of any backfills that are currently running? would I need to search the dagster instance logs in the body of the sensor?", "replies": ["Hi Danny, So I\u2019m not sure if there\u2019s an official means in the public API (looking into it), but I think this should work:\n\n```@sensor\ndef my_sensor(context):\n    all_backfills = context.instance.get_backfills()\n    statuses = {b.backfill_id: b.status.value for b in all_backfills}```", "thank you!", "What would that status value be? would a backfill be in progress and have information on the partitions that have run and the partitions that still need to run?", "The backfill status will be one of these values: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/execution/backfill.py#L21-L27>", "So no fine-grained info on individual partitions", "Still waiting to hear if we have a pubilc API for this-- if not then we should probably open an issue", "Hi Danny.\n\nThis is slightly complicated because there is a status of the backfill, but that is mostly to track whether the backfill runs have been scheduled or not (helps us track daemon progress).  You probably care the most about whether the runs have completed or not.  To fetch that status, you will need to query the status of the individual runs that compose that backfill.\n\nDisclaimer: How to do this accurately depends on whether or not you are doing an asset backfill or a job backfill.  I\u2019ve included some snippets that queries for job backfills:\n\n```expected_partition_count = backfill.get_num_partitions()\nruns = instance.get_runs(filters=RunsFilter.for_backfill(backfill))\nsuccessful_count = len([run for run in runs where run.status == DagsterRunStatus.SUCCESS])```\nPlease note that the number of runs matching the backfill will probably depend on whether things get retried automatically.  To get full partition status, you might want to bucket run status by partition:\n\n```from dagster._core.storage.tags import PARTITION_NAME_TAG\n\npartitions = backfill.get_partition_names()\nruns = instance.get_runs(filters=RunsFilter.for_backfill(backfill))\nrun_status_by_partition = {}\nfor run in runs:\n    partition = run.tags.get(PARTITION_NAME_TAG)\n    if partition and partition not in run_status_by_partition:\n        # just take the first one, since runs are in descending time order (we want the last run per partition)\n        run_status_by_partition[partition] = run.status```\n", "thank you so much for the help!"], "is_dagster_empl": [true, false, false, true, true, true, true, false], "gpt4_replies_target": "Yes, you can create a sensor that checks the status of any backfills currently running. You can use the `context.instance.get_backfills()` function to get all backfills and then check their status. However, the backfill status mostly tracks whether the backfill runs have been scheduled or not. To get the status of individual runs, you can use the `instance.get_runs(filters=RunsFilter.for_backfill(backfill))` function.", "dagster_empl_first_target": "Hi Danny, So I\u2019m not sure if there\u2019s an official means in the public API (looking into it), but I think this should work:\n\n```@sensor\ndef my_sensor(context):\n    all_backfills = context.instance.get_backfills()\n    statuses = {b.backfill_id: b.status.value for b in all_backfills}```", "dagster_empl_last_target": "Hi Danny.\n\nThis is slightly complicated because there is a status of the backfill, but that is mostly to track whether the backfill runs have been scheduled or not (helps us track daemon progress).  You probably care the most about whether the runs have completed or not.  To fetch that status, you will need to query the status of the individual runs that compose that backfill.\n\nDisclaimer: How to do this accurately depends on whether or not you are doing an asset backfill or a job backfill.  I\u2019ve included some snippets that queries for job backfills:\n\n```expected_partition_count = backfill.get_num_partitions()\nruns = instance.get_runs(filters=RunsFilter.for_backfill(backfill))\nsuccessful_count = len([run for run in runs where run.status == DagsterRunStatus.SUCCESS])```\nPlease note that the number of runs matching the backfill will probably depend on whether things get retried automatically.  To get full partition status, you might want to bucket run status by partition:\n\n```from dagster._core.storage.tags import PARTITION_NAME_TAG\n\npartitions = backfill.get_partition_names()\nruns = instance.get_runs(filters=RunsFilter.for_backfill(backfill))\nrun_status_by_partition = {}\nfor run in runs:\n    partition = run.tags.get(PARTITION_NAME_TAG)\n    if partition and partition not in run_status_by_partition:\n        # just take the first one, since runs are in descending time order (we want the last run per partition)\n        run_status_by_partition[partition] = run.status```\n"}, {"question": "Hi, I have issue backfilling partitioned assets that have dependency across code locations. The error came from dagit/daemon, not user repositories.\n```RuntimeError: generator raised StopIteration\n  File \"/usr/local/lib/python3.11/site-packages/dagster/_daemon/backfill.py\", line 34, in execute_backfill_iteration\n    yield from execute_asset_backfill_iteration(backfill, workspace, instance)\nThe above exception was caused by the following exception:\nStopIteration\n  File \"/usr/local/lib/python3.11/site-packages/dagster/_core/execution/asset_backfill.py\", line 252, in execute_asset_backfill_iteration\n    submit_run_request(\n  File \"/usr/local/lib/python3.11/site-packages/dagster/_core/execution/asset_backfill.py\", line 271, in submit_run_request\n    job_name = _get_implicit_job_name_for_assets(\n               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/usr/local/lib/python3.11/site-packages/dagster/_core/execution/asset_backfill.py\", line 330, in _get_implicit_job_name_for_assets\n    return next(job_name for job_name in job_names if is_base_asset_job_name(job_name))```\nHere is the a minimal reproducible example: <https://bitbucket.org/berry-ai/dagster-asset-backfill-error-report/src/master/>\nHope it's enough information to for you to point out any of my mistakes.", "replies": ["Thanks Boen, I can see you filed an issue as well. Since this seems like a legitimate bug more suited for the issue tracker, I\u2019m going to resolve this thread and we can have any further discussion in the GH issue."], "is_dagster_empl": [true], "gpt4_replies_target": "This issue appears to be a legitimate bug, so it's best to continue the discussion on the issue tracker where you filed the report.", "dagster_empl_first_target": "Thanks Boen, I can see you filed an issue as well. Since this seems like a legitimate bug more suited for the issue tracker, I\u2019m going to resolve this thread and we can have any further discussion in the GH issue.", "dagster_empl_last_target": "Thanks Boen, I can see you filed an issue as well. Since this seems like a legitimate bug more suited for the issue tracker, I\u2019m going to resolve this thread and we can have any further discussion in the GH issue."}, {"question": "Hi guys :wave:\nIs it possible, without implementing a custom IO manager, to get only the metadata of the upstream asset, without loading it?\n\nFor example, I would have an asset that is stored in s3 and a downstream one that would only get its path (which is in the metadata) to run a COPY from s3 to Redshift. In this case, there is no need to load the data, just get the path and run the SQL query.", "replies": ["If I understand your problem correctly, this is a recurring issue. <@U011CET83FG> offers some workarounds for accessing materialization metadata here: <https://github.com/dagster-io/dagster/issues/8521>\n\nI integrated them in my own repo as 3 utils:\n```def get_asset_metadata(instance, asset_key):\n    event_log_entry = instance.get_latest_materialization_event(asset_key)\n    return event_log_entry.dagster_event.event_specific_data.materialization.metadata\n\ndef get_asset_metadata_within_io_manager(context, asset_key):\n    return get_asset_metadata(context.step_context.instance, asset_key)\n\n\ndef get_asset_metadata_within_asset(context, asset_key):\n    return get_asset_metadata(context.instance, asset_key)```", "I found that I could use `input_manager_key` but it doesn't seem to work with `graph_asset`.\nBut indeed, I will need it too, thanks, I will take a look."], "is_dagster_empl": [false, false], "gpt4_replies_target": "Yes, it is possible to get only the metadata of the upstream asset without loading it by using the `get_asset_metadata` function. However, using `input_manager_key` may not work with `graph_asset`.", "dagster_empl_first_target": "If I understand your problem correctly, this is a recurring issue. <@U011CET83FG> offers some workarounds for accessing materialization metadata here: <https://github.com/dagster-io/dagster/issues/8521>\n\nI integrated them in my own repo as 3 utils:\n```def get_asset_metadata(instance, asset_key):\n    event_log_entry = instance.get_latest_materialization_event(asset_key)\n    return event_log_entry.dagster_event.event_specific_data.materialization.metadata\n\ndef get_asset_metadata_within_io_manager(context, asset_key):\n    return get_asset_metadata(context.step_context.instance, asset_key)\n\n\ndef get_asset_metadata_within_asset(context, asset_key):\n    return get_asset_metadata(context.instance, asset_key)```", "dagster_empl_last_target": "I found that I could use `input_manager_key` but it doesn't seem to work with `graph_asset`.\nBut indeed, I will need it too, thanks, I will take a look."}, {"question": "Hi!\nWhen deploying dagster with helm in kuber, we have an issue which happened on Thursday and everything worked before\n\n```dagster._core.errors.DagsterInvalidConfigError: Errors whilst loading configuration for {'postgres_url': Field(&lt;dagster._config.source.StringSourceType object at 0x7f9695861bd0&gt;, default=@, is_required=False), 'postgres_db': Field(&lt;dagster._config.field_utils.Shape object at 0x7f96713d6260&gt;, default=@, is_required=False), 'should_autocreate_tables': Field(&lt;dagster._config.config_type.Bool object at 0x7f96957e85b0&gt;, default=True, is_required=False)}.\nError 1: Post processing at path root:postgres_db:password of original value {'env': 'DAGSTER_PG_PASSWORD'} failed:\ndagster._config.errors.PostProcessingError: You have attempted to fetch the environment variable \"DAGSTER_PG_PASSWORD\" which is not set. In order for this execution to succeed it must be set in this environment.```\nIt happened after recreating pods with jenkins, nothing was changed in deploy settings.\n\nwhat it can be? maybe subbest the way what we can think about.. for now - no idea what it can be", "replies": ["Which pod did this occur in?", "The `DAGSTER_PG_PASSWORD` env gets set on dagit, daemon, and user deployments using a postgres secret\n```          env:\n            - name: DAGSTER_PG_PASSWORD\n              valueFrom:\n                secretKeyRef:\n                  name: {{ include \"dagster.postgresql.secretName\" . | quote }}\n                  key: postgresql-password```\nWhich by default is created by our helm chart <https://github.com/dagster-io/dagster/blob/master/helm/dagster/templates/secret-postgres.yaml#L14>", "Maybe\n\u2022 jenkins created the pod in a different namespace that doesn\u2019t have the secret? I think that would error on pod creation about a missing secret though\n\u2022 The value of the secret is empty now?", "Thank you very much for your answers! Looking what is the problem. Will be back with the result later.", "It seems the problem is in dagster daemon. Because user code and de[ployment is OK, I can run a job from launchpad, the problem is only with\n\u2022 schedule - it is not run when schedule is and the error I\u2019ve sent above is raised at that moment\n\u2022 in dagit I don\u2019t see any logs for any op. But pipeline runs well and gives the result I expect.\n(((", "<@U015C9U9RLK> thank you for your answer. really there was an update of dagster and after this behaviour was changed.  and yes, adding the DAGSTER_PG_PASSWORD to secrets helped.", "Maybe you can suggest why after the update I don\u2019t see logs in dagit at all?", "There was an accidental ui change that is hiding logs by default, it will be fixed in the release tomorrow", "You can still see them using the new levels drop-down ", "Nice, thanks."], "is_dagster_empl": [true, true, true, false, false, false, false, true, true, false], "gpt4_replies_target": "The issue seems to be related to the environment variable \"DAGSTER_PG_PASSWORD\" not being set. This can be fixed by adding the DAGSTER_PG_PASSWORD to secrets. The absence of logs in dagit is due to a UI change that hides logs by default, but this will be fixed in the next release.", "dagster_empl_first_target": "Which pod did this occur in?", "dagster_empl_last_target": "You can still see them using the new levels drop-down "}, {"question": "Hey y'all, I recently switched from using a repository to definitions for setting up all my dagster assets, sensors, and jobs. So far so good running locally.\n\nHowever I deploy to kubernetes in our staging environment (this project isn't in production quite yet). There used to be a workspace.yml file that specified the port that was used for communication between dagit and dagster and this is removed when using defintions.\n\nIs there anything I need to change to make sure those two can communicate in k8? Or is that already handled since I'm using the helm chart? In the values.yaml for helm there's a section that looks like this I've been using during deployment to get the usercode stuff to talk to dagster/dagit:\n\n`dagsterApiGrpcArgs:`\n          - \"--python-file\"\n          - \"hq_dagster/repository.py\"\n          - \"--working-directory\"\n          - \"/opt/dagster/app\"\n          - \"-h\"\n          - \"0.0.0.0\"", "replies": ["Hi Jayme - no changes should be required here. (There's actually still a workspace.yaml under the hood here, but it's entirely managed by the Helm chart)", "Thanks <@U016C4E5CP8> in my config code above there's a reference to the repository.py which no longer exists. What would you recommend I change with that? Appreciate your help as always.", "Is there a new file or module that you can reference instead?", "I followed some examples in the issues for when the repository to definitions change was made. This recommended putting the definitions file in a module initialization file containing the usercode assets of the source. In my case `hq_dagster/__init__.py`", "When you run this code locally, what command do you run?", "(in `dagster dev` or `dagit`)", "dagster dev -p 9000", "What's in your pyproject.toml?", "[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[tool.dagster]\nmodule_name = \"hq_dagster\"", "This should work then:\n```dagsterApiGrpcArgs:\n          - \"--module-name\"\n          - \"hq_dagster\"\n          - \"--working-directory\"\n          - \"/opt/dagster/app\"\n          - \"-h\"\n          - \"0.0.0.0\"```", "Sweet. Thanks man! I'm sure y'all are planning it, but maybe a doc topic on how to convert from repositories to definitions in the helm/deployment area would help here. Not sure how many people are doing k8 though. Python is one of the languages I have less experience with so some of this stuff may be elementary to more people though.", "absolutely, i can pass that along to our docs hit list", "I am also using Helm, but I have multi module repo, is it possible to pass in multiple `--module-name`  arguments to the `dagsterApiGrpcArgs` ?", "It's not currently possible to specify multiple modules to a gRPC server - that's a limitation of the underlying command, not the Helm chart", "What we recommend there currently is to have a single entrypoint module that imports the repositories from each of the modules", "right that\u2019s what I am doing but that would mean I have to keep using repositories and not use the new `Definitions`", "Oh i see - right now you can only have one Definitions object per code location. If you want multiple repositories, our recommend is to keep using repository, there are no plans to remove it", "If you want to use the new Definitions syntax with a repository, you can use the `create_repository_using_definitions_args` function", "oh.. interesting.. yes that might be worthwhile switching to it", "So this link <https://docs.dagster.io/_apidocs/definitions#dagster.create_repository_using_definitions_args> still says its a \u201cstopgap\u201d solution, is there anything better already in the works ?", "I don't think currently - I expect that to work for quite some time"], "is_dagster_empl": [true, false, true, false, true, true, false, true, false, true, false, true, false, true, true, false, true, true, false, false, true], "gpt4_replies_target": "To ensure communication between dagit and dagster in Kubernetes, no changes are required when switching from a repository to definitions. However, you need to update the `dagsterApiGrpcArgs` in your helm chart to reference the new module containing your definitions. If you have multiple modules, you can create a single entrypoint module that imports the repositories from each of the modules.", "dagster_empl_first_target": "Hi Jayme - no changes should be required here. (There's actually still a workspace.yaml under the hood here, but it's entirely managed by the Helm chart)", "dagster_empl_last_target": "I don't think currently - I expect that to work for quite some time"}, {"question": "how experimental are dynamic partitions now?", "replies": ["still experimental--they were only formally released 3 weeks ago in 1.2.0", "ok got it, thanks!!", "I think I'm going to go ahead and use them anyway", "sounds good! The APIs may still change, but there aren't any planned changes to functionality"], "is_dagster_empl": [true, false, false, true], "gpt4_replies_target": "Dynamic partitions in Dagster are still experimental as they were only formally released 3 weeks ago in version 1.2.0, and while there aren't any planned changes to functionality, the APIs may still change.", "dagster_empl_first_target": "still experimental--they were only formally released 3 weeks ago in 1.2.0", "dagster_empl_last_target": "sounds good! The APIs may still change, but there aren't any planned changes to functionality"}, {"question": "hey y\u2019all, i\u2019m trying to upgrade to `dagster==1.2.0` but it looks like i\u2019ve used `dagster-pandera` whose latest release appears to be `dagster-pandera==1.0.5`", "replies": ["can you update dagster-pandera to 0.18.0.  I think that would the corresponding version of the library", "yup! that did it"], "is_dagster_empl": [false, false], "gpt4_replies_target": "You should update `dagster-pandera` to version `0.18.0` as it corresponds with `dagster==1.2.0`.", "dagster_empl_first_target": "can you update dagster-pandera to 0.18.0.  I think that would the corresponding version of the library", "dagster_empl_last_target": "yup! that did it"}, {"question": "Hi everyone!\nI'm a beginner and I have the following question: We use 2 independent dagsters (don't ask why, it was necessary) on the same server running via docker containers. I was given the task of limiting the resources of pipelines, so sometimes such pipelines are launched that gobble up all the RAM, and after which the server is very bad. Where and how can resources be limited?", "replies": ["I know that it is possible to limit resources through docker compose, can it be possible to limit resources using dagster?", "Hi Vyacheslav, I don\u2019t believe that we offer APIs to constrain resources, this should be done at the docker level.", "Hi Sean, ok, thanks!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "Dagster does not offer APIs to constrain resources, so limiting resources should be done at the docker level.", "dagster_empl_first_target": "Hi Vyacheslav, I don\u2019t believe that we offer APIs to constrain resources, this should be done at the docker level.", "dagster_empl_last_target": "Hi Vyacheslav, I don\u2019t believe that we offer APIs to constrain resources, this should be done at the docker level."}, {"question": "Hi all!\nCould you give me a rough idea on how I can go about adding code from a git repository with the docker setup in `examples/deploy_docker/` ?\nI see that there is a docker container with example code, so I imagine I would now set up my own docker container similar to that for my own project and add it to the `docker-compose` configuration.\nI'm unsure as to what the best practice is to pull the latest code from a git repository in that setup.", "replies": ["For now I put together a simple build script:\n```cd code_repo\ngit pull\ncd ..\nsudo docker-compose build docker_example_user_code\nsudo docker-compose up -d```\nI think Docker has some built in capabilites to load code from a github repository, which might be an improvement.", "But I guess there is no way around actually rebuilding the complete container?", "These are our docs on a basic docker setup: <https://docs.dagster.io/deployment/guides/docker>\n\nI\u2019m not a docker expert, but I don\u2019t think there is a way around rebuilding the image. You could just clone the repo in your Dockerfile though if you don\u2019t want to use the script", "Thanks <@U01GTMVMGQH>!", "I think you are right.", "Maybe I could work around it by mounting an external volume and automatically installing any new dependencies if the code changes.", "But it feels too hacky.", "yeah, there\u2019s probably a lot of possible approaches-- but fundamentally this isn\u2019t really a dagster issue but a docker one so I\u2019m going to resolve this thread"], "is_dagster_empl": [false, false, true, false, false, false, false, true], "gpt4_replies_target": "To add code from a git repository to a docker setup in `examples/deploy_docker/`, you can create a simple build script that pulls the latest code from the repository and rebuilds the docker container. However, there's no way around rebuilding the image. Alternatively, you could clone the repo in your Dockerfile or mount an external volume and automatically install any new dependencies if the code changes.", "dagster_empl_first_target": "These are our docs on a basic docker setup: <https://docs.dagster.io/deployment/guides/docker>\n\nI\u2019m not a docker expert, but I don\u2019t think there is a way around rebuilding the image. You could just clone the repo in your Dockerfile though if you don\u2019t want to use the script", "dagster_empl_last_target": "yeah, there\u2019s probably a lot of possible approaches-- but fundamentally this isn\u2019t really a dagster issue but a docker one so I\u2019m going to resolve this thread"}, {"question": "Hi all. I'm currently exploring dagster and building a simple ML pipeline. I'm trying to figure out the following and couldn't find an answer on GH/docs/slack:\n\nLet's say that I have two pipelines: training &amp; inference. I want to re-use some of the assets (at least the business logic) between them, because e.g. ingestion/feature engineering/splitting etc. will all use the same logic. Of course, they will operate on different slices of the data, so they need some input to govern this. What's the recommended pattern to share assets between these two pipelines? Thanks in advance!", "replies": ["Solved this by using <https://docs.dagster.io/concepts/ops-jobs-graphs/ops#op-factory|op factory> pattern", "Great that you were able to solve your issue"], "is_dagster_empl": [false, true], "gpt4_replies_target": "The recommended pattern to share assets between two pipelines in dagster is by using the op factory pattern.", "dagster_empl_first_target": "Great that you were able to solve your issue", "dagster_empl_last_target": "Great that you were able to solve your issue"}, {"question": "Hi all, need some help :slightly_smiling_face: I try to parametrize all similar assets based on Yaml file.  How to catch asset metadata (name, keys, ...) during an asset execution ?\n\nExample of what I try to :\n```@asset()\ndef test_asset(context):\n    print(context.asset_name)\n    &gt;&gt;&gt; test_asset```\nThanks !", "replies": ["This should be possible. You can check everything that\u2019s passed to the `context` arg by inspecting the `OpExecutionContext` Class: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/execution/context/compute.py#L91>\n\nFrom then it\u2019s just a matter of figuring out what information you need exactly", "I will look at it, thanks :slightly_smiling_face:", "I would maybe look into what you\u2019re trying to do exactly though, in many cases, using partitioned assets might be the way to go.", "Today, I create one extraction asset for each source table. The format looks always the same, only the parameters changes.\n\nExample :\n```@asset(key_prefix=[\"ETL\"],required_resource_keys={\"my_sap_co\"},retry_policy=sap_retry_policy)\ndef SAP_TABLE1(context):\n    \n    my_connection = context.resources.my_sap_co\n    SAP_TABLE1 = my_connection.get_table(\n      table=\"SAP_TABLE1\",\n      fields=\"F1,F2,F3\",\n      filters=[\"Filter1\",\"Filter2\"])\n  \n\n    return SAP_TABLE1```\nI would like to create a Yaml file containing all the parameters for each asset instead.\nExample :\n```asset:\n  SAP_TABLE1:\n      fields: F1,F2,F3\n      filters: \n        - [\"Filter1\",\"Filter2\"]\n  SAP_TABLE2:\n      fields: ....\n      filters: \n        - [....]```\nThe ultimate goal is to be able to create assets on the fly by reading this Yaml file :)", "What I'm trying do is very similar to this, if you find an example or have one you can share that'd be much appreciated!", "Right, in case you wanna track these assets separately (read: not as partitions of a single asset) I\u2019d probably use an asset factory type of logic, something like:\n\n```def make_sap_table(conf):\n    my_assets = []\n    for item in conf:\n        @asset(name=item.name, key_prefix=[\"ETL\"],required_resource_keys={\"my_sap_co\"},retry_policy=sap_retry_policy)\n        def table_asset(context):\n            \n            my_connection = context.resources.my_sap_co\n            SAP_TABLE = my_connection.get_table(\n            table=item.table,\n            fields=item.fields,\n            filters=item.filters)\n        \n\n            return SAP_TABLE\n        my_assets.append(table_asset)\n\n    return my_assets```", "very cool ! I will try it :slightly_smiling_face: thanks"], "is_dagster_empl": [false, false, false, false, false, false, false], "gpt4_replies_target": "To parametrize similar assets based on a Yaml file, you can use an asset factory type of logic. This involves creating a function that loops through the configuration and creates an asset for each item. The asset's name, key_prefix, required_resource_keys, and retry_policy can be set based on the item's properties.", "dagster_empl_first_target": "This should be possible. You can check everything that\u2019s passed to the `context` arg by inspecting the `OpExecutionContext` Class: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/execution/context/compute.py#L91>\n\nFrom then it\u2019s just a matter of figuring out what information you need exactly", "dagster_empl_last_target": "very cool ! I will try it :slightly_smiling_face: thanks"}, {"question": "Hi ! I am using GraphQL to run executions. I notice that the object JobOrPipelineSelector takes as parameters pipelineName and JobName. I have trouble understanding the difference between these two concepts?  Is there one of the two parameters to choose ?", "replies": ["Hi Jordan, Just use `jobName`. `pipelineName` is legacy and will be removed eventually (jobs used to be pipelines).", "Okay, thanks for the clarification"], "is_dagster_empl": [true, false], "gpt4_replies_target": "You should use `jobName` as `pipelineName` is a legacy term that will eventually be removed, as jobs used to be referred to as pipelines in Dagster.", "dagster_empl_first_target": "Hi Jordan, Just use `jobName`. `pipelineName` is legacy and will be removed eventually (jobs used to be pipelines).", "dagster_empl_last_target": "Hi Jordan, Just use `jobName`. `pipelineName` is legacy and will be removed eventually (jobs used to be pipelines)."}, {"question": "I have come across what appears to be a bug with partitioned assets. I have created an example completely devoid of any custom code. If you take this example and materialize the job with the date of 2023-03-02 it works, no problem. If you materialize the asset with it's first possible date of 2023-03-01 then the first asset's return value that is passed to the second asset (which should be a simple string) will in fact be a None and the second asset will blow up. I am on dagster 1.1.13.\n\n`from dagster import asset, define_asset_job, build_schedule_from_partitioned_job, DailyPartitionsDefinition`\n\n`@asset(config_schema={},`\n       `partitions_def=DailyPartitionsDefinition(start_date='2023-03-01'))`\n`def first_asset(context) -&gt; str:`\n   `part_date_str = context.asset_partition_key_for_output()`\n    `return part_date_str + \" This text came from first asset\"`\n\n`@asset(config_schema={},`\n       `partitions_def=DailyPartitionsDefinition(start_date='2023-03-01'))`\n`def second_asset(context, first_asset) -&gt; None:`\n    `part_date_str = context.asset_partition_key_for_output()`\n   `<http://context.log.info|context.log.info>(\"first asset returned: \" + first_asset)`\n    `return None`\n\n\n`example_job = define_asset_job(\"example_job\", selection=[\"*second_asset\"],`\n                               `partitions_def=DailyPartitionsDefinition(start_date='2023-03-01'), config={})`\n\n`example_schedule = build_schedule_from_partitioned_job(example_job,`\n                                                      `name=\"Daily_Partitioned_Job_For_example_job\",`\n                                                       `minute_of_hour=15, hour_of_day=7)`", "replies": ["Upgraded to 1.2.4 and the problem is resolved"], "is_dagster_empl": [false], "gpt4_replies_target": "The issue with partitioned assets was resolved by upgrading to Dagster version 1.2.4.", "dagster_empl_first_target": "Upgraded to 1.2.4 and the problem is resolved", "dagster_empl_last_target": "Upgraded to 1.2.4 and the problem is resolved"}, {"question": "When selecting assets, say I have this which selects 50 assets downstream\n\n`asset_bar*`\n\nBut 10 of those assets are part of the group call `group_foo` .\n\nIs there anyway to select all assets downstream of `asset_bar`  *excluding* assets part of `group_foo` (so 40 assets should be selected in total)?", "replies": ["AssetSelection is supposed to allow operators between asset selections.  specifically I think you should be able to\n `AssetSelection.keys('asset_bar').downstream() - AssetSelection.groups('group_foo')`", "relevant docs: <https://docs.dagster.io/_apidocs/assets#dagster.AssetSelection>", "Giovanni is correct, you can use the `-` operator between asset selections!"], "is_dagster_empl": [false, false, true], "gpt4_replies_target": "Yes, you can select all assets downstream of `asset_bar` excluding assets part of `group_foo` by using the `-` operator between asset selections like this: `AssetSelection.keys('asset_bar').downstream() - AssetSelection.groups('group_foo')`.", "dagster_empl_first_target": "Giovanni is correct, you can use the `-` operator between asset selections!", "dagster_empl_last_target": "Giovanni is correct, you can use the `-` operator between asset selections!"}, {"question": "as in this example, where a job directly uses the value of an asset - <https://docs.dagster.io/concepts/ops-jobs-graphs/graphs#loading-an-asset-as-an-input> - can I force the asset to run, to 'refresh' itself?", "replies": ["No, it is not currently possible to run the materialization function of the asset in the same run as an op", "got it - very clear answer, thank you!!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "No, you cannot force the asset to run or 'refresh' itself in the same run as an op in Dagster.", "dagster_empl_first_target": "No, it is not currently possible to run the materialization function of the asset in the same run as an op", "dagster_empl_last_target": "No, it is not currently possible to run the materialization function of the asset in the same run as an op"}, {"question": "Hi! Is there a way to configure jobs so that assets will only be rerun if the partition is missing or out of date? Or do I need to use a sensor?", "replies": ["Hi Yang, versioning/staleness isn\u2019t supported yet for partitioned assets (we\u2019re working towards it), so using a sensor for now is your best bet.", "Oh got it! Thanks"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, Dagster does not support versioning/staleness for partitioned assets, so you would need to use a sensor to rerun jobs if the partition is missing or out of date.", "dagster_empl_first_target": "Hi Yang, versioning/staleness isn\u2019t supported yet for partitioned assets (we\u2019re working towards it), so using a sensor for now is your best bet.", "dagster_empl_last_target": "Hi Yang, versioning/staleness isn\u2019t supported yet for partitioned assets (we\u2019re working towards it), so using a sensor for now is your best bet."}, {"question": "I'm trying to use a `MultiPartitionMapping` that is more-or-less just the identity mapping.\n```@asset(\n    required_resource_keys={\"duckdb\"},\n    ins={\n        \"historical_daily\": AssetIn(\n            partition_mapping=MultiPartitionMapping(\n                {\n                    \"days\": DimensionPartitionMapping(\n                        dimension_name=\"days\",\n                        partition_mapping=TimeWindowPartitionMapping(\n                            start_offset=-window, end_offset=0\n                        )\n                    ),\n                    \"dynamic_partition\": DimensionPartitionMapping(\n                        dimension_name=\"dynamic_partition\",\n                        partition_mapping=IdentityPartitionMapping()\n                    ),\n                }\n            )\n        )\n    },\n    partitions_def=MultiPartitionsDefinition(\n        {\n            \"days\": DailyPartitionsDefinition(\n                timezone=\"Etc/UTC\",\n                start_date=spec[0][\"start_date\"][\"inner\"],\n                end_offset=-1\n            ),\n            \"dynamic_partition\": dynamic_partition\n        }\n    ))```\nWith the upstream asset having more-or-less the identical partitions_def as the downstream. I'm getting the following error.\n``` Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions: \nMulti-partitioned, with dimensions: \nDays: Daily, starting 2019-09-10 Etc/UTC. End offsetted by -1 partitions. \nDynamic_partition: Dynamic partitions: \"dynamic_partition\": {AssetKey(['key'])}\nMulti-partitioned, with dimensions: \nDays: Daily, starting 2019-09-10 Etc/UTC. End offsetted by -1 partitions. \nDynamic_partition: Dynamic partitions: \"dynamic_partition\": {AssetKey(['other_key'])}```", "replies": ["Hi Drew. This error occurs when the partitions defs of assets selected for a job defined via `define_assets_job` are not equivalent python objects. Would you mind sharing partitions def of the upstream asset so I can reproduce this?\n\nI think if you did\n```assert key.partitions_def == other_key.partitions_def```\nthis assertion would fail, I'm curious to see why", "I was able to fix this one by just setting a variable = the partition definition and reusing it between assets. I think I had a str for a date in one and a datetime in the other. Thanks for your help claire."], "is_dagster_empl": [true, false], "gpt4_replies_target": "The error occurs when the partitions definitions of assets selected for a job are not equivalent python objects, and can be fixed by setting a variable equal to the partition definition and reusing it between assets, ensuring that the date formats are consistent.", "dagster_empl_first_target": "Hi Drew. This error occurs when the partitions defs of assets selected for a job defined via `define_assets_job` are not equivalent python objects. Would you mind sharing partitions def of the upstream asset so I can reproduce this?\n\nI think if you did\n```assert key.partitions_def == other_key.partitions_def```\nthis assertion would fail, I'm curious to see why", "dagster_empl_last_target": "Hi Drew. This error occurs when the partitions defs of assets selected for a job defined via `define_assets_job` are not equivalent python objects. Would you mind sharing partitions def of the upstream asset so I can reproduce this?\n\nI think if you did\n```assert key.partitions_def == other_key.partitions_def```\nthis assertion would fail, I'm curious to see why"}, {"question": "Hi, is there a way to organize jobs into folders in the dagit interface?", "replies": ["Hi Brittany, There currently is not-- if you would like this feature, I encourage you to open a a feature request on GH"], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, there is no feature to organize jobs into folders in the dagit interface.", "dagster_empl_first_target": "Hi Brittany, There currently is not-- if you would like this feature, I encourage you to open a a feature request on GH", "dagster_empl_last_target": "Hi Brittany, There currently is not-- if you would like this feature, I encourage you to open a a feature request on GH"}, {"question": "Hi, is it possible to define variable or resource that is shared across multiprocess executor?", "replies": ["Not at the Python level-- if you have the need for this, you should make your dagster resources thin wrappers/interfaces over the \u201creal\u201d resource"], "is_dagster_empl": [true], "gpt4_replies_target": "No, it's not possible to define a variable or resource that is shared across a multiprocess executor at the Python level in Dagster. Instead, you should make your Dagster resources thin wrappers or interfaces over the actual resource.", "dagster_empl_first_target": "Not at the Python level-- if you have the need for this, you should make your dagster resources thin wrappers/interfaces over the \u201creal\u201d resource", "dagster_empl_last_target": "Not at the Python level-- if you have the need for this, you should make your dagster resources thin wrappers/interfaces over the \u201creal\u201d resource"}, {"question": "With Dagster deployed on Kubernetes, I am trying to materialize a downstream TimeWindowPartition and I am getting this error, even though I know that the upstream assets exist (currently in s3) - Any idea what might cause this error? I was thinking it could be the IO manager, but not sure\n```dagster._check.CheckError: Failure condition: Tried to access partition key for asset 'AssetKey(['alarm_watchdog_report_graph'])', but the number of input partitions != 1: 'TimeWindowPartitionsSubset([PartitionKeyRange(start='2023-03-27', end='2023-03-28')])'```", "replies": ["Using the `dagster_aws/s3/io_manager.py` So, now I am thinking the dagster_aws IO manager does not currently support getting TimeWindowPartionsSubset", "I have also been struggling with this. In my case it's a dynamic partition:\n```partitions_def = DynamicPartitionsDefinition(name='top_stories')\n\n@asset(partitions_def=partitions_def)\ndef top_stories(context):\n    # omitted\n\n@asset\ndef unique_top_stories(top_stories):\n    # omitted```\nIf I have exactly 1 partition I get that partition's value in `unique_top_stories` (was expecting some collection). If I have multiple partitions I get the same error as Aaron.\n\nI tried explicitly specifying an `AssetIn` and `AllPartitionMapping` but no change.", "I think you're right about this being tied to the IO manager <@U04N73MPZUM>.\n\nConfirmed just now that if I use the default IO manager my assets work, but switching to the following (pointing at empty bucket) breaks.\n\n```s3_pickle_io_manager.configured({ 's3_bucket': 'my_bucket_here', 's3_prefix': 'my_prefix_here'         })```", "Brand new to `dagster` and its code base but looking at the source code comparing the FS IO manager and S3 IO manager I see these inheritance chains:\n\n`PickledObjectFilesystemIOManager` -&gt; `UPathIOManager` -&gt; `MemoizableIOManager` -&gt; ...\nand\n`PickledObjectS3IOManager` -&gt; `MemoizableIOManager`  -&gt; ...\n\n`UPathIOManager` specifically says \"handles partitioned assets\", and comparing the following two methods seems to support the idea that the S3 IO manager just doesn't handle partitions.\n\n<https://github.com/dagster-io/dagster/blob/8a83cd5d52e5be18dd0ab7a991cf48f594feedbd/python_modules/dagster/dagster/_core/storage/upath_io_manager.py#L187>\n\n<https://github.com/dagster-io/dagster/blob/8a83cd5d52e5be18dd0ab7a991cf48f594feedbd/python_modules/libraries/dagster-aws/dagster_aws/s3/io_manager.py#L68|https://github.com/dagster-io/dagster/blob/8a83cd5d52e5be18dd0ab7a991cf48f594feedb[\u2026]thon_modules/libraries/dagster-aws/dagster_aws/s3/io_manager.py>", "This issue seems relevant:\n<https://github.com/dagster-io/dagster/issues/10908>\n\nBut crucially is missing the actual exception message, which is why I missed it before.", "<@U04N73MPZUM>, in case this is useful to you:\n<https://github.com/dagster-io/dagster/issues/13290>\n\nIt includes a custom S3 IO manager that works for me.", "thanks for the heads up! I will take a look"], "is_dagster_empl": [false, false, false, false, false, false, false], "gpt4_replies_target": "The error you're encountering might be due to the `dagster_aws` IO manager not currently supporting `TimeWindowPartionsSubset`. This issue seems to be tied to the IO manager, and switching to the default IO manager might resolve the issue. You might also want to check out this custom S3 IO manager that seems to work: https://github.com/dagster-io/dagster/issues/13290.", "dagster_empl_first_target": "Using the `dagster_aws/s3/io_manager.py` So, now I am thinking the dagster_aws IO manager does not currently support getting TimeWindowPartionsSubset", "dagster_empl_last_target": "thanks for the heads up! I will take a look"}, {"question": "Hi Guys, I'm running into some interesting behavior with dagit. I'm running v1.2.4 and implemented my own ComplexRepositoryData(RepositoryData) to support refresh of new jobs. When I execute dagit on my local, I see this:\n```AttributeError: 'NoneType' object has no attribute 'items'\n  File \".venv/lib/python3.8/site-packages/dagster/_grpc/server.py\", line 529, in _get_serialized_external_repository_data\n    external_repository_data_from_def(\n  File \".venv/lib/python3.8/site-packages/dagster/_core/host_representation/external_data.py\", line 1117, in external_repository_data_from_def\n    for res_name, res_data in resource_datas.items()```\nThe repo looks like:\n```@repository\ndef my_repo() -&gt; ComplexRepositoryData:\n    return ComplexRepositoryData()```\nFWIW, I'm upgrading from 1.0.15 (hoping <http://1.2.to|1.2.4 >solves some dagit refresh behavior I'm encountering with the complex repo)  therefore I imagined I would see some breaking changes. Thanks!", "replies": ["Hi Matthieu - RepositoryData now has a get_top_level_resources abstract method, do you konw if your class implements that?", "My class currently implements both `get_env_vars_by_top_level_resource` and `get_top_level_resources` but the functions are not implemented...", "ah, returning empty {} solves this. okay will continue to play", "return an empty dict from get_top_level_resources and i believe this will go away", "yeah", "lol ty"], "is_dagster_empl": [true, false, false, true, true, false], "gpt4_replies_target": "The error you're encountering is due to the 'get_top_level_resources' method in your 'ComplexRepositoryData' class not being implemented. You can resolve this issue by returning an empty dictionary from 'get_top_level_resources'.", "dagster_empl_first_target": "Hi Matthieu - RepositoryData now has a get_top_level_resources abstract method, do you konw if your class implements that?", "dagster_empl_last_target": "yeah"}, {"question": "Hi team, we are trying to emit a metric when a job run is requested (from backfill, playground, or sensor). I am not able to find a good place to inject this code to. (I couldn\u2019t put it inside the job graph as the job may potentially fail even before reaching the first step). Wondering if you have any recommendations please?", "replies": ["Hi Hebo - I could imagine using a custom run coordinator for this? <https://docs.dagster.io/deployment/run-coordinator#custom-run-coordinators> That lets you run code when a run is submitted", "like in the example here: <https://docs.dagster.io/guides/dagster/run-attribution#custom-run-coordinator> ( for submitting a metric instead of adding a header)", "Oh right! The run coordinator!! Thanks Daniel!"], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "You can use a custom run coordinator to run code when a run is submitted, as it allows you to emit a metric even before the job reaches the first step.", "dagster_empl_first_target": "Hi Hebo - I could imagine using a custom run coordinator for this? <https://docs.dagster.io/deployment/run-coordinator#custom-run-coordinators> That lets you run code when a run is submitted", "dagster_empl_last_target": "like in the example here: <https://docs.dagster.io/guides/dagster/run-attribution#custom-run-coordinator> ( for submitting a metric instead of adding a header)"}, {"question": "Hi team, can I use `DynamicOut` with an asset?", "replies": ["Hi Qwame, that\u2019s not currently supported-- if you are interested in this I encourage you to open a feature request on our Github."], "is_dagster_empl": [true], "gpt4_replies_target": "No, currently you cannot use `DynamicOut` with an asset in dagster.", "dagster_empl_first_target": "Hi Qwame, that\u2019s not currently supported-- if you are interested in this I encourage you to open a feature request on our Github.", "dagster_empl_last_target": "Hi Qwame, that\u2019s not currently supported-- if you are interested in this I encourage you to open a feature request on our Github."}, {"question": "Hello,\nOther small issue with latest Dagster release (`1.1.11`): when a backfill is `completed`, run status shows partitions status as `missing`", "replies": ["Hello,\nFollowing installation of the latest release (`1.1.13`), I'm still seeing the issue.\n\nWhen a backfill is triggered or completed, partitions run status is not updated and remain in \"missing\" state.", "Hi Lucas - we're looking into this, thanks for the report"], "is_dagster_empl": [false, true], "gpt4_replies_target": "The issue of the run status showing partitions as 'missing' even after a backfill is completed in Dagster release `1.1.11` is still persisting in the latest release (`1.1.13`), and the team is currently looking into it.", "dagster_empl_first_target": "Hi Lucas - we're looking into this, thanks for the report", "dagster_empl_last_target": "Hi Lucas - we're looking into this, thanks for the report"}, {"question": "I might be confused, but does `build_asset_reconciliation_sensor` not work with partitioned assets currently? I thought that it would materialize missing partitions, but it's skipping every tick. I tried adding a FreshnessPolicy but found that that's not available to partitioned assets yet", "replies": ["withdrawn!", "I had one sensor that covered all assets, but that was failing. when I split up the asset groups, I forgot to add the partition def to the new one :neutral_face:", "phew", "Indeed", "Glad it\u2019s working though, it\u2019s an amazing feature"], "is_dagster_empl": [false, false, true, false, false], "gpt4_replies_target": "The user initially thought that `build_asset_reconciliation_sensor` was not working with partitioned assets, but realized they had forgotten to add the partition definition to the new asset group when they split them up.", "dagster_empl_first_target": "phew", "dagster_empl_last_target": "phew"}, {"question": "I'm a bit confused on how slack hooks work here.  I've set these two function up in my job and they do send messages to slack but they send a success or failure message for each op within my job rather than just sending a message when the job completes with a success or failure.  Is there a standard way to send messages when the job completes?  Is it a modification on this approach?  I thought I was setting these at the job level.\n```@success_hook(required_resource_keys={\"slack_webhook_url\"})\ndef slack_message_on_success(context: HookContext):\n    send_slack_message(context, True)\n\n@failure_hook(required_resource_keys={\"slack_webhook_url\"})\ndef slack_message_on_failure(context: HookContext):\n    send_slack_message(context, False)\n\nand then I have my job decorator set with \n\n@job(\n    resource_defs={\"slack_webhook_url\": slack_webhook_url_resource},\n    hooks={slack_message_on_success, slack_message_on_failure},\n)```", "replies": ["I think I've found the right docs for the behavior I'm looking for <https://docs.dagster.io/dagster-cloud/managing-deployments/setting-up-alerts>"], "is_dagster_empl": [false], "gpt4_replies_target": "The user is looking to send Slack messages when a job completes rather than after each operation, and they have found relevant documentation on setting up alerts for this behavior.", "dagster_empl_first_target": "I think I've found the right docs for the behavior I'm looking for <https://docs.dagster.io/dagster-cloud/managing-deployments/setting-up-alerts>", "dagster_empl_last_target": "I think I've found the right docs for the behavior I'm looking for <https://docs.dagster.io/dagster-cloud/managing-deployments/setting-up-alerts>"}, {"question": "is there a way of yielding multiple RunRequest using a custom schedule?", "replies": ["hi <@U03MXV86UNS>! yep you're allowed to yield multiple run requests from the body of a schedule function", "```yield  [\n        RunRequest(\n            job_name=pretty_job\n            run_config=awesome_fn_that_creates_run_config(date),\n            partition_key=date\n        ) for date in pretty_list\n]```\nso this should do the job, right? thanks!!", "ah you'll want to do `yield from` instead of `yield`, but otherwise yep should work (basically the difference between yielding multiple run requests and yielding a single list of run requests)"], "is_dagster_empl": [true, false, true], "gpt4_replies_target": "Yes, you can yield multiple RunRequests from a custom schedule function in Dagster. You should use `yield from` instead of `yield` to yield multiple run requests.", "dagster_empl_first_target": "hi <@U03MXV86UNS>! yep you're allowed to yield multiple run requests from the body of a schedule function", "dagster_empl_last_target": "ah you'll want to do `yield from` instead of `yield`, but otherwise yep should work (basically the difference between yielding multiple run requests and yielding a single list of run requests)"}, {"question": "Hi!\nIt is probably a simple question, but I cannot figured it out by my own.\nSo my goal is to have the same timestamp on every ops in one run. Is there any way to achieve it without using partitioning?\nI`ve tried different approaches, but nothing works, like:\n\u2022 Making timestamp a common resource - resources are initiated separately for ops, there is different timestamps, dependent on actual resource initiation\n\u2022 Making timestamp a tag - timestamp is generated on time of code reload and appears to be fixed\n\u2022 Making timestamp part of the config in job - as well, generation happens on reload and fixes the timestamp.\nSo any ideas?", "replies": ["can you subclass the io manager you're using e.g. fs_io_manager and override handle_output to add a timestamp?", "Isn\u2019t io managers initialized just like resources - per ops and not per run? I understand how to add timestamp to output, but have no clue where from to get it. ", "I'm not near a computer right now but I'd poke around the context object to see if there's a timestamp somewhere", "Unfortunately there is only run_id <https://docs.dagster.io/_apidocs/execution#contexts|https://docs.dagster.io/_apidocs/execution#contexts>", "Hey ivan, would using the start time of the run be an option for this? If so you can fetch that via this slightly convoluted code snippet:\n\n```run_record = context.instance.get_run_records(\n    filters=RunsFilter(run_ids=[context.dagster_run.run_id]),\n    limit=1\n)[0]\nrun_started_time = run_record.start_time```", "<@U016C4E5CP8> oh. thank you so much! That was my next thought to get information about run by its id somehow, but you saved my so much time!"], "is_dagster_empl": [false, false, false, false, true, false], "gpt4_replies_target": "To have the same timestamp on every ops in one run without using partitioning, you can use the start time of the run. You can fetch that via this code snippet: `run_record = context.instance.get_run_records(filters=RunsFilter(run_ids=[context.dagster_run.run_id]), limit=1)[0]; run_started_time = run_record.start_time`.", "dagster_empl_first_target": "Hey ivan, would using the start time of the run be an option for this? If so you can fetch that via this slightly convoluted code snippet:\n\n```run_record = context.instance.get_run_records(\n    filters=RunsFilter(run_ids=[context.dagster_run.run_id]),\n    limit=1\n)[0]\nrun_started_time = run_record.start_time```", "dagster_empl_last_target": "Hey ivan, would using the start time of the run be an option for this? If so you can fetch that via this slightly convoluted code snippet:\n\n```run_record = context.instance.get_run_records(\n    filters=RunsFilter(run_ids=[context.dagster_run.run_id]),\n    limit=1\n)[0]\nrun_started_time = run_record.start_time```"}, {"question": "*Resolved*\n\nFeel like I'm going a bit crazy since I must be completely missing something!\n\nI'm trying to define a <https://docs.dagster.io/_modules/dagster_gcp/bigquery/resources#bigquery_resource|Bigquery resource> but cannot see where I'm supposed to pass info to authenticate such as the JSON credentials and project id! My goal is to be able to query my BQ tables in an asset.\n\nIt works by simply adding `bigquery_resource` to my repository but I'd like to know _why_ it works, mostly because I have two different BQ warehouses and I want to be able to create resources for both!\n\nI'm quite confused where `bigquery_resource` is a able to authenticate given that I haven't passed anything to it.\n\n---\n\nFor those confused as well, I was able to write a more generalizable resource with the below, and I simply add it to my resources with `\"bigquery\": bq_connection.configured({\"credentials\": [enter bq json-like string]})`\n```from dagster import StringSource, resource \nfrom google.cloud import bigquery  # type: ignore\nfrom google.oauth2 import service_account\nimport json\n\n@resource(\n    config_schema={\n        \"credentials\": StringSource,\n    }, \n) \n\ndef bq_connection(init_context):\n\n    credentials = (\n        service_account.\n        Credentials.\n        from_service_account_info(json.loads(init_context.resource_config['credentials']))\n    )\n    return bigquery.Client(credentials=credentials, project=credentials.project_id)```", "replies": ["hey man from what i've done, bigquery actually requires you to have an environment variable `GOOGLE_APPLICATION_CREDENTIALS='C:\\Users\\my_user_eredentials`", "What about if I have multiple BQ warehouses?", "that i have no idea about...you might use something like `dotenv` ??", "If I understand correctly what you need, you can parametrize resource with credentials the following way:\n\n```@resource(\n    config_schema={\n        \"gcs_credential\": StringSource,\n    }\n)\ndef big_query_adapter(init_context):\n    return BigQueryAdapter(\n        init_context.resource_config[\"gcs_credential\"]\n    )\n\nclass BigQueryAdapter:\n    def __init__(self, gcs_credential: str):\n        creds = self._create_gcp_credentials(gcs_credential)\n        self._client = bigquery.Client(credentials=creds)\n\n    @staticmethod\n    def _create_gcp_credentials(gcs_credential: str) -&gt; service_account.Credentials:\n            parsed_creds = json.loads(gcs_credential, strict=False)\n            parsed_creds[\"private_key\"] = parsed_creds[\"private_key\"].replace(\"\\\\n\", \"\\n\")\n            creds = service_account.Credentials.from_service_account_info(parsed_creds)\n            return creds\n\n\n# here we can create as many different BigQueryAdapter-based resources as needed \n# with different credentials stored in variables like GOOGLE_APPLICATION_CREDENTIALS_1\n# Dagster will load envar values for us since we set it as {\"env\": \"name_of_variable\"}\nour_graph_name.to_job(\n    resource_defs={\n        \"big_query_adapter_1\": big_query_adapter.configured(\n            {\"gcs_credential\": {\"env\": \"GOOGLE_APPLICATION_CREDENTIALS_1\"}}\n        ),\n        \"big_query_adapter_2\": big_query_adapter.configured(\n            {\"gcs_credential\": {\"env\": \"GOOGLE_APPLICATION_CREDENTIALS_2\"}}\n        )\n    }\n)```"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "The `bigquery_resource` in Dagster authenticates by using the `GOOGLE_APPLICATION_CREDENTIALS` environment variable. If you have multiple BigQuery warehouses, you can create multiple resources with different credentials by parametrizing the resource with credentials and configuring it with different environment variables like `GOOGLE_APPLICATION_CREDENTIALS_1` and `GOOGLE_APPLICATION_CREDENTIALS_2`.", "dagster_empl_first_target": "hey man from what i've done, bigquery actually requires you to have an environment variable `GOOGLE_APPLICATION_CREDENTIALS='C:\\Users\\my_user_eredentials`", "dagster_empl_last_target": "If I understand correctly what you need, you can parametrize resource with credentials the following way:\n\n```@resource(\n    config_schema={\n        \"gcs_credential\": StringSource,\n    }\n)\ndef big_query_adapter(init_context):\n    return BigQueryAdapter(\n        init_context.resource_config[\"gcs_credential\"]\n    )\n\nclass BigQueryAdapter:\n    def __init__(self, gcs_credential: str):\n        creds = self._create_gcp_credentials(gcs_credential)\n        self._client = bigquery.Client(credentials=creds)\n\n    @staticmethod\n    def _create_gcp_credentials(gcs_credential: str) -&gt; service_account.Credentials:\n            parsed_creds = json.loads(gcs_credential, strict=False)\n            parsed_creds[\"private_key\"] = parsed_creds[\"private_key\"].replace(\"\\\\n\", \"\\n\")\n            creds = service_account.Credentials.from_service_account_info(parsed_creds)\n            return creds\n\n\n# here we can create as many different BigQueryAdapter-based resources as needed \n# with different credentials stored in variables like GOOGLE_APPLICATION_CREDENTIALS_1\n# Dagster will load envar values for us since we set it as {\"env\": \"name_of_variable\"}\nour_graph_name.to_job(\n    resource_defs={\n        \"big_query_adapter_1\": big_query_adapter.configured(\n            {\"gcs_credential\": {\"env\": \"GOOGLE_APPLICATION_CREDENTIALS_1\"}}\n        ),\n        \"big_query_adapter_2\": big_query_adapter.configured(\n            {\"gcs_credential\": {\"env\": \"GOOGLE_APPLICATION_CREDENTIALS_2\"}}\n        )\n    }\n)```"}, {"question": "Hi all , I am using `dbt-dagster`  and `dagster_aws s3` resource. After the execution of DBT i put the result in an S3 but getting and error ..... any hints ?\n`botocore.exceptions.ClientError: An error occurred (AccessDenied) when calling the PutObject operation: Access Denied`", "replies": ["That looks like an AWS specific permission issue"], "is_dagster_empl": [false], "gpt4_replies_target": "The error you're encountering seems to be an AWS specific permission issue.", "dagster_empl_first_target": "That looks like an AWS specific permission issue", "dagster_empl_last_target": "That looks like an AWS specific permission issue"}, {"question": "Hey Guys,\nHow can I make sure that a daily scheduled asset job only runs if the previous day was successful?", "replies": ["Hi Timo, `ScheduleEvaluationContext` gives you access to a `DagsterInstance`, which you can use to query for the previous day's runs:\n\n```@schedule(job_name=\"some_asset_job\")\ndef foo_schedule(context: ScheduleEvaluationContext) -&gt; Sequence[RunRequest]:\n    records = context.instance.get_run_records(\n        filters=RunsFilter(job_name=\"some_asset_job\")\n        order_by=\"create_timestamp\",\n        ascending=False,\n        limit=1\n    )\n    if records and records[0].dagster_run.is_success:\n        return [RunRequest(...)]\n    else:\n        return []```", "Hey Sean, ok I see, you\u2019d have to check somehow. So the default behavior is to run every tick no matter what correct?", "Correct, but if you provide a schedule evaluation function like I did above then you can have pretty much arbitrary logic to determine the run(s) that will be launched each tick", "Alright, thanks. Do you see this as future feature for schedules? I know e.g. Airflow back then pretty much had this outta the box. I believe they called it trigger rules\n<https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#trigger-rules>\n&gt; Trigger Rules\n&gt; By default, Airflow will wait for all upstream (direct parents) tasks for a task to be <https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/tasks.html#concepts-task-states|successful> before it runs that task.\n&gt; However, this is just the default behaviour, and you can control it using the `trigger_rule` argument to a Task. The options for `trigger_rule` are:\n&gt; \u2022 `all_success` (default): All upstream tasks have succeeded\n&gt; \u2022 `all_failed`: All upstream tasks are in a `failed` or `upstream_failed` state\n&gt; \u2022 `all_done`: All upstream tasks are done with their execution\n&gt; \u2022 `all_skipped`: All upstream tasks are in a `skipped` state\n&gt; \u2022 `one_failed`: At least one upstream task has failed (does not wait for all upstream tasks to be done)\n&gt; \u2022 `one_success`: At least one upstream task has succeeded (does not wait for all upstream tasks to be done)\n&gt; \u2022 `one_done`: At least one upstream task succeeded or failed\n&gt; \u2022 `none_failed`: All upstream tasks have not `failed` or `upstream_failed` - that is, all upstream tasks have succeeded or been skipped\n&gt; \u2022 `none_failed_min_one_success`: All upstream tasks have not `failed` or `upstream_failed`, and at least one upstream task has succeeded.\n&gt; \u2022 `none_skipped`: No upstream task is in a `skipped` state - that is, all upstream tasks are in a `success`, `failed`, or `upstream_failed` state\n&gt; \u2022 `always`: No dependencies at all, run this task at any time\n&gt; You can also combine this with the <https://airflow.apache.org/docs/apache-airflow/stable/core-concepts/dags.html#concepts-depends-on-past|Depends On Past> functionality if you wish.", "I don't think we have any plans to add a higher level API to only launch the next schedule run if the previous one succeeded.\n\nI'm personally not that familiar with airflow, but my impression is that \"task\" maps roughly to Dagster's \"op\" concept-- in which case IIUC \"Trigger rules\" wouldn't really map onto successive runs in a schedule, but rather ops in a graph-- and yes if you execute a Dagster graph, downstream ops will not execute by default if an upstream one fails.", "Alright, thanks for clarifying :slightly_smiling_face:\nA follow up question to your proposal, assuming this schedule is supposed to run a partitioned asset job daily:\nIs it theoretically possible to launch an asset RunRequest via API with all the failed partition keys including the latest one and pass that partition range to a single run as you can do in the UI?", "Or what happens behind the scenes when I check \u201cPass partition ranges to single run\u201d? That would help", "```@schedule(job_name=\"some_asset_job\")\ndef foo_schedule(context: ScheduleEvaluationContext) -&gt; Sequence[RunRequest]:\n    records = context.instance.get_run_records(\n        filters=RunsFilter(job_name=\"some_asset_job\")\n        order_by=\"create_timestamp\",\n        ascending=False,\n        limit=1\n    )\n    if records and records[0].dagster_run.is_success:\n        return [RunRequest(...)]\n    else:\n        return []```\nAlso how would this work with partitioned asset jobs, anyways?", "(Marking this resolved since the convo is going in the other thread you opened)"], "is_dagster_empl": [true, false, true, false, true, false, false, false, true], "gpt4_replies_target": "To ensure a daily scheduled asset job only runs if the previous day was successful, you can use `ScheduleEvaluationContext` to access a `DagsterInstance` and query for the previous day's runs. If the previous run was successful, the job will run, otherwise it won't.", "dagster_empl_first_target": "Hi Timo, `ScheduleEvaluationContext` gives you access to a `DagsterInstance`, which you can use to query for the previous day's runs:\n\n```@schedule(job_name=\"some_asset_job\")\ndef foo_schedule(context: ScheduleEvaluationContext) -&gt; Sequence[RunRequest]:\n    records = context.instance.get_run_records(\n        filters=RunsFilter(job_name=\"some_asset_job\")\n        order_by=\"create_timestamp\",\n        ascending=False,\n        limit=1\n    )\n    if records and records[0].dagster_run.is_success:\n        return [RunRequest(...)]\n    else:\n        return []```", "dagster_empl_last_target": "(Marking this resolved since the convo is going in the other thread you opened)"}, {"question": "Tried searching but haven't found any results here in slack or online. I am using pycopg2 for some things and dagster is struggling to build\n```#10 3.541   \u00d7 python setup.py egg_info did not run successfully.\n#10 3.541   \u2502 exit code: 1\n#10 3.541   \u2570\u2500&gt; [23 lines of output]\n#10 3.541       running egg_info\n#10 3.541       creating /tmp/pip-pip-egg-info-iekbphju/psycopg2.egg-info\n#10 3.541       writing /tmp/pip-pip-egg-info-iekbphju/psycopg2.egg-info/PKG-INFO\n#10 3.541       writing dependency_links to /tmp/pip-pip-egg-info-iekbphju/psycopg2.egg-info/dependency_links.txt\n#10 3.541       writing top-level names to /tmp/pip-pip-egg-info-iekbphju/psycopg2.egg-info/top_level.txt\n#10 3.541       writing manifest file '/tmp/pip-pip-egg-info-iekbphju/psycopg2.egg-info/SOURCES.txt'\n#10 3.541       \n#10 3.541       Error: pg_config executable not found.\n#10 3.541       \n#10 3.541       pg_config is required to build psycopg2 from source.  Please add the directory\n#10 3.541       containing pg_config to the $PATH or specify the full executable path with the\n#10 3.541       option:\n#10 3.541       \n#10 3.541           python setup.py build_ext --pg-config /path/to/pg_config build ...\n#10 3.541       \n#10 3.541       or with the pg_config option in 'setup.cfg'.\n#10 3.541       \n#10 3.541       If you prefer to avoid building psycopg2 from source, please install the PyPI\n#10 3.541       'psycopg2-binary' package instead.\n#10 3.541       \n#10 3.541       For further information please check the 'doc/src/install.rst' file (also at\n#10 3.541       &lt;<https://www.psycopg.org/docs/install.html>&gt;).\n#10 3.541       \n#10 3.541       [end of output]\n#10 3.541   \n#10 3.541   note: This error originates from a subprocess, and is likely not a problem with pip.\n#10 3.544 error: metadata-generation-failed\n#10 3.544 \n#10 3.544 \u00d7 Encountered error while generating package metadata.\n#10 3.544 \u2570\u2500&gt; See above for output.\n#10 3.544 \n#10 3.544 note: This is an issue with the package mentioned above, not pip.\n#10 3.544 hint: See above for details.```\nany ideas would be greatly appreciated!", "replies": ["have you tried using `psycopg2-binary` instead of `psycopg2`?", "I havent yet. From what I read, psycopg2 is preferable when interacting with database servers. The former is supposed to be very basic psql and python interactions. I've been ysing psycopg2 locally and haven't had any issues.", "ah I see. have you tried installing `libpq-dev`? (<https://stackoverflow.com/a/12037133/6297800>)", "pretty sure that worked for me a while ago when I ran into this error", "This is already installed on my local machine", "and psycopg2 is working in other projects on your local machine, but it just won't build when you try to install dagster locally?", "yep, exactly", "how are you installing Dagster?", "im using the serverless deploy, importing my own project", "```dagit                  1.1.14\ndagster                1.1.14\ndagster-graphql        1.1.14```\n", "I guess I'm still a bit confused - are you having issues just getting Dagster to install at all locally, or is it when you're deploying that you're seeing issues in CI/CD, or both? I'm not super familiar with dagster serverless but in that context it might make sense to try adding `dagster-postgres`", "sorry, its when Im deploying where I see issues in the CI/CD -", "ah okay, yeah I suspect in that case that `libpq-dev` isn't installed in the container that's being built. it seems likely that you'll need to <https://docs.dagster.io/dagster-cloud/deployment/serverless#using-a-different-base-image-or-using-native-dependencies|provide your own docker image> which includes `libpq-dev`, or <https://docs.dagster.io/dagster-cloud/deployment/serverless#disabling-pex-based-deploys|disabling PEX-based deploys> and providing a `dagster_cloud_pre_install.sh` script to install `libpq-dev`", "but adding `psycopg2-binary` should \"just work\". if you're just using psycopg2 for some basic postgres interactions I don't think you have much to worry about using it, it's very stable", "yeah, its just reading and writing...do you think its better to use binary insterad?", "instead*", "I think it'd be easier, but it doesn't seem like it'd be super hard to modify your serverless deployment workflow to user a custom docker image or pre-install script - I do think your deploys would be a decent amount slower as a result though. really up to you", "the latter option of just moving to binary fixed thus", "this*", "thanks <@U0300665S8G> appreciate the assist"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "The issue with psycopg2 not building during deployment in the CI/CD process can be resolved by either providing your own Docker image which includes `libpq-dev`, disabling PEX-based deploys and providing a `dagster_cloud_pre_install.sh` script to install `libpq-dev`, or by using `psycopg2-binary` instead of `psycopg2`.", "dagster_empl_first_target": "have you tried using `psycopg2-binary` instead of `psycopg2`?", "dagster_empl_last_target": "thanks <@U0300665S8G> appreciate the assist"}, {"question": "Hello Team.\n<@U04MJTPUVDK>\n<@U046AGUTUAV>\nI am doing a POC with dbt+Bigquery+Dasgter I am using the Dagster_starter project with dbt. and for the IO Manager I am using this <https://github.com/dagster-io/quickstart-gcp/blob/main/quickstart_gcp/io_managers.py>\nCurrently I only have 2 assets. *orders* (Which is a csv) and *orders_cleaned* (Which is a dbt model using orders)\nWhen I tried to Materialize all, it works ok for orders but It creates the table with the prefix on it. ex: *raw_data__orders*\nthen dbt asset *orders_cleaned* fails because is trying to read from a table called *orders* which does not exists.\nI have tried removing the prefix, but then I loose the lineage between the assets (see image attached)\nHere is the code:\n```@asset(compute_kind=\"random\", io_manager_key=\"io_manager_raw\")\ndef orders() -&gt; pd.DataFrame:\n    data = pd.read_csv(\"<https://docs.dagster.io/assets/orders.csv>\")\n    return data```\n```dbt_assets = load_assets_from_dbt_project(\n    DBT_PROJECT_DIR,\n    DBT_PROFILES_DIR,\n)\n\nraw_data_assets = load_assets_from_package_module(\n    raw_data,\n    group_name=\"raw_data\",\n    # all of these assets live in the duckdb database, under the schema raw_data\n    key_prefix=[\"raw_data\"],\n)\n\nresources = {\n    \"io_manager\": bigquery_pandas_io_manager.configured(\n        {\n            \"credentials\": {\"env\": \"BIGQUERY_SERVICE_ACCOUNT_CREDENTIALS\"},\n            \"project_id\": {\"env\": \"BIGQUERY_PROJECT_ID\"},\n            \"dataset_id\": \"analytics\"\n        }\n    ),\n    # this io_manager is responsible for storing/loading our pickled machine learning model\n    \"model_io_manager\": fs_io_manager,\n    # this resource is used to execute dbt cli commands\n    \"dbt\": dbt_cli_resource.configured(\n        {\"project_dir\": DBT_PROJECT_DIR, \"profiles_dir\": DBT_PROFILES_DIR}\n    ),\n}\n\ndefs = Definitions(\n    assets=[*dbt_assets,*raw_data_assets],\n    resources=resources,\n)```\nI am also trying to write data in bigquery in 2 different datasets.\nraw_data in one dataset and dbt generated data in a different dataset called analytics\n\nThank you in advance", "replies": ["What does your `sources.yaml` file look like for the dbt project?\n\nIn that example, the BQ IO manager assumes a single dataset and then it names the table with the entire asset key (prefix + asset name).\n\n<https://github.com/dagster-io/quickstart-gcp/blob/main/quickstart_gcp/io_managers.py#L34-L38>\n\nSo you could potentially do a few things:\n\u2022 update sources.yaml to tell it to read from raw_data___orders\n\u2022 remove the key prefix altogether\n\u2022 tweak the IO manager behavior, for example if you want to only use the asset name for the table name, you'd do something like this:\n<https://github.com/slopp/dagster-conditional-etl-gcp-demo/blob/main/dagster_project/resources.py#L80-L81>", "I tried the first suggestion\nthis is my original sources.yml file\n```version: 2\n\nsources:\n  - name: raw_data\n    tables:\n      - name: orders\n      - name: users\n  - name: forecasting\n    tables:\n      - name: predicted_orders```\nI change it to\n```sources:\n  - name: raw_data\n    tables:\n      - name: raw_data__orders\n      - name: users\n  - name: forecasting\n    tables:\n      - name: predicted_orders```\nbut this is what happens", "if I remove the key prefix. I have something similar", "will try to modify the IO manger", "ah gotcha, yea the first result makes sense and was a poor suggestion on my part. if you remove the prefix altogether what happened?\n\nfwiw <@U030M2AL48M> is working on an official bq io manager and it will likely tackle many of these challenges\n\nother IO managers treat the  key prefix and database schema as synonyms, but because BQ does not have a schema concept we'll likely adjust the quickstart once the official IO manager is out", "yes, seems like the solution is in modify the IO manager.\nand agree, I prefer 3 levels of hierarchy like Snowflake and Databricks (wit data catalogs). but Bigquery needs a different approach.\nis there any way to follow jamie's work?", "I don\u2019t have a PR for the BQ io manager work yet, but i can post it here when it exists", "awesome, Thank you!!!", "node_info_to_asset_key will remove the pre fix from the dbt source table\n\ndbt_assets = load_assets_from_dbt_project(\n    project_dir=DBT_PROJECT_PATH,\n    profiles_dir=DBT_PROFILES,\n    node_info_to_asset_key=lambda node_info: AssetKey(node_info[\"name\"])\n)"], "is_dagster_empl": [true, false, false, false, true, false, true, false, false], "gpt4_replies_target": "The user is facing an issue with Dagster where the table created has a prefix, causing the dbt asset to fail as it tries to read from a table without the prefix. One solution suggested is to modify the IO manager behavior to only use the asset name for the table name. Another solution is to remove the key prefix altogether. An official BigQuery IO manager is also being developed which might address these challenges.", "dagster_empl_first_target": "What does your `sources.yaml` file look like for the dbt project?\n\nIn that example, the BQ IO manager assumes a single dataset and then it names the table with the entire asset key (prefix + asset name).\n\n<https://github.com/dagster-io/quickstart-gcp/blob/main/quickstart_gcp/io_managers.py#L34-L38>\n\nSo you could potentially do a few things:\n\u2022 update sources.yaml to tell it to read from raw_data___orders\n\u2022 remove the key prefix altogether\n\u2022 tweak the IO manager behavior, for example if you want to only use the asset name for the table name, you'd do something like this:\n<https://github.com/slopp/dagster-conditional-etl-gcp-demo/blob/main/dagster_project/resources.py#L80-L81>", "dagster_empl_last_target": "I don\u2019t have a PR for the BQ io manager work yet, but i can post it here when it exists"}, {"question": "Hello All.\nIs it possible to have nested dynamic graphs? I'm able todo the following with dynamic output chaining:\n```from dagster import job, DynamicOutput, DynamicOut, op, Definitions\nimport time\n\n\n@op(out=DynamicOut())\ndef get_dynamic_numbers():\n    for i in range(5):\n        yield DynamicOutput(value=i, mapping_key=str(i))\n\n\n@op\ndef get_number_result(number):\n    time.sleep(2)\n    return number * 2\n\n\n@op\ndef get_number_result_from_result(number):\n    time.sleep(2)\n    return number * 2\n\n\n@job()\ndef dynamic_job():\n    def _for_each_number(number):\n        number_result = get_number_result(number)\n        number_result_from_result = get_number_result_from_result(number_result)\n\n        return number_result_from_result\n\n    results = get_dynamic_numbers().map(_for_each_number)\n    results.collect()\n\n\ndefs = Definitions(jobs=[dynamic_job])```\n\nHowever, if i try adding another dynamic output in the chaining steps like this:\n```from dagster import job, DynamicOutput, DynamicOut, op, Definitions\nimport time\n\n\n@op(out=DynamicOut())\ndef get_dynamic_numbers():\n    for i in range(5):\n        yield DynamicOutput(value=i, mapping_key=str(i))\n\n\n@op\ndef get_number_result(number):\n    time.sleep(2)\n    return number * 2\n\n\n@op(out=DynamicOut())\ndef get_dynamic_numbers_for_result(number: int):\n    for i in range(number):\n        yield DynamicOutput(value=i, mapping_key=str(i))\n\n\n@job()\ndef dynamic_job():\n    def _for_each_number(number):\n        number_result = get_number_result(number)\n        dynamics = get_dynamic_numbers_for_result(number_result)\n\n        results = dynamics.map(get_number_result)\n        results.collect()\n\n    get_dynamic_numbers().map(_for_each_number)\n\n\ndefs = Definitions(jobs=[dynamic_job])```\nI get the following error:\n```dagster._core.errors.DagsterInvalidDefinitionError: op 'get_number_result_2' cannot be downstream of more than one dynamic output. It is downstream of both \"get_dynamic_numbers:result\" and \"get_dynamic_numbers_for_result:result\"```\nAny help is appreciated. Thanks in advance!", "replies": ["I guess this isn't currently supported <https://github.com/dagster-io/dagster/issues/4364>. Unless I'm missing something...", "You're right-- this isn't yet supported, and that issue is the place to track status."], "is_dagster_empl": [false, true], "gpt4_replies_target": "Currently, having nested dynamic graphs is not supported in Dagster, as mentioned in the issue on their GitHub page (https://github.com/dagster-io/dagster/issues/4364).", "dagster_empl_first_target": "You're right-- this isn't yet supported, and that issue is the place to track status.", "dagster_empl_last_target": "You're right-- this isn't yet supported, and that issue is the place to track status."}, {"question": "Hello,\n\nTrying to use *bigquery_pandas_io_manager* however  having an issue trying to finding a way to pass in \"dataset_id\" when  @asset()  is initialized instead of using the default.\n\n```@io_manager(\n    config_schema={\n        \"credentials\": StringSource,\n        \"project_id\": StringSource,\n        \"dataset_id\": Field(\n            str, default_value=\"my_dataset\", description=\"Dataset ID. Defaults to 'my_dataset'\"\n        ),\n    }\n)\ndef bigquery_pandas_io_manager(init_context: InitResourceContext) -&gt; BigQueryDataframeIOManager:\n    return BigQueryDataframeIOManager(\n        credentials=json.loads(init_context.resource_config[\"credentials\"]),\n        project_id=init_context.resource_config[\"project_id\"],\n        dataset_id=init_context.resource_config[\"dataset_id\"],\n    )```\nFrom:\n<https://github.com/dagster-io/dagster/blob/78df951f1f0d3934a5ca3ea34a84036f31d3525c/examples/quickstart_gcp/quickstart_gcp/io_managers.py>", "replies": ["Hi <@U046AGUTUAV>, I was looking at you snowreport project because you are using Bigquery and I was wondering if you had any ideas on how you can pass in the dataset name as a parameter for the asset. I see that you have it specified  to \"snowreport\" for prod and \"snowreport_branch\" for branch deployment in the repository.py.\n\nI'm hoping to find a way to have those specified here somewhere if that is possible so you're not locked into one dataset:\n\n```@asset(\n    io_manager_key=\"bq_io_manager\",\n    required_resource_keys={\"bq_auth\"},\n    ins = {key: AssetIn(key) for key in asset_keys},\n    partitions_def=DailyPartitionsDefinition(start_date=\"2022-10-05\"),\n    key_prefix=\"snocountry\"\n)\ndef resort_raw(context, **resort_assets) -&gt; pd.DataFrame:\n    \"\"\"Insert clean resort records to BQ\"\"\"```", "<@U04C9F3B6G4> I think you'll need to adjust the IO manager so that it reads the dataset from part of the asset key. That example project is a little wonky because it uses the GCP clients directly, in more recent examples I've been using pandas_gbq. Here is another example, <https://github.com/slopp/dagster-conditional-etl-gcp-demo/blob/main/dagster_project/resources.py#L80>\n\nIn either case,  you could do something like this to have the asset key_prefix specify the dataset name:\n\n```dataset_table = f\"{context.asset_key.path[-2]\"}.{context.asset_key.path[-1]}\"```\n", "Ah ok that makes sense. Thanks for the assistance!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "To pass in the dataset name as a parameter for the asset, you can adjust the IO manager to read the dataset from part of the asset key. You can use the asset key_prefix to specify the dataset name, like so: `dataset_table = f\"{context.asset_key.path[-2]\"}.{context.asset_key.path[-1]}\"`.", "dagster_empl_first_target": "<@U04C9F3B6G4> I think you'll need to adjust the IO manager so that it reads the dataset from part of the asset key. That example project is a little wonky because it uses the GCP clients directly, in more recent examples I've been using pandas_gbq. Here is another example, <https://github.com/slopp/dagster-conditional-etl-gcp-demo/blob/main/dagster_project/resources.py#L80>\n\nIn either case,  you could do something like this to have the asset key_prefix specify the dataset name:\n\n```dataset_table = f\"{context.asset_key.path[-2]\"}.{context.asset_key.path[-1]}\"```\n", "dagster_empl_last_target": "<@U04C9F3B6G4> I think you'll need to adjust the IO manager so that it reads the dataset from part of the asset key. That example project is a little wonky because it uses the GCP clients directly, in more recent examples I've been using pandas_gbq. Here is another example, <https://github.com/slopp/dagster-conditional-etl-gcp-demo/blob/main/dagster_project/resources.py#L80>\n\nIn either case,  you could do something like this to have the asset key_prefix specify the dataset name:\n\n```dataset_table = f\"{context.asset_key.path[-2]\"}.{context.asset_key.path[-1]}\"```\n"}, {"question": "How do you programatically replicate passing a partition range into single run? What happens behind the scenes?", "replies": ["Can you yield a `RunRequest` with a `asset_partition_range_start` and `asset_partition_range_end` ?", "Hi Timo I put out a request for a colleague who knows the answer to this, they should respond here soon.", "Hey <@U01GTMVMGQH> Any news here?", "Hey Timo - yes, you should be able to yield a `RunRequest` with those tags and it will fill the range of partitions", "Oh yeah that adds up. Thanks <@U011CET83FG>"], "is_dagster_empl": [false, true, false, true, false], "gpt4_replies_target": "To programmatically replicate passing a partition range into a single run, you can yield a `RunRequest` with an `asset_partition_range_start` and `asset_partition_range_end`. This will fill the range of partitions.", "dagster_empl_first_target": "Hi Timo I put out a request for a colleague who knows the answer to this, they should respond here soon.", "dagster_empl_last_target": "Hey Timo - yes, you should be able to yield a `RunRequest` with those tags and it will fill the range of partitions"}, {"question": "*Matching version between dagit, dagster-daemon and repos*\n\nHow important is it to match the version of the dagster python package between components of a dagster deployment? For example, if `dagit` and `dagster-daemon` are running in docker containers using `dagster` `1.1.12`, is it OK for me to have a repo (running in a different container) which is running `dagster` `1.1.15` ?", "replies": ["Hey Stefan - it's fine for them to be at different versions, you can upgrade your user code and system code independently", "Thanks, <@U016C4E5CP8>"], "is_dagster_empl": [true, false], "gpt4_replies_target": "It's acceptable for different components of a dagster deployment to run on different versions of the dagster python package, allowing for independent upgrades of user code and system code.", "dagster_empl_first_target": "Hey Stefan - it's fine for them to be at different versions, you can upgrade your user code and system code independently", "dagster_empl_last_target": "Hey Stefan - it's fine for them to be at different versions, you can upgrade your user code and system code independently"}, {"question": "Hi All! Question regarding access permissions: Does a user need to be an `orgadmin` just to view a branch deployment? That\u2019s what I\u2019m seeing with Dagster Cloud. It seems excessively permissive just to view a deployment. Confused why a `Viewer` can\u2019t view branch deployments. Thanks!", "replies": ["Hi Vivek - right now it's anybody who's an editor or higher. Agree that this is suboptiomal, we're going to be rolling out a new permissions UI in the next few weeks that lets you specifically customize which individuals can view branch deployments."], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, only users with editor or higher permissions can view branch deployments in Dagster Cloud, but a new permissions UI is being developed to allow customization of access to branch deployments.", "dagster_empl_first_target": "Hi Vivek - right now it's anybody who's an editor or higher. Agree that this is suboptiomal, we're going to be rolling out a new permissions UI in the next few weeks that lets you specifically customize which individuals can view branch deployments.", "dagster_empl_last_target": "Hi Vivek - right now it's anybody who's an editor or higher. Agree that this is suboptiomal, we're going to be rolling out a new permissions UI in the next few weeks that lets you specifically customize which individuals can view branch deployments."}, {"question": "Hi team, asking about [limiting concurrent runs](<https://docs.dagster.io/deployment/run-coordinator>), is it supported in Dagster cloud? Also is it per project based (meaning I put a dagster.yaml in my project and it applies to only my project)? Or it is globally configured somewhere?", "replies": ["Hi Sean - you can configure these limits in cloud via deployment settings in the UI: <https://docs.dagster.io/dagster-cloud/developing-testing/managing-deployments#configuring-deployment-settings>\n\nThe limits are applied at the deployment level", "I see I will need `Organization Admin permissions` to modify that, which I don\u2019t, is there any other ways that I could apply tag based limits only to my project?", "You would need to be an Admin for that deployment to set those settings (you don\u2019t need to be a full Organization Admin)", "Actually I think that Organization Admin may be a mistake in the docs - believe Editors can even change those", "<https://docs.dagster.io/dagster-cloud/account/managing-users#understanding-user-permissions|https://docs.dagster.io/dagster-cloud/account/managing-users#understanding-user-permissions> says editor which I think is correct", "I think the user permission chart might contain a mistake, as an editor I don\u2019t see Settings button next to the deployment I want to configure.", "I\u2019ll check with the team and report back - there is definitely a mistake somewhere", "Hi Sean, are you able to access configuration through the Settings button in the Deployments dropdown?", "I don\u2019t think so?", "Double-checking, it looks like this is gated to org. admins at the moment. I think our docs need to be updated to represent this.", "I think the likely end state here is certain roles being able to change certain settings (for example, editors being able to change the run queue settings, while only admins can change certain other settings). And sooner than that we can change it so that you can change the settings if you're an admin on that particular deployment (i.e. you won't need to be an organization admin). But today it does seem like you need to be an org admin to change the settings :confused:", "Er wait! sorry for all the confusion here. I think this will actually work if you use the dagster-cloud CLI to change the settings (See the \"dagster-cloud CLI\" tab here: <https://docs.dagster.io/dagster-cloud/developing-testing/managing-deployments#configuring-deployment-settings>)\n\nWe'll get that fixed in the UI (and docs) to reflect that. It should be EDITOR permissions after all today.", "We should have this fixed in tomorrow's release - you'll be able to edit the deployment settings if you're an EDITOR on that deployment (the backend was always allowing that and the UI just needed to be updated). Thanks for the report!"], "is_dagster_empl": [true, false, true, true, true, false, true, true, false, true, true, true, true], "gpt4_replies_target": "Dagster cloud supports limiting concurrent runs and it can be configured via deployment settings in the UI, which are applied at the deployment level. Currently, you need to be an organization admin to change these settings, but you can also use the dagster-cloud CLI to change the settings if you have editor permissions. The team is working on updating the UI and docs to reflect this.", "dagster_empl_first_target": "Hi Sean - you can configure these limits in cloud via deployment settings in the UI: <https://docs.dagster.io/dagster-cloud/developing-testing/managing-deployments#configuring-deployment-settings>\n\nThe limits are applied at the deployment level", "dagster_empl_last_target": "We should have this fixed in tomorrow's release - you'll be able to edit the deployment settings if you're an EDITOR on that deployment (the backend was always allowing that and the UI just needed to be updated). Thanks for the report!"}, {"question": "Hi All!\nwhen a repository is defined with the new `Definitions` type, the repository name is not defined (can\u2019t see the docs for it). This created some confusion when I was trying to kickoff a Job remotely via a GraphQL request.\nhad to pass in `\"repositoryName\": \"__repository__\"` for it to work. Should we update the docs to make it more clear or perhaps add `name` to `Definitions` ?", "replies": ["Hi Sanidhya, thanks for pointing this out-- we should update the GQL client to be more friendly with the new `Definitions` API. Would you consider creating an issue for it? <https://github.com/dagster-io/dagster/issues>", "Hi Sean,\nI just created one\n<https://github.com/dagster-io/dagster/issues/12157>"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The user suggested updating the GQL client to be more compatible with the new `Definitions` API in Dagster, and an issue has been created for this on GitHub.", "dagster_empl_first_target": "Hi Sanidhya, thanks for pointing this out-- we should update the GQL client to be more friendly with the new `Definitions` API. Would you consider creating an issue for it? <https://github.com/dagster-io/dagster/issues>", "dagster_empl_last_target": "Hi Sanidhya, thanks for pointing this out-- we should update the GQL client to be more friendly with the new `Definitions` API. Would you consider creating an issue for it? <https://github.com/dagster-io/dagster/issues>"}, {"question": "I have a few assets as follows, where the data_assets below loads data and returns a class:\n\n```@asset(partitions_def=daily_partitions_def)\ndef data_asset_a(context):\n\treturn DataAssetA(context)\n\n@asset(partitions_def=daily_partitions_def)\n@def data_asset_b(context):\n    return DataAssetB(context)\n\n@asset(partitions_def=partitions_def)\ndef calc_asset(context, data_asset):\n    dt = context.partition_key\n    df = CALC(data_obj=data_asset).daily_run(dt)\n    return df```\n\nI would like to run the clac asset with the two different data_assets on two different schedules. Is there an good way to do this without creating two calc assets?", "replies": ["Hi Nicholas, if `data_asset_a` and `data_asset_b` are differently structured, then you'll need separate `calc_asset` assets. However, it's possible that you could use our new <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#multi-dimensionally-partitioned-assets|Multidimensionally partitioned assets> to model your problem, and collapse `data_asset_a` and `data_asset_b` into one multidimensionally partitioned `data_asset`."], "is_dagster_empl": [true], "gpt4_replies_target": "If `data_asset_a` and `data_asset_b` are differently structured, you'll need separate `calc_asset` assets, but you could potentially use Dagster's Multidimensionally partitioned assets to collapse `data_asset_a` and `data_asset_b` into one multidimensionally partitioned `data_asset`.", "dagster_empl_first_target": "Hi Nicholas, if `data_asset_a` and `data_asset_b` are differently structured, then you'll need separate `calc_asset` assets. However, it's possible that you could use our new <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#multi-dimensionally-partitioned-assets|Multidimensionally partitioned assets> to model your problem, and collapse `data_asset_a` and `data_asset_b` into one multidimensionally partitioned `data_asset`.", "dagster_empl_last_target": "Hi Nicholas, if `data_asset_a` and `data_asset_b` are differently structured, then you'll need separate `calc_asset` assets. However, it's possible that you could use our new <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#multi-dimensionally-partitioned-assets|Multidimensionally partitioned assets> to model your problem, and collapse `data_asset_a` and `data_asset_b` into one multidimensionally partitioned `data_asset`."}, {"question": "When testing software defined assets that make use of time range partitions, what's the best way to assemble the context? `build_op_context` allows a single partition key, not a range", "replies": ["Looks like this is related:\n<https://github.com/dagster-io/dagster/pull/13088>", "Hi Brendan, It looks like we don\u2019t support this right now-- partition key ranges are a bit rough around the edges. The technical reason is that these ranges are sourced from run tags, and there are no run tags when doing direct op invocation. However, I think it\u2019d be simple enough to add support for this to `build_op_context`, if you create a issue for it I\u2019m pretty sure I can knock it out quickly.", "I <@U01GTMVMGQH> - that's interesting. I will make an issue, but I'd like a bit of clarity first, as I actually got it to work with:\n```context = dagster.build_op_context(partition_key=\"2023-01-05\")```\nand then `time_window = context.asset_partitions_time_window_for_output()` works.\nObviously this doesn't cover a range beyond a single day (which I would like to test).\n(I notice that this method is marked `deprecated` on the `OpExecutionContext` (but not the `AssetExecutionContext`) - which leaves me a bit confused. What's going on here?)\nFinally, this `context.partition_time_window` does *not* work, despite the above working. This is another confusing part! Any ideas before I make the issue?", "<https://github.com/dagster-io/dagster/issues/14990>", "If we have a daily partitioned asset, then each key corresponds to a timespan over a whole day (a \u201ctime window\u201d)-- so, no surprise that `asset_partitions_time_window_for_output` works, since it can be derived from one partition key.\n\n&gt; (I notice that this method is marked deprecated on the OpExecutionContext (but not the AssetExecutionContext) - which leaves me a bit confused. What\u2019s going on here?)\nI\u2019m not 100% up to date here but that\u2019s probably bc `OpExecutionContext` is only being used now in situations where there is no associated asset. If you have an `@asset` it should always receive the `AssetExecutionContext` subclass instead.\n\n&gt; Finally, this context.partition_time_window does not work, despite the above working. This is another confusing part! Any ideas before I make the issue?\nSeems odd, what error gets thrown?", "``` AttributeError: 'BoundOpExecutionContext' object has no attribute '_step_execution_context'. Did you mean: 'get_step_execution_context'?```", "I'll add the error to the issue", "Thanks, seems like a straightforward bug", "With the bug fixed, presumably inside an `@asset`, `context.partition_time_window` should be preferred to `context.asset_partitions_time_window_for_output()` ? The latter I'd expect from the name to be more associated with output steps.", "I agree"], "is_dagster_empl": [false, true, false, false, true, false, false, true, false, true], "gpt4_replies_target": "Currently, Dagster does not support partition key ranges for testing software defined assets that make use of time range partitions. However, you can use `build_op_context` with a single partition key for testing. If you encounter any issues or bugs, it's recommended to report them for further assistance.", "dagster_empl_first_target": "Hi Brendan, It looks like we don\u2019t support this right now-- partition key ranges are a bit rough around the edges. The technical reason is that these ranges are sourced from run tags, and there are no run tags when doing direct op invocation. However, I think it\u2019d be simple enough to add support for this to `build_op_context`, if you create a issue for it I\u2019m pretty sure I can knock it out quickly.", "dagster_empl_last_target": "I agree"}, {"question": "There is a point in the docs about providing inputs to an op that made me stop:\n&gt; An op only starts to execute once all of its inputs have been resolved. Inputs can be resolved in two ways:\n&gt; \u2022 The upstream output that the input depends on has been successfully emitted and stored.\n&gt; \u2022 *The input was stubbed through config.*\n<https://docs.dagster.io/concepts/ops-jobs-graphs/ops#inputs>\n\nHow does one \u201cstub\u201d an input \u201cthrough config\u201d? Is it somehow possible to have a simple op\n```@op\ndef my_op(my_input:str):\n    # do something with my_input\n    # ...```\nand provide `my_input`  through a Pythonic config object without having to do something like this:\n```@op(ins={\"my_input\": In(Optional[str], default_value=None)})\ndef my_op(config: MyConfig, my_input:str | None = None):\n    my_input = my_input if my_input is not None else config.configured_input```\nI wouldn\u2019t consider the latter as \u201cstubbing\u201d the input, given that all the gymnastics around finding the actual value is done in the body of the function. Am I missing something here?", "replies": ["If you want a simple add a parameter to an op, you can do that. If you\u2019re going to do a MyConfig, I don\u2019t see why you wouldn\u2019t add `my_input` to that.\n\nHere\u2019s an example op with an array of Outputs passed to it\n```@op(ins={\"start\": In(Nothing)})\ndef OrderPipeline(context, nums: List[Output]):\n    shell_script_runner(context, \"MyOrderPipeline.ksh\")```", "Hi Andras, The \u201cstubbing\u201d being referred to here is using the \u201cinputs\u201d field for an op on the run config schema:\n\n```from dagster import job, op\n\n@op\ndef foo(a: int) -&gt; int:\n    return a + 1\n\n@job\ndef foo_job():\n    foo()\n\nfoo_job.execute_in_process(\n    run_config={\"ops\": {\"foo\": {\"inputs\": {\"a\": 1}}}}\n)```\nI realize the terminology is confusing, but this is separate from the \u201cconfig\u201d for the op, (i.e. what the op\u2019s config schema defines).", "Hi <@U01GTMVMGQH>, totally makes sense, thanks! Can I contribute to the docs to clarify this?", "Thanks for the offer, but it\u2019s such a small change that it\u2019s not worth the overhead of a community PR, so I created a PR that links to the right place: <https://github.com/dagster-io/dagster/pull/14991>", ":+1:"], "is_dagster_empl": [false, true, false, true, false], "gpt4_replies_target": "To \"stub\" an input through config in Dagster, you can use the \"inputs\" field for an op on the run config schema. This is separate from the \"config\" for the op, which is defined by the op's config schema.", "dagster_empl_first_target": "Hi Andras, The \u201cstubbing\u201d being referred to here is using the \u201cinputs\u201d field for an op on the run config schema:\n\n```from dagster import job, op\n\n@op\ndef foo(a: int) -&gt; int:\n    return a + 1\n\n@job\ndef foo_job():\n    foo()\n\nfoo_job.execute_in_process(\n    run_config={\"ops\": {\"foo\": {\"inputs\": {\"a\": 1}}}}\n)```\nI realize the terminology is confusing, but this is separate from the \u201cconfig\u201d for the op, (i.e. what the op\u2019s config schema defines).", "dagster_empl_last_target": "Thanks for the offer, but it\u2019s such a small change that it\u2019s not worth the overhead of a community PR, so I created a PR that links to the right place: <https://github.com/dagster-io/dagster/pull/14991>"}, {"question": "Hi, I am new to dagster, so sorry for the basic question: I have several json endpoints that need to be scraped regularly and get processed by the same logic. I can make an `asset` for each endpoint but then I need to copy paste the same boilerplate code:\n```@asset\ndef fetch_one():\n        return requests.get(\"<http://example.com/one>\").json()\n\n@asset\ndef fetch_two():\n        return requests.get(\"<http://example.com/two>\").json()\n    \n@asset\ndef clean_one(fetch_one):\n    df = pd.json_normalize(fetch_one)\n    return df.dropna()\n\n@asset\ndef clean_two(fetch_two):\n    df = pd.json_normalize(fetch_two)\n    return df.dropna()```\nWhat would be the recommended way to avoid that?\n\u2022 Do I work with a `config` like <https://docs.dagster.io/concepts/configuration/config-schema> where each endpoint is a different `RunConfig`? That seems a bit heavy.\n\u2022 Another option is maybe with `context` and `multi-asset` like <https://docs.dagster.io/concepts/assets/multi-assets#subsetting-multi-assets> ?\n\u2022 Or do I use an asset-factory pattern? (Any example of this?)", "replies": ["I had a similar thing where I was doing boilerplate code between Ops. First do it with a few Assets like that. If it works, then write a python function to encapsulate the boilerplate and have your Asset call that function to get the boilerplate code.\nSimplified example:\nI have an pure python function and ops calling it:\n```def shell_script_runner(context, script_name):\n    execute_shell_command(f\"{script_name}\")\n\n@op\ndef script1(context):\n    shell_script_runner(context, \"hello_world.sh\")\n\n@op\ndef script2(context):\n    shell_script_runner(context, \"hello_dagster.sh\")```\nI\u2019m thinking you could do something similar for assets.", "Thx for the suggestion. But script1 and script2 in your example are still very much copy pasted. For a second step in the pipeline, I would need to write the same individual functions. Like process_script1(script1) and process_script2(script2)", "Yeah, they are copy paste looking because I\u2019m doing Ops and migrating some existing scripts. For ops that are dependent on each other, I alter the op signature so I can make them dependent in the job.\n\nHere\u2019s how I make an op dependent:\n```@op\ndef data_reporter(context):\n    pass\n    \n@op(ins={\"start\": In(Nothing)})\ndef data_transform(context):\n    pass\n\n@job()\ndef Sample_etl_job():\n    data = data_transform(start=data_reporter())```\nI think if you\u2019d like a better answer, I\u2019d suggest putting some example of your boilerplate asset code so you can get a better answer.", "Sure, fair enough. How about this:\n```@asset\ndef fetch_one():\n        return requests.get(\"<http://example.com/one>\").json()\n\n@asset\ndef fetch_two():\n        return requests.get(\"<http://example.com/two>\").json()\n    \n@asset\ndef clean_one(fetch_one):\n    df = pd.json_normalize(fetch_one)\n    return df.dropna()\n\n@asset\ndef clean_two(fetch_two):\n    df = pd.json_normalize(fetch_two)\n    return df.dropna()```\nNow, I have many one, two, .... endpoints", "Oh I see now, have you thought about making clean_json an Op since it seems to be doing compute on the asset passed to it?\n\nI think you\u2019re in the same boat I was. Everything in Dagster tutorial starts from the assets, but if you have an existing process, you should probably start from the Op and Job. Because One and Two could be partition keys. Even Fetch looks like an Op to me.", "I don't think I need Ops per se. I can rewrite it as assets. But the problem remains. Maybe I need partition keys?", "yeah maybe as\n\n```endpoint_partitions_def = StaticPartitionsDefinition([\"one\", \"two\", \"three\"])\n\n@asset(partitions_def=endpoint_partitions_def)\ndef raw_json_asset():\n    \u2026\n\n@asset(partitions_def=endpoint_partitions_def)\ndef cleaned_asset(raw_json_asset):\n   \u2026```", "Thx. I will try that...", "Hi Carl,\n\nWhat you should do here depends on whether you want to manage these as separate assets or not within Dagster-- it\u2019s hard to say whether you should do that without knowing more about your use case.\n\nIf you _do_ want to manage them as separate assets, but just want to cut down on boilerplate defining them, you could use an asset factory approach;\n\n```endpoints = [\"one\", \"two\"]\nassets=[]\n\n# necessary to create a new scope to make sure endpoint varies\ndef make_fetch(endpoint):\n    @asset(name=f\"fetch_{endpoint}\")\n    def fetch():\n        return requests.get(\"<http://example.com/{endpoint}>\").json()\n    return fetch\n\nfor endpoint in endpoints:\n    assets.append(make_fetch(endpoint))```\nIf you don\u2019t want to manage them as separate assets, then partitions make sense as you and Harry have been discussing.", "if you don\u2019t manage them as separate assets, <@U01GTMVMGQH> could you show how you would do the body of the raw_json_asset asset? I\u2019m trying to understand how you get the partition_key used.\n\n```@asset(partitions_def=endpoint_partitions_def)\ndef raw_json_asset():\n   return requests.get(f\"<http://example.com/{get_asset_partition_key()}>\").json()```\n`get_asset_partition_key` is a guess at a method", "Ah I see. So with seperate assets you mean that they represent different nodes in the graph? With partitions, it will be one node (but parametrized in a way). So the last option, would not allow parallel execution?", "&gt; if you don\u2019t manage them as separate assets, <@U01GTMVMGQH> could you show how you would do the body of the raw_json_asset asset? I\u2019m trying to understand how you get the partition_key used.\nUse `context.partition_key`:\n\n```@asset(partitions_def=endpoint_partitions_def)\ndef raw_json_asset(context: AssetExecutionContext):\n   return requests.get(f\"<http://example.com/{context.partition_key}>\").json()```\n&gt; Ah I see. So with seperate assets you mean that they represent different nodes in the graph? With partitions, it will be one node (but parametrized in a way).\nYes.\n\n&gt; So the last option, would not allow parallel execution?\nThe answer here is complicated. It will require multiple runs to materialize the two different partition keys-- those runs can be in parallel though. With Option 1, both endpoints can be done within the same run in parallel.", "Great. Thanks a lot. I will try that. Awesame that you guys have this kind of support :slightly_smiling_face:", "<@U01GTMVMGQH>, for the factory approach,\n```assets=[]\nfor endpoint in endpoints:\n    assets.append(make_fetch(endpoint))```\nyou\u2019d then add those assets in the Definitions block in the `__init__.py` ?", "&gt; you\u2019d then add those assets in the Definitions block in the __init__.py ?\nexactly"], "is_dagster_empl": [false, false, false, false, false, false, false, false, true, false, false, true, false, false, true], "gpt4_replies_target": "You can reduce the boilerplate code by using an asset factory approach where you create a new scope for each endpoint, or by using partitions if you don't want to manage them as separate assets. You can also use `context.partition_key` to get the partition key used.", "dagster_empl_first_target": "Hi Carl,\n\nWhat you should do here depends on whether you want to manage these as separate assets or not within Dagster-- it\u2019s hard to say whether you should do that without knowing more about your use case.\n\nIf you _do_ want to manage them as separate assets, but just want to cut down on boilerplate defining them, you could use an asset factory approach;\n\n```endpoints = [\"one\", \"two\"]\nassets=[]\n\n# necessary to create a new scope to make sure endpoint varies\ndef make_fetch(endpoint):\n    @asset(name=f\"fetch_{endpoint}\")\n    def fetch():\n        return requests.get(\"<http://example.com/{endpoint}>\").json()\n    return fetch\n\nfor endpoint in endpoints:\n    assets.append(make_fetch(endpoint))```\nIf you don\u2019t want to manage them as separate assets, then partitions make sense as you and Harry have been discussing.", "dagster_empl_last_target": "&gt; you\u2019d then add those assets in the Definitions block in the __init__.py ?\nexactly"}, {"question": "Question on materializing assets. I\u2019m migrating an existing C# pipeline. I\u2019ve made that work using Jobs and Ops with dagster-shell. I\u2019m at SourceAssets now. I want to use dagster\u2019s to help see gaps in the the date partition keys.\n1) How do I get externally created assets (files) into Dagster\u2019s asset system? Scheduled job for a Sensor op to emit AssetObservations of SourceAssets?\n2) Do I then write an IO manager to load and write these assets? Or is this optional.\n3) If I want to run backfills on the SourceAsset, do I make an Op that calls the execute_shell_script C# program with the keys to make the asset and then have it emit an AssetObservation?", "replies": ["&gt; 1) How do I get externally created assets (files) into Dagster\u2019s asset system? Scheduled job for a Sensor op to emit AssetObservations of SourceAssets?\nYes, model them as `SourceAssets`. One approach is to use <https://docs.dagster.io/concepts/assets/asset-observations#observable-source-assets|observable source assets> to generate `AssetObservations`.\n\n&gt; 2) Do I then write an IO manager to load and write these assets? Or is this optional.\nIf they are specified as dependencies in the usual way, then your IO manager will need to be able load them (not write them if they are source assets). Or you can use `non_argument_deps` and write whatever logic you want to load them inside of an upstream asset compute function (i.e. skip the IO manager)\n\n&gt; 3) If I want to run backfills on the SourceAsset, do I make an Op that calls the execute_shell_script C# program with the keys to make the asset and then have it emit an AssetObservation?\n\u201cBackfills of a SourceAsset\u201d is an undefined notion in Dagster, because a `SourceAsset` is by definition an asset for which Dagster does not control materializations.", "Ahh, that 3rd point is great. so if i do want to do backfills, i need to make it a regular asset. Thanks, I see now why assets are so important to dagster."], "is_dagster_empl": [true, false], "gpt4_replies_target": "To get externally created assets into Dagster's system, you can model them as `SourceAssets` and use observable source assets to generate `AssetObservations`. If these assets are specified as dependencies, your IO manager will need to load them. For backfills on the SourceAsset, you would need to make it a regular asset as Dagster does not control materializations for a `SourceAsset`.", "dagster_empl_first_target": "&gt; 1) How do I get externally created assets (files) into Dagster\u2019s asset system? Scheduled job for a Sensor op to emit AssetObservations of SourceAssets?\nYes, model them as `SourceAssets`. One approach is to use <https://docs.dagster.io/concepts/assets/asset-observations#observable-source-assets|observable source assets> to generate `AssetObservations`.\n\n&gt; 2) Do I then write an IO manager to load and write these assets? Or is this optional.\nIf they are specified as dependencies in the usual way, then your IO manager will need to be able load them (not write them if they are source assets). Or you can use `non_argument_deps` and write whatever logic you want to load them inside of an upstream asset compute function (i.e. skip the IO manager)\n\n&gt; 3) If I want to run backfills on the SourceAsset, do I make an Op that calls the execute_shell_script C# program with the keys to make the asset and then have it emit an AssetObservation?\n\u201cBackfills of a SourceAsset\u201d is an undefined notion in Dagster, because a `SourceAsset` is by definition an asset for which Dagster does not control materializations.", "dagster_empl_last_target": "&gt; 1) How do I get externally created assets (files) into Dagster\u2019s asset system? Scheduled job for a Sensor op to emit AssetObservations of SourceAssets?\nYes, model them as `SourceAssets`. One approach is to use <https://docs.dagster.io/concepts/assets/asset-observations#observable-source-assets|observable source assets> to generate `AssetObservations`.\n\n&gt; 2) Do I then write an IO manager to load and write these assets? Or is this optional.\nIf they are specified as dependencies in the usual way, then your IO manager will need to be able load them (not write them if they are source assets). Or you can use `non_argument_deps` and write whatever logic you want to load them inside of an upstream asset compute function (i.e. skip the IO manager)\n\n&gt; 3) If I want to run backfills on the SourceAsset, do I make an Op that calls the execute_shell_script C# program with the keys to make the asset and then have it emit an AssetObservation?\n\u201cBackfills of a SourceAsset\u201d is an undefined notion in Dagster, because a `SourceAsset` is by definition an asset for which Dagster does not control materializations."}, {"question": "Hi all,\n\nI was wondering if anyone has used Dagster to manage a reinforcement learning model. I have the training data set up as a daily partitioned asset and I'd like to set up the model's training runs similarly. My guess is that if I make the model partitions dependent on the prior day's partitions, this should work, but how do I make it so the first run doesn't error out due to a missing dependency?\n\nIdeally, this would work out such that if, on Monday, I found out that Saturday's training run failed, Sunday and Monday would get held up and I'd be able to \"catch the model up\" after fixing whatever the issue is.", "replies": ["Hey Rishi, IIUC this example exactly models the offset-by-1 pattern you describe: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#partitionmappings>", "Ah, in this case, the offset-by-1 dependency is on itself - the model asset wants to load yesterday's model and train with today's incremental data. I'm just a little confused how to provide some sort of configuration option to initialize the model from scratch for the very first run.", "Ah, check this out for self-dependencies. I think our docs are quite thin on this: <https://github.com/dagster-io/dagster/discussions/11829>", "Alright, I think I got a simple example working:\n\n```from typing import Optional\nfrom dagster import (\n    asset,\n    AssetIn,\n    HourlyPartitionsDefinition,\n    TimeWindowPartitionMapping,\n    Config,\n)\n\n\nclass MyAssetConfig(Config):\n    initialize_model: bool = False\n\n\n@asset(\n    partitions_def=HourlyPartitionsDefinition(start_date=\"2020-01-01-00:00\"),\n    ins={\n        \"asset1\": AssetIn(\n            partition_mapping=TimeWindowPartitionMapping(\n                start_offset=-1, end_offset=-1\n            ),\n            dagster_type=Optional[int],\n        )\n    },\n)\ndef asset1(config: MyAssetConfig, asset1: Optional[int]) -&gt; int:\n    if config.initialize_model:\n        return 1\n    if asset1 is None:\n        return ValueError(\"asset1 must be provided\")\n    return asset1 + 1```\nThe very first instance has to be materialized manually, but this throws an exception if `asset1` can't be found for some reason. The key was including `Optional`"], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "To manage a reinforcement learning model with Dagster, you can set up the model partitions to be dependent on the prior day's partitions. For the first run, you can provide a configuration option to initialize the model from scratch. If a training run fails, subsequent runs would be held up until the issue is fixed.", "dagster_empl_first_target": "Hey Rishi, IIUC this example exactly models the offset-by-1 pattern you describe: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#partitionmappings>", "dagster_empl_last_target": "Ah, check this out for self-dependencies. I think our docs are quite thin on this: <https://github.com/dagster-io/dagster/discussions/11829>"}, {"question": "Hi support, for partitioned jobs launched in a *single run* (see screenshot)/\n\nDo you need to loop on `_context_.asset_partition_key_range` and then return an array for each key? e.g range if the range is `['2023-06-20', '2023-06-21', '2023-06-22']` the asset would return `'hello', 'dagster', 'world'`\n\nIf that\u2019s correct what do you recommend doing in an IO Manager to prevent end users errors?\n\u2022 Check the length of the partition range is the same as the output? \n\u2022 Ask the end user to return a key value dictionary with the shape: `range_key: value` like\n```{\n    \"2023-06-20\": \"hello\",\n    \"2023-06-21\": \"dagster\",\n    \"2023-06-22\": \"world\"\n}```\n\u2022 Else?\nThanks in advance, and sorry I couldn\u2019t find docs regarding this.", "replies": ["I actually posted something related to this: <https://dagster.slack.com/archives/C01U5LFUZJS/p1687941747304719>", "Hey Chris, this is an area of active development. Currently the value returned needs to depend on the IO Manager you are using-- the IO manager will receive the value and is responsible for breaking it up and writing it to disk, so that value can be anything the IO Manager can digest.\n\nHowever, we will soon be introducing a container object (probably called `PartitionedOutput`) that you will return: <https://github.com/dagster-io/dagster/pull/14621>", "&gt; what do you recommend doing in an IO Manager to prevent end users errors?\nJust validate whatever structure you\u2019re expecting, as with the dictionary example you provided.", "Alright perfect, thanks for the super quick answer", "<@U01GTMVMGQH> I have kinda of follow up question. Let\u2019s say I have an asset with 5 partitions that is the upstream of an non-partitioned asset. Do I understand correctly that I need my IO Manager to collect all 5 partitions and then wrap it in an array that will become the input of the downstream? Is there a way to call the IO Manager X amount of times and return each partitions as a separate input argument", "&gt; Do I understand correctly that I need my IO Manager to collect all 5 partitions and then wrap it in an array that will become the input of the downstream?\nYes, although it doesn\u2019t need to be an array, you can return any value. I believe our default `UPathIOManager` returns a dict keyed by partition key.\n\n&gt; Is there a way to call the IO Manager X amount of times and return each partitions as a separate input argument\nThere is presently no way to do this.", "Gotcha thanks really helpful"], "is_dagster_empl": [false, true, true, false, false, true, false], "gpt4_replies_target": "The value returned from partitioned jobs in a single run needs to depend on the IO Manager you are using, as it is responsible for breaking it up and writing it to disk. To prevent end user errors, you can validate the structure you're expecting, such as the dictionary example provided. If you have an asset with multiple partitions that is the upstream of a non-partitioned asset, your IO Manager needs to collect all partitions and wrap it in a value that will become the input of the downstream, but there is currently no way to call the IO Manager multiple times and return each partition as a separate input argument.", "dagster_empl_first_target": "Hey Chris, this is an area of active development. Currently the value returned needs to depend on the IO Manager you are using-- the IO manager will receive the value and is responsible for breaking it up and writing it to disk, so that value can be anything the IO Manager can digest.\n\nHowever, we will soon be introducing a container object (probably called `PartitionedOutput`) that you will return: <https://github.com/dagster-io/dagster/pull/14621>", "dagster_empl_last_target": "&gt; Do I understand correctly that I need my IO Manager to collect all 5 partitions and then wrap it in an array that will become the input of the downstream?\nYes, although it doesn\u2019t need to be an array, you can return any value. I believe our default `UPathIOManager` returns a dict keyed by partition key.\n\n&gt; Is there a way to call the IO Manager X amount of times and return each partitions as a separate input argument\nThere is presently no way to do this."}, {"question": "Hi there! It seems that the answer is no from poking around the repo, but is there an IO Manager for Redshift out of the box?", "replies": ["Is it recommended to just use S3? I see there's a bundled IO manager for S3, but wasn't positive on the performance implications of leveraging pickling + s3 over a db in redshift", "Hi Armando, There is no out-of-the-box redshift IO manager but we do provide a redshift client resource that you can use in either a custom IO manager or just to read/write directly from ops:\n\n<https://docs.dagster.io/_apidocs/libraries/dagster-aws#redshift>\n\n&gt; Is it recommended to just use S3?\nUnfortunately I can\u2019t really speak to this, I\u2019m not very knowledgable about S3/Redshift and their differences.", "Oh interesting, ok will dig into those docs, thank you!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "There is no out-of-the-box Redshift IO Manager in Dagster, but a Redshift client resource is provided which can be used in a custom IO manager or to read/write directly from operations.", "dagster_empl_first_target": "Hi Armando, There is no out-of-the-box redshift IO manager but we do provide a redshift client resource that you can use in either a custom IO manager or just to read/write directly from ops:\n\n<https://docs.dagster.io/_apidocs/libraries/dagster-aws#redshift>\n\n&gt; Is it recommended to just use S3?\nUnfortunately I can\u2019t really speak to this, I\u2019m not very knowledgable about S3/Redshift and their differences.", "dagster_empl_last_target": "Hi Armando, There is no out-of-the-box redshift IO manager but we do provide a redshift client resource that you can use in either a custom IO manager or just to read/write directly from ops:\n\n<https://docs.dagster.io/_apidocs/libraries/dagster-aws#redshift>\n\n&gt; Is it recommended to just use S3?\nUnfortunately I can\u2019t really speak to this, I\u2019m not very knowledgable about S3/Redshift and their differences."}, {"question": "Hello, is there any way to surface regular logging like `<http://logger.info|logger.info>` to show up in job run logs just like `<http://context.log.info|context.log.info>` ? Just to avoid passing context into other functions", "replies": ["Hi Peter, Have a look at this doc: <https://docs.dagster.io/concepts/logging/python-logging#capturing-python-logs->", "oh perfect, thank you!!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Yes, you can surface regular logging like `logger.info` to show up in job run logs just like `context.log.info` by following the instructions in this documentation: <https://docs.dagster.io/concepts/logging/python-logging#capturing-python-logs->.", "dagster_empl_first_target": "Hi Peter, Have a look at this doc: <https://docs.dagster.io/concepts/logging/python-logging#capturing-python-logs->", "dagster_empl_last_target": "Hi Peter, Have a look at this doc: <https://docs.dagster.io/concepts/logging/python-logging#capturing-python-logs->"}, {"question": "I have Dagster setup on AWS ECS. It works, but now I want to add a kind of E2E test, where I start given job and later check if that job made modifications to S3, database etc. I am unable to find a way to launch Dagster Job on AWS ECS *without* using Dagit (for example: trigger it in code using some command).\n*Is there any way to trigger a Dagster Job on AWS ECS from code?* E.g. start it using CLI?\n\nIf that is impossible, then I have idea for a workaround. To give you more details: I deploy the AWS ECS Dagster infrastructure using AWS CDK - I have a separate stack just for those E2E. I want to trigger the Dagster job to run as soon as that AWS CDK stack with infrastructure is deployed. Maybe it is possible to set Scheduler to run the Job as soon as possible - however, how can I force it so that this job is run only once?\n\nThanks in advance! :wink:", "replies": ["Hi Piotr, I\u2019m not very knowledgable on AWS, but is the host/port of the gRPC server hosting your code location exposed? Then you can use:\n\n```$ dagster job launch --grpc-host=&lt;host&gt; --grpc-port=&lt;port&gt; --job=&lt;job-name&gt; ...```\nSee `dagster job launch --help` for details.", "Hello <@U01GTMVMGQH>, thanks for the response! gRPC is not exposed (it is in private VPC), but maybe I could make it work somehow. Thanks for the suggestion!", "FWIW there is also the GQL API and GQL Python client, which will go through the dagit process (rather than direct to code server via grpc):\n\n<https://docs.dagster.io/concepts/dagit/graphql>"], "is_dagster_empl": [true, false, true], "gpt4_replies_target": "You can trigger a Dagster Job on AWS ECS from code by using the gRPC server hosting your code location, provided the host/port is exposed. You can use the command `$ dagster job launch --grpc-host=<host> --grpc-port=<port> --job=<job-name> ...`. Alternatively, you can use the GQL API and GQL Python client which will go through the Dagit process.", "dagster_empl_first_target": "Hi Piotr, I\u2019m not very knowledgable on AWS, but is the host/port of the gRPC server hosting your code location exposed? Then you can use:\n\n```$ dagster job launch --grpc-host=&lt;host&gt; --grpc-port=&lt;port&gt; --job=&lt;job-name&gt; ...```\nSee `dagster job launch --help` for details.", "dagster_empl_last_target": "FWIW there is also the GQL API and GQL Python client, which will go through the dagit process (rather than direct to code server via grpc):\n\n<https://docs.dagster.io/concepts/dagit/graphql>"}, {"question": "Hello, I am trying to run this command \"`dagster project from-example --example tutorial --name my-tutorial-project`\" in my terminal which is provided by your dagster tutorial. I expect to create a new directory called \"tutorial-project\" but there is nothing even if I got a success message. Besides, the name 'tutorial-project' seems weird, I am wondering why it is not 'my-tutorial-project' which is set as parameter 'name'. The success message shows as belows:\n```Success! Created my-tutorial-project at /Users/xxx/Desktop/xxxxx/dagster/my-tutorial-project.```\nI don't see any changes in the directory after getting this message, neither \"tutorial-project\" nor \"my-tutorial-project\"", "replies": ["Hi Haishuo, I just tried this in an empty directory and it worked:\n\n```dagster project from-example --example tutorial --name my-tutorial-project\nDownloading example 'tutorial'. This may take a while.\nSuccess! Created my-tutorial-project at /Users/smackesey/stm/code/elementl/oss/_scripts/testgen/my-tutorial-project.\nls\nmy-tutorial-project                 my-tutorial-project_dbt_dagster     my-tutorial-project_notebook_assets```\nWhat version of dagster do you have?", "&gt; Besides, the name \u2018tutorial-project\u2019 seems weird, I am wondering why it is not \u2018my-tutorial-project\u2019 which is set as parameter \u2018name\u2019. \nYou are right, that\u2019s an error in our docs (it will actually create `my-tutorial-project`). I have pushed a fix: <https://github.com/dagster-io/dagster/pull/15014>", "Thank you for quick and kind reply, Sean. I am also wondering is there any way to make contributions to dagster development for developers out of your team?", "Have you been able to get it to work?\n\n&gt; I am also wondering is there any way to make contributions to dagster development for developers out of your team?\nYes, you can submit pull requests on our Github: <https://github.com/dagster-io/dagster>", "No, it doesn't work. I try to run this under a new empty directory but I failed.", "(base) haishuochen@xxx dagster % cd new\n(base) haishuochen@xxx new % dagster project from-example --example tutorial --name my-tutorial-project\nDownloading example 'tutorial'. This may take a while.\nSuccess! Created my-tutorial-project at /Users/xxx/dagster/new/my-tutorial-project.\n(base) haishuochen@xxx new % ls", "it is still an empty directory", "That is weird. What does `dagster --version` give?", "<@U03AJ8UAPDW> have you ever seen this before? Haishuo is running the scaffolding step of the tutorial and it says it is creating a directory but apparently nothing is being created.", "dagster, version 1.3.11", "Just a sanity check, does this work inside of `new`:\n\n```$ mkdir foo\n$ ls```", "yes", "Hi! Sorry to hear you're bumping into this.\n\nAre you seeing that a new directory called `my-tutorial-project` is made, but the directory is empty?", "Yes, I and running this under the `new` folder and expect to get\n```-new\n---my-tutorial-project```\nbut I run ls and find the `new` folder is empty", "I searched on your github for other examples like 'deploy_ecs' . It doesn't work either.", "Going to lunch. We can continue conversation in one hour! Thank you, Tim and Sean", "Hi <@U03AJ8UAPDW> how is the progress\uff1f", "Hi! Tried some permutations of the current and old Dagster versions and can't seem to reproduce it.\n\nDo you happen to work at a company with heavy firewalls? The command runs a download, so it might be blocking the request and we're not catching the exception properly", "Yes, I am using company's internal network. But I also tried with my iphone's hotspot and nothing changes", "Apologies for this!\n\nWhile I figure this out, here's a workaround for you:\n\nthe contents of that directory are made from here: <https://github.com/dagster-io/dagster/tree/master/examples/tutorial> You can clone the repo and grab that part, or copy+paste the files", "Okay, thank you, please also notify me when you fix the problem.", "Hi haishuo, we\u2019ve identified and fixed the problem and are rolling out a new release today with the fix. Thank you for the report!", "Awesome! Thank you!\nI was facing this issue too", "It works. Thank you Sean."], "is_dagster_empl": [true, true, false, true, false, false, false, true, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false], "gpt4_replies_target": "The issue you're experiencing with the Dagster command not creating the expected directory could be due to your company's internal network firewall blocking the download request. A workaround is to manually clone the repository and copy the necessary files. The Dagster team has been notified of this issue and is working on a fix.", "dagster_empl_first_target": "Hi Haishuo, I just tried this in an empty directory and it worked:\n\n```dagster project from-example --example tutorial --name my-tutorial-project\nDownloading example 'tutorial'. This may take a while.\nSuccess! Created my-tutorial-project at /Users/smackesey/stm/code/elementl/oss/_scripts/testgen/my-tutorial-project.\nls\nmy-tutorial-project                 my-tutorial-project_dbt_dagster     my-tutorial-project_notebook_assets```\nWhat version of dagster do you have?", "dagster_empl_last_target": "Hi haishuo, we\u2019ve identified and fixed the problem and are rolling out a new release today with the fix. Thank you for the report!"}, {"question": "Hi there, I'm still new to Dagster, so I'm not positive on the terminology to look up, but I was curious what the recommended path was to parameterize a pipeline, I essentially want to apply the same set of transformations on the same type of data on distinct database instances and have say N pipeline that I can individually monitor and schedule, but running the same code, with different inputs (the credentials to the database)\n\nIt seems like resources sort of lets you parameterize, but doesn't let you have N pipelines?", "replies": ["Is the idea that I should have one pipeline with N jobs that all have different resource parameters?\nAnd then persist them to different spots? How would i reference all these tables in a downstream job (ex, after I pull these tables from this db, I want to union them together)", "It is also worth noting the N is known before hand, we have a set of N static configurations of DB credentials", "You can parameterize a job using run configuration: <https://docs.dagster.io/concepts/configuration/config-schema>", "And then how do you then consume the distinct outputs downstream?", "The full workflow I'm trying to do:\n\u2022 Pull data from multiple data sources with the same schema and data types\n\u2022 Do a set of common transforms across this data, given the data is the same shape\n\u2022 Union that data together for use in additional transforms", "Depends how you architect this.\n\u2022 You could have `1` job that dynamically triggers `N` subgraphs to do the transformations. Then you collect the data from the `N` subgraphs for downstream computations.\n\u2022 You could have `N` jobs that are each parametrized by what data source they are operating on. Then there\u2019s a downstream job that runs after these `N` jobs are completed to trigger downstream computation.\n\u2022 You could use partitions to model each of the data sources\n\u2022 etc", "Lemme look through the docs to see if I can make sense of that!", "Oh partitioning looks really promising, is there a way to have the job continue despite one of the partitions fails? Use case I'm thinking is to continue to run the pipeline even if we can't read from one of the DBs for whatever reason", "Hey Armando, If you use partitions then typically each partition will be processed in a separate run (of the same job), so yes, if the run processing one partition fails it won\u2019t affect the others."], "is_dagster_empl": [false, false, true, false, false, true, false, false, true], "gpt4_replies_target": "You can parameterize a job using run configuration and use partitions to model each of the data sources. Each partition will be processed in a separate run, so if one fails, it won't affect the others. You can architect your job to either dynamically trigger subgraphs for transformations and collect the data for downstream computations, or have separate jobs for each data source and a downstream job that runs after these are completed.", "dagster_empl_first_target": "You can parameterize a job using run configuration: <https://docs.dagster.io/concepts/configuration/config-schema>", "dagster_empl_last_target": "Hey Armando, If you use partitions then typically each partition will be processed in a separate run (of the same job), so yes, if the run processing one partition fails it won\u2019t affect the others."}, {"question": "I'm trying to limit the number of ecs containers that dagster launches. Will using a `QueuedRunCoordinator` in conjunction with the `EcsRunLauncher` accomplish that? i.e.\n```run_coordinator:\n  module: dagster.core.run_coordinator\n  class: QueuedRunCoordinator\n  config:\n    max_concurrent_runs: 5\n    tag_concurrency_limits:\n      - key: \"database\"\n        value: \"redshift\"\n        limit: 4\n      - key: \"dagster/backfill\"\n        limit: 5```", "replies": ["Yes, I think that will do it\u2026."], "is_dagster_empl": [true], "gpt4_replies_target": "Yes, using a `QueuedRunCoordinator` in conjunction with the `EcsRunLauncher` will help limit the number of ecs containers that dagster launches.", "dagster_empl_first_target": "Yes, I think that will do it\u2026.", "dagster_empl_last_target": "Yes, I think that will do it\u2026."}, {"question": "I have a sensor that yields multiple RunRequests for a configured job. There is an op in that job that generates a list of processes that we want to run dynamically, however we want them to run sequentially. Is there some way to chain together ops in a job during fan-out to run sequentially, or some way I can specify the max concurrency of ops in executor in the job definition or in the RunRequest from the sensor?", "replies": ["ah it looks like it's inside the job config? <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#controlling-job-execution>", "Yes, `max_concurrent` in the config for multiprocess executor is your friend here.", "I added this to the config_mapping that my job is using:\n```@config_mapping(config_schema={\"sql_conn_name\": str, \"schema_name\": str, \"table_name\": str, \"logical_pipe_id\": int})\ndef index_defrag_simplified_config(val):\n    return {\n        \"execution\": {\n            \"config\": {\n                \"multiprocess\": {\n                    \"start_method\": {\n                        \"forkserver\": {},\n                    },\n                    \"max_concurrent\": 1,\n                },\n            }\n        },\n        \"ops\": {```", "but it looks like it didn't function correctly", "all the processes attempted to run at the same time", "The event messages say `Launching subprocess for` is that expected in multiprocess, or is it using a different executor?", "Hmmm-- if you `dagit -f` this file and launch a job, does it only one run at a time? It does for me:\n\n```from time import sleep\nfrom dagster import op, job\n\n@op\ndef my_op():\n    sleep(1)\n    return 1\n\n@job(config={\"execution\": {\"config\": {\"multiprocess\": {\"max_concurrent\": 1}}}})\ndef my_job():\n    my_op()\n    my_op()\n    my_op()\n    my_op()```", "but that's not fanned-out, right? is that the same behavior?", "trying to build up from the simplest case here, let me put together a fan-out minimal", "for context, this is to rebuild indexes on a sql server table using the ONLINE=on option. The errors I'm getting are:\n```[SQL Server]Could not proceed with index DDL operation on table 'Team' because it conflicts with another concurrent operation that is already in progress on the object. The concurrent operation could be an online index operation on the same object or another concurrent operation that moves index pages like DBCC SHRINKFILE.```", "which is why I believe they're running concurrently", "on retry, they work, which is another hint", "another semi-relevant question, is there any sort of insight in the number of retries an op has taken from the `OpExecutionContext` ?", "nvm, looks like it's `retry_number`", "`max_concurrent` is working for me here:\n\n```from time import sleep\nfrom typing import List\n\nfrom dagster import job, op\nfrom dagster._core.definitions.events import DynamicOutput\nfrom dagster._core.definitions.output import DynamicOut\n\n\n@op\ndef my_op():\n    sleep(1)\n    return 1\n\n\n@op(out=DynamicOut())\ndef foo():\n    for idx in range(4):\n        yield DynamicOutput(idx, mapping_key=str(idx))\n\n\n@op\ndef bar(x: int):\n    sleep(1)\n    return x\n\n\n@op\ndef baz(x: List[int]):\n    return sum(x)\n\n\n@job(\n    config={\n        \"execution\": {\n            \"config\": {\"multiprocess\": {\"start_method\": {\"forkserver\": {}}, \"max_concurrent\": 2}}\n        }\n    }\n)\ndef my_job():\n    parts = foo()\n    parts_2 = parts.map(bar)\n    baz(parts_2.collect())```", "hm, I'll try to continue playing with it, maybe there's something else on the database going on I'm not aware of. Thank you!", "ok, just confirm that the above works-- I think `max_concurrent` was added relatively recently", "ah heres a better pic of what I'm seeing with the timing:", "And when I look at the run config, I don't see the execution section at all:", "although maybe that's because I'm using a simplified config?", "Yeah you don\u2019t see it because you\u2019re using config mapping, so your input isn\u2019t in the run config format.\n\nBTW what OS are you on?", "Windows 11 with Docker running on WSL", "I see-- can you try my simple examples?", "I wonder if `max_concurrent` just doesn\u2019t work on windows.", "sure, we're also on dagster version `1.3.10` FYI", "I've never tried just running a dagster file before, how do I use `dagit -f` on your test file? All of our environments are set up in docker images, so I've never tried installing or setting up dagster to run locally", "figured it out, got this error:\n```\nError\n\nCopy\ndagster._core.errors.DagsterUnmetExecutorRequirementsError: The selected start_method 'forkserver' is not available. Only ['spawn'] are valid options on win32 python 3.10.4 (tags/v3.10.4:9d38120, Mar 23 2022, 23:13:41) [MSC v.1929 64 bit (AMD64)].\n  File \"C:\\Users\\Danny\\AppData\\Roaming\\Python\\Python310\\site-packages\\dagster\\_core\\execution\\context_creation_job.py\", line 302, in orchestration_context_event_generator\n    executor = create_executor(context_creation_data)\n  File \"C:\\Users\\Danny\\AppData\\Roaming\\Python\\Python310\\site-packages\\dagster\\_core\\execution\\context_creation_job.py\", line 392, in create_executor\n    return creation_fn(init_context)\n  File \"C:\\Users\\Danny\\AppData\\Roaming\\Python\\Python310\\site-packages\\dagster\\_core\\definitions\\executor_definition.py\", line 522, in multi_or_in_process_executor\n    return _core_multiprocess_executor_creation(\n  File \"C:\\Users\\Danny\\AppData\\Roaming\\Python\\Python310\\site-packages\\dagster\\_core\\definitions\\executor_definition.py\", line 334, in _core_multiprocess_executor_creation\n    return MultiprocessExecutor(\n  File \"C:\\Users\\Danny\\AppData\\Roaming\\Python\\Python310\\site-packages\\dagster\\_core\\executor\\multiprocess.py\", line 135, in __init__\n    raise DagsterUnmetExecutorRequirementsError(```", "going to remove forkserver from the config", "ah I guess my dagster image is actually based on Alpine linux, not windows, apologies on the wrong information there :facepalm:", "oddly enough, that file didn't even have concurrency of 2, it had concurrency of 1. I made no changes to your code", "", "Looks like success for the example?", "No, the concurrency was set to 2, but it was only spawning 1 for some reason", "Hmm\u2026 so did you run that in windows or alpine linux", "windows", "Can you set `max_concurrent` to 4 and retry (you can just change it in the launchpad without reloading defs)", "That did 3 for some reason", "interesting, so it looks like it\u2019s -1", "I think he\u2019s probably done for the day but I\u2019m going to page <@UH3RM70A2> who I think has done some of the `max_concurrent` work and might\u2019ve seen cross-OS variation before\n\n(alex the issue is that `max_concurrent` is behaving weirdly for Danny, on windows it appears to be acting like -1 wrt parallelized ops; on linux getting ignored)", "I tried setting `max_concurrent` in my code to 2, but it's still running all the processes at once. Could there be an issue with using a simplified config in the op?", "&gt; I tried setting max_concurrent in my code to 2,\nBut when set to 2 for the example it gives you 1 at a time right?", "yep", "but it just seems like whatever I set `max_concurrent` to in my code, it just ignores", "is there a way to grab the job config and output it inside of an op?", "the process init time (the blue bar) is part of the step executing, so the images above do reflect the concurrency settings of 2 and 4\n\nas to why they are staggered instead of all progressing immediately, my first guess would be insufficient resources available to the container.\n\nyou can use tags to do more targeted concurrency controls for just the ops that are doing conflicting SQL statements\n\n<https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines#limiting-concurrency-using-tags>", "I'm a bit confused by what you mean. Doesn't this picture show that 3 ops are executing concurrently, even though our `max_concurrency` is set to 1? <https://dagster.slack.com/archives/C01U954MEER/p1688071379583469?thread_ts=1687990338.815749&amp;cid=C01U954MEER>", "i added :four:  and :two:  to the images I was speaking towards", "to the original issue it seems there may be a problem specifying the execution config via `config_mapping`", "if you want `max_concurrent: 1` you could try setting `executor_def=in_process_executor`", "using the in_process_executor certainly worked, thank you very much, that thought didn't even occur to me", "it does seem like that config is an issue using config mapping however", "would you be up for filing an issue for that?", "Sure thing!", "<https://github.com/dagster-io/dagster/issues/15061>"], "is_dagster_empl": [false, true, false, false, false, false, true, false, true, false, false, false, false, false, true, false, true, false, false, false, true, false, true, true, false, false, false, false, false, false, false, true, false, true, false, true, false, true, true, false, true, false, false, false, true, false, true, true, true, false, false, true, false, false], "gpt4_replies_target": "To run ops sequentially in a job during fan-out, you can use the `max_concurrent` parameter in the config for the multiprocess executor. However, there seems to be an issue with specifying the execution config via `config_mapping`. As a workaround, you can use `executor_def=in_process_executor` to ensure `max_concurrent: 1`.", "dagster_empl_first_target": "Yes, `max_concurrent` in the config for multiprocess executor is your friend here.", "dagster_empl_last_target": "would you be up for filing an issue for that?"}, {"question": "Hi - I have a job which runs every hour to collect and report on any/all runs which have failed in the last hour. I am attempting to gather the assets from any job which materializes assets that were affected by the failure and did not successfully materialize.\n\n(Yes, I know there's a run failure sensor but it would send an email for each failure during that period while I need a single email containing *all* failures during that period)\n\nTo that end, I need to write a unit test which can test fetching context and failure event data from (hopefully mocked) failed assets.\n\nAnyone have any suggestions on how I could accomplish this?", "replies": ["Hi Adam, There\u2019s a function called `instance_for_test` that\u2019s part of our public API. We use this extensively internally as well. You use it like this:\n\n```from dagster import instance_for_test\n\nwith instance_for_test() as instance:\n    some_job.execute_in_process(instance=instance)\n    ...```\nYou can use this approach to populate the test instance with various failed runs. This should allow you to set it up appropriately to test your job.", "That is fantastic! Thank you!", "Are the event logs all in memory or are they stored in the same default Sqlite DB?", "pretty sure it uses sqlite but just in a tmpdir:\n\n<https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/instance_for_test.py#L16-L82>"], "is_dagster_empl": [true, false, false, true], "gpt4_replies_target": "You can use the `instance_for_test` function from Dagster's public API to populate the test instance with various failed runs, which will allow you to set up your job appropriately for testing. This function uses SQLite in a temporary directory to store event logs.", "dagster_empl_first_target": "Hi Adam, There\u2019s a function called `instance_for_test` that\u2019s part of our public API. We use this extensively internally as well. You use it like this:\n\n```from dagster import instance_for_test\n\nwith instance_for_test() as instance:\n    some_job.execute_in_process(instance=instance)\n    ...```\nYou can use this approach to populate the test instance with various failed runs. This should allow you to set it up appropriately to test your job.", "dagster_empl_last_target": "pretty sure it uses sqlite but just in a tmpdir:\n\n<https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/instance_for_test.py#L16-L82>"}, {"question": "Hey Team, getting this error in our OSS implementation all of a sudden. We have been using Python GraphQL Client for a while but never got this before..\n\n```sqlalchemy.exc.OperationalError: (psycopg2.OperationalError) SSL connection has been closed unexpectedly\n\n[SQL: INSERT INTO event_logs (run_id, event, dagster_event_type, timestamp, step_key, asset_key, partition) VALUES (%(run_id)s, %(event)s, %(dagster_event_type)s, %(timestamp)s, %(step_key)s, %(asset_key)s, %(partition)s) RETURNING event_logs.run_id, event_logs.id]\n[parameters: {'run_id': '28f30b2d-f74a-443e-842d-98d901921509', 'event': '{\"__class__\": \"EventLogEntry\", \"dagster_event\": {\"__class__\": \"DagsterEvent\", \"event_specific_data\": null, \"event_type_value\": \"PIPELINE_ENQUEUED\", \" ... (256 characters truncated) ... ate_customer_infrastructure\", \"run_id\": \"28f30b2d-f74a-443e-842d-98d901921509\", \"step_key\": null, \"timestamp\": 1688011188.979952, \"user_message\": \"\"}', 'dagster_event_type': 'PIPELINE_ENQUEUED', 'timestamp': datetime.datetime(2023, 6, 29, 3, 59, 48, 979952), 'step_key': None, 'asset_key': None, 'partition': None}]\n(Background on this error at: <https://sqlalche.me/e/20/e3q8>)\n\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 2108, in submit_run\n    SubmitRunContext(run, workspace=workspace)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/run_coordinator/queued_run_coordinator.py\", line 234, in submit_run\n    self._instance.report_dagster_event(enqueued_event, run_id=dagster_run.run_id)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 1998, in report_dagster_event\n    self.handle_new_event(event_record)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 1916, in handle_new_event\n    self._event_storage.store_event(event)\n  File \"/usr/local/lib/python3.7/site-packages/dagster_postgres/event_log/event_log.py\", line 179, in store_event\n    SqlEventLogStorageTable.c.run_id, SqlEventLogStorageTable.c.id\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1416, in execute\n    execution_options or NO_OPTIONS,\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/sql/elements.py\", line 484, in _execute_on_connection\n    self, distilled_params, execution_options\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1647, in _execute_clauseelement\n    cache_hit=cache_hit,\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1847, in _execute_context\n    dialect, context, statement, parameters\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1988, in _exec_single_context\n    e, str_statement, effective_parameters, cursor, context\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 2344, in _handle_dbapi_exception\n    raise sqlalchemy_exception.with_traceback(exc_info[2]) from e\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1969, in _exec_single_context\n    cursor, str_statement, effective_parameters, context\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 920, in do_execute\n    cursor.execute(statement, parameters)\n\nThe above exception was caused by the following exception:\npsycopg2.OperationalError: SSL connection has been closed unexpectedly\n\n\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1969, in _exec_single_context\n    cursor, str_statement, effective_parameters, context\n  File \"/usr/local/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 920, in do_execute\n    cursor.execute(statement, parameters)```", "replies": ["Hi Abhishek, This looks like a networking error-- can you say a little more about the conditions under which this is arising?", "Hmm actually everything was working and then one of the jobs we triggered via the GraphQL client failed.. but then after some time it was working fine.. so I am not sure why this happened", "maybe we can ignore for now"], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "The error you encountered seems to be a temporary networking issue, as it resolved itself after some time. If it recurs, further investigation into your network conditions and job triggering process may be necessary.", "dagster_empl_first_target": "Hi Abhishek, This looks like a networking error-- can you say a little more about the conditions under which this is arising?", "dagster_empl_last_target": "Hi Abhishek, This looks like a networking error-- can you say a little more about the conditions under which this is arising?"}, {"question": "hello everyone, I am new to dagster.\nI am trying to create dasgter in local docker. I followed the example <https://github.com/dagster-io/dagster/tree/1.2.4/examples/deploy_docker|here>.\nhowever in the example dagster picks up all the assets and code from the repo.py file which is a single file. in my usecase, I am creating a folder called \u201cetl\u201d which will have the assets, schedules and other job files. is there a way to pick up all files under a folder when using the gRPC command? currently it is\nlike this\nCMD [\u201cdagster\u201d, \u201capi\u201d, \u201cgrpc\u201d, \u201c-h\u201d, \u201c0.0.0.0\", \u201c-p\u201d, \u201c4000\", \u201c-f\u201d, \u201c./repo.py\u201d]\nhow would I change this to pick up all files from under a folder?", "replies": ["You should be able to use the `-m` command instead of `-f` assuming your `etl` folder is a python module with an `__init__.py` in it", "it does not seem to work.\nthis is how my repo is structured:\nand also my docker file is\n\n```FROM python:3.7-slim\n\n# Checkout and install dagster libraries needed to run the gRPC server\n# exposing your repository to dagit and dagster-daemon, and to load the DagsterInstance\n\nRUN pip install \\\n    dagster \\\n    dagster-postgres \\\n    dagster-docker\n\n# Add repository code\n\nWORKDIR /opt/dagster/app\n\nCOPY etl /opt/dagster/app\n\n# Run dagster gRPC server on port 4000\n\nEXPOSE 4000\n\n# CMD allows this to be overridden from run launchers or executors that want\n# to run other commands against your repository\nCMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"/opt/dagster/app\"]```", "I think you would also need a pyproject.toml and setup.py -\npyproject.toml\n```[build-system]\nrequires = [\"setuptools\"]\nbuild-backend = \"setuptools.build_meta\"\n\n[project]\nname = \"etl project name\"\nversion = \"0.0.1\"\nauthors = [\n    { name=\"your name\", email=\"your email\" }\n]\n\n[tool.dagster]\nmodule_name = \"etl\"```", "I think it works nw. thanks for the help"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "To pick up all files from a folder in Dagster using the gRPC command, you can use the `-m` command instead of `-f`, assuming your `etl` folder is a python module with an `__init__.py` in it. You would also need a `pyproject.toml` and `setup.py` file.", "dagster_empl_first_target": "You should be able to use the `-m` command instead of `-f` assuming your `etl` folder is a python module with an `__init__.py` in it", "dagster_empl_last_target": "I think it works nw. thanks for the help"}, {"question": "Is there a way for a Job to generate RunRequests for another job? I know it\u2019s something you\u2019d usually do with a sensor, but I have a case where doing this from a job would be more convenient. thanks!", "replies": ["Would something like the `execute_in_process` method work?\n\n<https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#python-apis>", "No, I don\u2019t think so :slightly_smiling_face: I want to generate ~500 job runs of another job and let them run as independent jobs in their own k8s pods.\n(eventually we are planning to solve this with assets and partitions, but the partitioning in our case is a bit tricky)", "You can use the graphQL API for this. I don' think you can yield RunRequests directly from a job", "<https://docs.dagster.io/concepts/dagit/graphql-client#submitting-a-job-run>", "Thanks <@U0300665S8G>!"], "is_dagster_empl": [false, false, false, false, false], "gpt4_replies_target": "While you cannot yield RunRequests directly from a job, you can use the graphQL API to generate job runs of another job and let them run as independent jobs in their own k8s pods.", "dagster_empl_first_target": "Would something like the `execute_in_process` method work?\n\n<https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#python-apis>", "dagster_empl_last_target": "Thanks <@U0300665S8G>!"}, {"question": "I have a daily partitioned asset that depends on a monthly partitioned asset.  When I try to materialize the daily asset, I get \"Invariant failed. Description: Can't supply a PartitionedConfig for 'config' with a different PartitionsDefinition than supplied for 'partitions_def'.\"\nwhat would cause that?", "replies": ["mind sharing some code snippets?", "```@asset(\n    partitions_def=MonthlyPartitionsDefinition(start_date=\"2017-01-01\", end_date=\"2020-01-01\")\n)\ndef rtlmp5min_monthly_zip_archive(context):\n\t...\n\n@asset(\n    partitions_def=DailyPartitionsDefinition(start_date=\"2017-01-01\", end_date=\"2020-01-01\"),\n    non_argument_deps={\"rtlmp5min_monthly_zip_archive\"},\n    io_manager_key=\"feather_io_manager\"\n)\ndef rtlmp5min_daily_archive(context):\n\t...```", "I think it was related to my sqllite db being corrupted. because I switched to postgresql, reran the exact same code and it worked perfectly..."], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "The error message you're seeing might be due to a corrupted SQLite database. Switching to PostgreSQL and rerunning the same code seemed to resolve the issue.", "dagster_empl_first_target": "mind sharing some code snippets?", "dagster_empl_last_target": "mind sharing some code snippets?"}, {"question": "trying to use Cloud Hybrid, got the agent working, but when i set up the code location, it's giving this error:\n```Task logs:\napi: line 1: dagster: command not found```\nmy Dockerfile base image is python:3.8, and dagster and dagster-cloud are in my requirements.txt. i even shelled into the container to confirm it's in `pip freeze`", "replies": ["actually i think i know what's going, one sec", "got it. it was a series of problems:\n\u2022 image not updated/reloaded\n\u2022 python file in wrong place\n\u2022 python file name (dagster.py) colliding with executable\nall good now! :rubberduck:"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The user resolved the issue by updating and reloading the image, moving the python file to the correct location, and renaming the python file to avoid collision with the executable.", "dagster_empl_first_target": "actually i think i know what's going, one sec", "dagster_empl_last_target": "got it. it was a series of problems:\n\u2022 image not updated/reloaded\n\u2022 python file in wrong place\n\u2022 python file name (dagster.py) colliding with executable\nall good now! :rubberduck:"}, {"question": "Just want to confirm that there currently is no way to synchronize io_managers between repos?  I would like to have repos share assets and thus am considering creating source assets in all repos to allow them to use 'canonical assets' from a master repo but this requires each repo (from what I can tell) to replicate the same IO_manager configs, correct? I suppose there is no current way to query the graphql interface to get IO_manager configs?  So I would have to create a synchronization mechanism to do this, right? or augment the graphql interface? Or maybe just mount the same io_manager config in all the repos.", "replies": ["that\u2019s right", "<@UULA0R2LV> thanks!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Yes, currently there is no way to synchronize io_managers between repos. You would need to replicate the same IO_manager configs in each repo or create a synchronization mechanism. There is also no way to query the graphql interface to get IO_manager configs.", "dagster_empl_first_target": "that\u2019s right", "dagster_empl_last_target": "that\u2019s right"}, {"question": "hi!\nI\u2019m using dagster with docker-compose and from yesterday I receive en error that I can\u2019t handle ((( Could you help, please?\nIt worked before, I don\u2019t know what really was changes so that the error happens\n\n```Traceback (most recent call last):\n\n  File \"/usr/local/bin/dagit\", line 8, in &lt;module&gt;\n\n    sys.exit(main())\n\n  File \"/usr/local/lib/python3.9/site-packages/dagit/cli.py\", line 225, in main\n\n    cli(auto_envvar_prefix=\"DAGIT\")  # pylint:disable=E1120\n\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1130, in __call__\n\n    return self.main(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1055, in main\n\n    rv = self.invoke(ctx)\n\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 1404, in invoke\n\n    return ctx.invoke(self.callback, **ctx.params)\n\n  File \"/usr/local/lib/python3.9/site-packages/click/core.py\", line 760, in invoke\n\n    return __callback(*args, **kwargs)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagit/cli.py\", line 160, in dagit\n\n    with get_instance_for_service(\n\n  File \"/usr/local/lib/python3.9/contextlib.py\", line 119, in __enter__\n\n    return next(self.gen)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_cli/utils.py\", line 20, in get_instance_for_service\n\n    with DagsterInstance.get() as instance:\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/instance/__init__.py\", line 463, in get\n\n    return DagsterInstance.from_config(dagster_home_path)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/instance/__init__.py\", line 479, in from_config\n\n    return DagsterInstance.from_ref(instance_ref)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/instance/__init__.py\", line 491, in from_ref\n\n    unified_storage = instance_ref.storage\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/instance/ref.py\", line 463, in storage\n\n    return self.storage_data.rehydrate() if self.storage_data else None\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_serdes/config_class.py\", line 101, in rehydrate\n\n    return klass.from_config_value(self, check.not_none(result.value))\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/storage/legacy_storage.py\", line 101, in from_config_value\n\n    event_log_storage = ConfigurableClassData(\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_serdes/config_class.py\", line 101, in rehydrate\n\n    return klass.from_config_value(self, check.not_none(result.value))\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster_postgres/event_log/event_log.py\", line 134, in from_config_value\n\n    return PostgresEventLogStorage(\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster_postgres/event_log/event_log.py\", line 91, in __init__\n\n    retry_pg_creation_fn(self._init_db)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster_postgres/utils.py\", line 66, in retry_pg_creation_fn\n\n    return fn()\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster_postgres/event_log/event_log.py\", line 101, in _init_db\n\n    stamp_alembic_rev(pg_alembic_config(__file__), conn)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/storage/sql.py\", line 50, in stamp_alembic_rev\n\n    stamp(alembic_config, rev)\n\n  File \"/usr/local/lib/python3.9/site-packages/alembic/command.py\", line 671, in stamp\n\n    script.run_env()\n\n  File \"/usr/local/lib/python3.9/site-packages/alembic/script/base.py\", line 569, in run_env\n\n    util.load_python_file(self.dir, \"env.py\")\n\n  File \"/usr/local/lib/python3.9/site-packages/alembic/util/pyfiles.py\", line 94, in load_python_file\n\n    module = load_module_py(module_id, path)\n\n  File \"/usr/local/lib/python3.9/site-packages/alembic/util/pyfiles.py\", line 110, in load_module_py\n\n    spec.loader.exec_module(module)  # type: ignore\n\n  File \"&lt;frozen importlib._bootstrap_external&gt;\", line 850, in exec_module\n\n  File \"&lt;frozen importlib._bootstrap&gt;\", line 228, in _call_with_frames_removed\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/storage/alembic/env.py\", line 17, in &lt;module&gt;\n\n    run_migrations_online(context, config, target_metadata)\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/storage/sql.py\", line 119, in run_migrations_online\n\n    with connectable.connect() as connection:\n\nAttributeError: 'Connection' object has no attribute 'connect'```", "replies": ["Might be related to SQLAlchemy 2.0 release. Pin SQLAlchemy in your dependencies to `SQLAlchemy~=1.4`  or something like that.\n\nSee the response here: <https://dagster.slack.com/archives/C01U954MEER/p1674792151613009>", "Thank you very much! It helped me !!!", "had the same issue, it helped, thank you !!!"], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "The error might be related to SQLAlchemy 2.0 release, so try pinning SQLAlchemy in your dependencies to `SQLAlchemy~=1.4`.", "dagster_empl_first_target": "Might be related to SQLAlchemy 2.0 release. Pin SQLAlchemy in your dependencies to `SQLAlchemy~=1.4`  or something like that.\n\nSee the response here: <https://dagster.slack.com/archives/C01U954MEER/p1674792151613009>", "dagster_empl_last_target": "had the same issue, it helped, thank you !!!"}, {"question": "Hello I'm trying to make a 2 asset graph work where given a dbt model in my redshift cluster that I materialize as an asset, I want to use a downstream asset to output this dbt model to google drive.\nI successfully created my asset in redshift and materialized it. When I run the downstream asset though, I'm getting the following error:\n`FileNotFoundError: [Errno 2] No such file or directory: '/opt/dagster/dagster_home/storage/gdrive_dw/gdrive'`  where `gdrive_dw` is my key_prefix and `gdrive` is the upstream asset. Should I define a specific IO manager to be able to pull the data from redshift and parse it as a pandas dataframe? I'm using the latest dagster version.", "replies": ["Managed to answer my own question. Yes I needed to create a custom IO Manager for extract from redshift and output as pandas dataframe"], "is_dagster_empl": [false], "gpt4_replies_target": "The user resolved their issue by creating a custom IO Manager to extract data from Redshift and output it as a pandas dataframe.", "dagster_empl_first_target": "Managed to answer my own question. Yes I needed to create a custom IO Manager for extract from redshift and output as pandas dataframe", "dagster_empl_last_target": "Managed to answer my own question. Yes I needed to create a custom IO Manager for extract from redshift and output as pandas dataframe"}, {"question": "Currently i only see the ability to pass it at runtime using run_config as shown here <https://docs.dagster.io/_apidocs/libraries/dagster-snowflake#resource>", "replies": ["`.configured` should work, like what you have with `s3_pickle_io_manager`", "Thanks ill give it a try", "That worked. Thank you"], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "The user can pass the run_config at runtime using `.configured`, similar to how it's done with `s3_pickle_io_manager`, and this solution has been confirmed to work.", "dagster_empl_first_target": "`.configured` should work, like what you have with `s3_pickle_io_manager`", "dagster_empl_last_target": "`.configured` should work, like what you have with `s3_pickle_io_manager`"}, {"question": "I'm following this <https://docs.dagster.io/tutorial/assets/testing-assets|testing assets> tutorial and I'm seeing `NameError: name 'nabisco_cereals' is not defined` when running `pytest complex_asset_graph.py`. Adding `from complex_asset_graph import nabisco_cereals` to my test script will avoid this error, but it feels like this is not in the spirit of testing with dagster and there is likely a better way. I reviewed the <https://docs.dagster.io/concepts/testing|testing docs> but was not able to find anything so far.  I am using dagster version 1.1.14 and python 3.7.9. Any insights are appreciated!", "replies": ["oh, I think this is assuming that the asset from here is in scope (either in the same file or imported): <https://docs.dagster.io/tutorial/assets/asset-graph#a-more-complex-asset-graph>", "This is helpful, thanks!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The error you're encountering is likely due to the 'nabisco_cereals' asset not being in scope, you can resolve this by ensuring it's either in the same file or properly imported as suggested in the tutorial.", "dagster_empl_first_target": "oh, I think this is assuming that the asset from here is in scope (either in the same file or imported): <https://docs.dagster.io/tutorial/assets/asset-graph#a-more-complex-asset-graph>", "dagster_empl_last_target": "oh, I think this is assuming that the asset from here is in scope (either in the same file or imported): <https://docs.dagster.io/tutorial/assets/asset-graph#a-more-complex-asset-graph>"}, {"question": "Hello , we are trying to deploy dagster in aws eks with the postgresql database in RDS. We would like to have the auth to the RDS using IAM ( <https://docs.aws.amazon.com/AmazonRDS/latest/UserGuide/UsingWithRDS.IAMDBAuth.html> ). Unfortunately i cannot make it to work. Does dagster support this ?", "replies": ["Hi Ovidiu, I can see you opened an issue:\n\n<https://github.com/dagster-io/dagster/issues/14177>\n\nI\u2019ve cc\u2019ed a more knowledgable team member about this issue there, so that\u2019s where we\u2019ll have the discussion.", "thank you very much"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Dagster does support deploying in AWS EKS with PostgreSQL database in RDS, but for issues with IAM authentication, it's recommended to follow the discussion in the opened issue on Dagster's GitHub page.", "dagster_empl_first_target": "Hi Ovidiu, I can see you opened an issue:\n\n<https://github.com/dagster-io/dagster/issues/14177>\n\nI\u2019ve cc\u2019ed a more knowledgable team member about this issue there, so that\u2019s where we\u2019ll have the discussion.", "dagster_empl_last_target": "Hi Ovidiu, I can see you opened an issue:\n\n<https://github.com/dagster-io/dagster/issues/14177>\n\nI\u2019ve cc\u2019ed a more knowledgable team member about this issue there, so that\u2019s where we\u2019ll have the discussion."}, {"question": "Hello everyone, I\u2019m a beginner in Dagster and I\u2019m having an issue where I can\u2019t see my jobs in the Dagit UI after defining them. I\u2019ve tried many methods but still can\u2019t figure out the reason. I\u2019m hoping to get some help from experts. Thank you.\nMy dagster version is : dagster, version 1.3.3", "replies": ["", "How are you launching dagit?\n\nHave you tried `dagster dev -f my_file.py` ?", "I started Dagster with \u201cdagster -dev\u201d, which loaded the \u201c*init*.py\u201d file in my uploaded image. The assets and DBT resources defined in this file are visible, but the jobs are not showing up.\nThanks.:heart:", "I\u2019m having trouble recreating the error - your code location is loading fine without any errors?", "also I see that you have `Defintions` imported, do you get the same issue when using the `Definitions` method of defining the code location?", "Hi <@U056CEL692P>,\nif I interpret this correctly, you are still using a `repository`, but you should rather switch to `Definitions`  if you are using a newer Dagster Version: <https://docs.dagster.io/concepts/code-locations>\n\nI see you import `Definitions`, but don't use it. It would be needed for Dagster to know about your jobs, assets etc.", "Repositories should still work but, like David says: `Definitions` are the preferred way to define code locations", "I\u2019m not replicating this issue (though I\u2019m not using exactly your code). I suggest two things:\n\n\u2022 `dagster dev -f my_file.py`, where `my_file.py` is the code you posted\n\u2022 As others are saying, please try with `Definitions`.\n", "Thank you everyone, I have resolved this issue using Definitions. Thank you."], "is_dagster_empl": [false, false, false, false, false, false, false, true, false], "gpt4_replies_target": "The issue of not seeing jobs in the Dagit UI could be due to using `repository` instead of `Definitions` to define code locations in Dagster. Switching to `Definitions` and launching Dagit with `dagster dev -f my_file.py` should resolve the issue.", "dagster_empl_first_target": "I\u2019m not replicating this issue (though I\u2019m not using exactly your code). I suggest two things:\n\n\u2022 `dagster dev -f my_file.py`, where `my_file.py` is the code you posted\n\u2022 As others are saying, please try with `Definitions`.\n", "dagster_empl_last_target": "I\u2019m not replicating this issue (though I\u2019m not using exactly your code). I suggest two things:\n\n\u2022 `dagster dev -f my_file.py`, where `my_file.py` is the code you posted\n\u2022 As others are saying, please try with `Definitions`.\n"}, {"question": "<https://github.com/dagster-io/dagster/discussions/14176>", "replies": ["I responded to your discussion."], "is_dagster_empl": [true], "gpt4_replies_target": "The expert has responded to your discussion on the provided GitHub link.", "dagster_empl_first_target": "I responded to your discussion.", "dagster_empl_last_target": "I responded to your discussion."}, {"question": "Hello everyone, I was wondering, is there a way to use sequence number as partition instead of dates or static elements?\n\nIf so is that done by using `DynamicPartitionsDefinition`?\n\nThis feature is experimental, so I\u2019m wondering if there is another way or if this is the only one.\n\nIf anyone have a draft of something related to that that would be helpfull too.\n\nThank you :slightly_smiling_face:", "replies": ["Hi Jacob, thanks for the question-- I\u2019m a little unclear what you mean here. If you want to use a fixed sequence of numbers, you can use `StaticPartitionsDefinition`. If you don\u2019t know the length of the sequence, then the partitioning scheme is necessarily dynamic, so you\u2019ll need `DynamicPartitionsDefinition`.", "It\u2019s a sequence of number but the end is not yet determined.\n\nI could use static and create a millions sequential number, but, I\u2019m concern that this may slow down the UI when trying to execute new task or play in the UI.\n\nA new partitions (Sequential number) would be created every 10 minutes.\n\nI\u2019m better off with DynamicPartitionsDefinition then right?", "&gt; I\u2019m better off with DynamicPartitionsDefinition then right?\nyes", "How about creating TimeWindowPartitionsDefinition by passing a cron schedule?", "You can use the partition as is or map the partition over 10 min to get your sequence "], "is_dagster_empl": [true, false, true, false, false], "gpt4_replies_target": "Yes, if the end of your sequence of numbers is not yet determined, you should use `DynamicPartitionsDefinition`. You can also consider creating `TimeWindowPartitionsDefinition` by passing a cron schedule.", "dagster_empl_first_target": "Hi Jacob, thanks for the question-- I\u2019m a little unclear what you mean here. If you want to use a fixed sequence of numbers, you can use `StaticPartitionsDefinition`. If you don\u2019t know the length of the sequence, then the partitioning scheme is necessarily dynamic, so you\u2019ll need `DynamicPartitionsDefinition`.", "dagster_empl_last_target": "&gt; I\u2019m better off with DynamicPartitionsDefinition then right?\nyes"}, {"question": "Is it possible to use a partition mapping between two assets which uses `non_argument_deps`? You'd usually indicate the mapping in the  `ins` argument which can take an `AssetIn` class, which can specify a partition mapping. However, `non_argument_deps`  seemingly only takes in `AssetKey` class, which cannot.", "replies": ["hi <@U04V1K70QJF>! good question -- you can do something like\n```@asset(ins={\"upstream\": AssetIn(dagster_type=Nothing, partition_mapping=...))\ndef my_asset():\n    ...```", "basically a `Nothing` type indicates a data-only dependency, and so dagster will not expect an argument (`non_argument_deps` is basically shorthand for this)", "Cool, thanks!"], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "Yes, it is possible to use a partition mapping between two assets which uses `non_argument_deps`. You can do this by using the `AssetIn` class with a `Nothing` type to indicate a data-only dependency.", "dagster_empl_first_target": "hi <@U04V1K70QJF>! good question -- you can do something like\n```@asset(ins={\"upstream\": AssetIn(dagster_type=Nothing, partition_mapping=...))\ndef my_asset():\n    ...```", "dagster_empl_last_target": "basically a `Nothing` type indicates a data-only dependency, and so dagster will not expect an argument (`non_argument_deps` is basically shorthand for this)"}, {"question": "<@U016C4E5CP8> hello! you once answered this about subprocesses limits (<https://dagster.slack.com/archives/C01U954MEER/p1679408182628029?thread_ts=1679284013.864289&amp;cid=C01U954MEER|here>):\n&gt; The default run launcher does each run in a subprocess on the gRPC server, so there's a risk of, when there are a high number of runs happening at once with no run queue in place, that the server gets overloaded.\n&gt; A couple of things I would suggest here to mitigate this:...\nwe are spinning k8s pods for each run but we don't create new pods for each ops\n\n*but,* we have a dynamic graph which can grow with ops pretty fast with the service usage growth\nit makes us to launch a lot of subprocesses inside of one pod and it seems to kill it with OOM\n\nis there any way to have some kind of queue which can help us to limit the number of subprocesses we can launch at a time?", "replies": ["oooh just found this, can be a solution?", "Yes `max_concurrent` will limit the number of concurrent ops within a run, but I don\u2019t think it will have any effect on the max number of processes arising from different runs", "no we have only one run per k8s pod so all good in that regard", "thanks for the answer, Sean!"], "is_dagster_empl": [false, true, false, false], "gpt4_replies_target": "Yes, you can use `max_concurrent` to limit the number of concurrent operations within a run, which can help manage the number of subprocesses launched at a time and prevent overloading.", "dagster_empl_first_target": "Yes `max_concurrent` will limit the number of concurrent ops within a run, but I don\u2019t think it will have any effect on the max number of processes arising from different runs", "dagster_empl_last_target": "Yes `max_concurrent` will limit the number of concurrent ops within a run, but I don\u2019t think it will have any effect on the max number of processes arising from different runs"}, {"question": "Is there a way to inspect the number of executors a job is running with (within the job itself)?\n\n`OpExecutionContext.run_config` allows us to inspect the run configurations, and I'm able to inspect `max_concurrent` in there, but _only_ if it's provided as a value when launching the job via Dagit.", "replies": ["based on <https://docs.dagster.io/_apidocs/execution#dagster.multi_or_in_process_executor|this>:\n```The max_concurrent arg is optional and tells the execution engine how many processes may run concurrently. By default, or if you set max_concurrent to be 0, this is the return value of python:multiprocessing.cpu_count().```\nso, i can inspect the `OpExecutionContext.run_config`, get the `execution.config.max_concurrent` key.\n\nif it's 0 or doesn't exist as a key in the `run_config`, then i should just use the `multiprocessing.cpu_count()` value?\n\nis there a built-in alternative?", "There is no built-in alternative, but this is something I think we should add to the `OpExecutionContext`. Would you mind opening a GH issue?", "will do, thanks!", "<https://github.com/dagster-io/dagster/issues/14197>"], "is_dagster_empl": [false, true, false, false], "gpt4_replies_target": "You can inspect the `OpExecutionContext.run_config` and get the `execution.config.max_concurrent` key. If it's 0 or doesn't exist as a key in the `run_config`, then you should use the `multiprocessing.cpu_count()` value. However, there is no built-in alternative in Dagster for this yet.", "dagster_empl_first_target": "There is no built-in alternative, but this is something I think we should add to the `OpExecutionContext`. Would you mind opening a GH issue?", "dagster_empl_last_target": "There is no built-in alternative, but this is something I think we should add to the `OpExecutionContext`. Would you mind opening a GH issue?"}, {"question": "how do I configure an asset definition for inclusion in a `Definitions` invocation? I remember with graphs, it was `to_job`, and I could pass run config.", "replies": ["You can put assets in `Definitions` and access them through an assets job. The default job created by `define_asset_job` will contain all assets in the `Definitions`:\n\n```defs = Definitions(\n    assets=[some_asset],\n    jobs=[define_asset_job(\"all_assets\")]\n)\n\ndefs.get_job_def(\"all_assets\")```", "sorry, I mean an asset definition that has a `config` param that is a subtype of `dagster.Config`. the example from <https://docs.dagster.io/concepts/assets/software-defined-assets#asset-configuration> is\n\n```from dagster import Config, asset\n\n\n@asset\ndef my_upstream_asset() -&gt; int:\n    return 5\n\n\nclass MyDownstreamAssetConfig(Config):\n    api_endpoint: str\n\n\n@asset\ndef my_downstream_asset(config: MyDownstreamAssetConfig, my_upstream_asset: int) -&gt; int:\n    data = requests.get(f\"{config.api_endpoint}/data\").json()\n    return data[\"value\"] + my_upstream_asset```\n", "```Definitions(\n    assets=[my_downstream_asset.configured({\"api_endpoint\": \"foo\"})],```\n?", "like I want a set of assets with varying configuration. sort of like it's a partitioned asset I guess, but the various config values wouldn't necessarily result in a set of mutually exclusive and collectively exhaustive assets (i.e. a true \"partitioning\")", "I\u2019m not sure I fully understand what you\u2019re looking for, but this works-- does it illustrate what you need?\n\n```class MyAssetConfig(Config):\n    api_endpoint: str\n\n\n@asset\ndef my_asset(config: MyAssetConfig) -&gt; str:\n    print(config.api_endpoint)\n    return config.api_endpoint\n\ndefs = Definitions(\n    assets=[my_asset],\n    jobs=[define_asset_job(\"all_assets\")],\n)\n\ndefs.get_job_def(\"all_assets\").execute_in_process(\n    run_config=RunConfig({\"my_asset\": MyAssetConfig(api_endpoint=\"<http://localhost:8080>\")})\n)```", "I don't want to `execute_in_process` (or `materialize` ) when loading the module. I want to be able to do this e.g. in dagit, but have pre-supplied config.", "as it is, I get `Missing required config entry \"ops\" at the root` in my Dagit launchpad, which makes sense, because required config is missing for an asset materialization.", "hmmm-- so you want an asset with a particular config schema, but the config params pre-set (but not to the defaults in the schema) when e.g. clicking Materialize in dagit?", "you could do this through setting the `config` param on `define_asset_job`, but I\u2019m not sure it\u2019s possible to do this for the hidden base asset job that is used by default when clicking Materialize in dagit", "```defs = Definitions(\n    assets=[my_asset],\n    jobs=[\n        define_asset_job(\"all_assets\", config=RunConfig({\"my_asset\": MyAssetConfig(api_endpoint=\"foo\")})\n    ],\n)```", "hmm, so it sounds like the way to go is to ensure default values for all of an asset's config class?", "well, let me revise my earlier statement-- if you materialize the assets while viewing the correct job in dagit, then the approach in the above code block should work:", "if you click \u201cMaterialize\u201d on `my_asset` when looking at the `all_assets` job (but not when looking at `default` asset group), it will use the specified config for `all_assets_job`", "okay, thanks. and that should still trigger all the asset-y logic hopefully, and `my_asset` materializations will appear in its history regardless of being initiated via a \"job\" hopefully. thanks.", "&gt; my_asset materializations will appear in its history regardless of being initiated via a \u201cjob\u201d\nmost definitely"], "is_dagster_empl": [true, false, false, false, true, false, false, true, true, true, false, true, true, false, true], "gpt4_replies_target": "To configure an asset definition for inclusion in a `Definitions` invocation, you can define your assets and jobs within the `Definitions`. If you want to set a particular config schema with pre-set config params, you can do this through setting the `config` param on `define_asset_job`. However, this might not work when clicking Materialize in dagit. Instead, if you materialize the assets while viewing the correct job in dagit, it will use the specified config for that job.", "dagster_empl_first_target": "You can put assets in `Definitions` and access them through an assets job. The default job created by `define_asset_job` will contain all assets in the `Definitions`:\n\n```defs = Definitions(\n    assets=[some_asset],\n    jobs=[define_asset_job(\"all_assets\")]\n)\n\ndefs.get_job_def(\"all_assets\")```", "dagster_empl_last_target": "&gt; my_asset materializations will appear in its history regardless of being initiated via a \u201cjob\u201d\nmost definitely"}, {"question": "Using a `QueuedRunCoordinator`, is there a way to clear the entire queue? There's enough runs in queue that manually paginating through and terminating runs is not feasible", "replies": ["Hi Tom, I wrote up an example script that you can use for this until bulk termination in the Dagster UI is ready (which is actively being worked on): <https://github.com/dagster-io/dagster/discussions/14203>"], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, there isn't a direct way to clear the entire queue in `QueuedRunCoordinator`, but you can use a script for this purpose until bulk termination is available in the Dagster UI. You can find the script in this discussion: <https://github.com/dagster-io/dagster/discussions/14203>.", "dagster_empl_first_target": "Hi Tom, I wrote up an example script that you can use for this until bulk termination in the Dagster UI is ready (which is actively being worked on): <https://github.com/dagster-io/dagster/discussions/14203>", "dagster_empl_last_target": "Hi Tom, I wrote up an example script that you can use for this until bulk termination in the Dagster UI is ready (which is actively being worked on): <https://github.com/dagster-io/dagster/discussions/14203>"}, {"question": "Hi all, is adding partitions support for observable source assets in the pipeline? And does anyone know of possible workarounds that can be used in the meantime?", "replies": ["Hi HP, Yes this is in the pipeline-- next week should see a release of at least some version of this.\n\nA workaround for the present is you could run a normal job/op and yield `AssetObservation` events from it for the target asset.", "Sounds good, thanks! Currently just setting up source assets as regular assets that are handled differently by my IO manager and are triggered by sensors that keep watch on S3."], "is_dagster_empl": [true, false], "gpt4_replies_target": "Yes, adding partitions support for observable source assets is in the pipeline and a workaround for now is to run a normal job/op and yield `AssetObservation` events from it for the target asset.", "dagster_empl_first_target": "Hi HP, Yes this is in the pipeline-- next week should see a release of at least some version of this.\n\nA workaround for the present is you could run a normal job/op and yield `AssetObservation` events from it for the target asset.", "dagster_empl_last_target": "Hi HP, Yes this is in the pipeline-- next week should see a release of at least some version of this.\n\nA workaround for the present is you could run a normal job/op and yield `AssetObservation` events from it for the target asset."}, {"question": "Hey! Ive run into a somewhat unique issue.  I have a `@graph_multi_asset` that accepts a set of Asset Keys as ins.  There are some cases where two of the ins are the same  Asset/AssetKey.  In this case, the <https://github.com/dagster-io/dagster/blob/1a853f994f681bad4ab974ed36aade7d30c7111c/python_modules/dagster/dagster/_core/definitions/decorators/asset_decorator.py#L594|build_asset_ins> method will eliminate one of the two keys in the mapping.  Thus causing the dagster invariant checker to not see one of the inputs.\n\nAny help or workaround is much appreciated!", "replies": ["Maybe I'm missing something, I thought asset keys are unique within the system. If you have multiple entries pointing to the same asset key wouldn't you just use the same reference?", "It is the same reference, same object", "but since it\u2019s a key in the dictionary the method I linked returns, only one reference goes through", "actually i think i can do a copy of the AssetKey to ensure its unique\n\nEdit: did not fix it", "after further looking into it, <https://github.com/dagster-io/dagster/blob/1a853f994f681bad4ab974ed36aade7d30c7111c/python_modules/dagster/dagster/_core/definitions/decorators/asset_decorator.py#L636-L649|its this loop> and <https://github.com/dagster-io/dagster/blob/1a853f994f681bad4ab974ed36aade7d30c7111c/python_modules/dagster/dagster/_core/definitions/decorators/asset_decorator.py#L653-L655|more specifically these lines assigning the dictionary key>. Which makes me believe a deepcopy would work, but it is a NamedTuple and python but be doing some magic there. Yeah even after a deep copy, they are equal", "Hi Jordan, could you elaborate on what you\u2019re aiming for with multiple asset copies of the same asset key in the inputs-- this is outside of Dagster\u2019s model, where an asset has a _set_ of dependencies on other assets (<@U053DKZ39MG> is correct), which is why the keys are de-duped.", "So I\u2019d like to keep a singular asset copy as the input for multiple keys. It is sometimes the case that two inputs to the graph asset will be the same file based on the functionality. So letting two of the inputs be the same graph asset is the main goal. It isn\u2019t always the case, but it sometimes will be. ", "Would you mind posting some simple example code so I can make sure we\u2019re on the same page?", "for sure! gimme one sec", "", "broadly where the issue comes up, however we are using a graph asset factory, so the asset inputs are variable, and sometimes the same.", "Hmmm-- I am pretty sure that the assumption of a one-to-one relationship between input names and asset keys is baked in at a fairly deep level.\n\nIt seems to me that if you are using a factory, you shouldn\u2019t need to define multiple inputs with the same key-- can you dedup at the top of the factory to determine the number of unique inputs and construct the graph asset accordingly?", "I can try that out!", "Thanks for all the help! "], "is_dagster_empl": [false, false, false, false, false, true, false, true, false, false, false, true, false, false], "gpt4_replies_target": "The issue arises from the assumption of a one-to-one relationship between input names and asset keys in Dagster. A potential workaround could be to deduplicate at the top of the factory to determine the number of unique inputs and construct the graph asset accordingly.", "dagster_empl_first_target": "Hi Jordan, could you elaborate on what you\u2019re aiming for with multiple asset copies of the same asset key in the inputs-- this is outside of Dagster\u2019s model, where an asset has a _set_ of dependencies on other assets (<@U053DKZ39MG> is correct), which is why the keys are de-duped.", "dagster_empl_last_target": "Hmmm-- I am pretty sure that the assumption of a one-to-one relationship between input names and asset keys is baked in at a fairly deep level.\n\nIt seems to me that if you are using a factory, you shouldn\u2019t need to define multiple inputs with the same key-- can you dedup at the top of the factory to determine the number of unique inputs and construct the graph asset accordingly?"}, {"question": "Is there a Dagster sensor for monitoring files added to Google GCS? I see there's one for S3. If not, happy to roll my own as the S3 one looks pretty straight forward, can use it as an example\n\n<https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors#s3-sensors>", "replies": ["we don\u2019t have a built-in gcs sensor. my recommendation would be to follow that example and write your own gcs sensor.", "Will do, thank you"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Dagster does not currently have a built-in sensor for monitoring files added to Google GCS, so you would need to write your own GCS sensor.", "dagster_empl_first_target": "we don\u2019t have a built-in gcs sensor. my recommendation would be to follow that example and write your own gcs sensor.", "dagster_empl_last_target": "we don\u2019t have a built-in gcs sensor. my recommendation would be to follow that example and write your own gcs sensor."}, {"question": "Is there a way to get the sensor\u2019s name from SensorEvaluationContext?", "replies": ["we haven\u2019t exposed it yet, but there\u2019s a private field: `context._sensor_name`"], "is_dagster_empl": [true], "gpt4_replies_target": "Yes, you can get the sensor's name from SensorEvaluationContext using a private field: `context._sensor_name`, although it's not officially exposed yet.", "dagster_empl_first_target": "we haven\u2019t exposed it yet, but there\u2019s a private field: `context._sensor_name`", "dagster_empl_last_target": "we haven\u2019t exposed it yet, but there\u2019s a private field: `context._sensor_name`"}, {"question": "hi dagster friends, i have a n00b question, i have a list of items that I want to process in parallel, how can i do that in dagster? Something like the following:\n```@asset\ndef list_of_files():\n    path = Path(\"/some/path\")\n    return list(path.iterdir())\n\n@asset\ndef process_files(list_of_files):\n    for file in list_of_files):\n        #how can i parellel process the files here?```", "replies": ["If you want to parallelize operations within an asset you'll need to refactor your asset into a <https://docs.dagster.io/concepts/assets/graph-backed-assets|graph-backed asset>, and use <https://docs.dagster.io/concepts/ops-jobs-graphs/dynamic-graphs#using-dynamic-outputs|DynamicOutputs> within the graph-backed asset to model the files you want to fan-out over", "got it, thanks <@U0300665S8G>!", "thanks <@U0300665S8G>!\nwe are also considering making dynamic mapping more native with assets. here\u2019s a tracking issue: <https://github.com/dagster-io/dagster/issues/9559>"], "is_dagster_empl": [false, false, true], "gpt4_replies_target": "To parallelize operations within an asset in Dagster, you need to refactor your asset into a graph-backed asset and use DynamicOutputs within the graph-backed asset to model the files you want to process in parallel.", "dagster_empl_first_target": "thanks <@U0300665S8G>!\nwe are also considering making dynamic mapping more native with assets. here\u2019s a tracking issue: <https://github.com/dagster-io/dagster/issues/9559>", "dagster_empl_last_target": "thanks <@U0300665S8G>!\nwe are also considering making dynamic mapping more native with assets. here\u2019s a tracking issue: <https://github.com/dagster-io/dagster/issues/9559>"}, {"question": "I have a question regarding assets, dependencies and jobs. I thought that if I have a DAG A-&gt;B-&gt;C and I schedule the materialization of C, then the dependency is inferred by Dagster, and A and B will be materialized as well as C depends on them. But the observation is that C fails due to the inavailability of B.\n\nI see that I can overcome this by saying `selection=\"*\"` in `define_asset_job` but wonder if I do this correctly, becasue by doing so, I would include assets as well that are not required, and by being more selective, I would resolve the DAG manually and therefore the job definition would be tightly coupled to the DAG, while by specifying a job I really only want to say \"I want C, and I don't care what C depends on, do the needful\".\n\nIs my conclusion correct or am I using Dagster incorrectly?", "replies": ["You can use the `AssetSelection` syntax with `define_asset_job` (<https://docs.dagster.io/_apidocs/assets#dagster.AssetSelection>). Something like the following should work:\n\n```define_asset_job(..., selection=AssetSelection.keys(\"C\").upstream())```", "Looks great!", "My mental model was somewhat different, I thought that in a graph, only the terminal nodes are of interest, but within a job, I can basically materialize each node, and therefore, I need to specify it."], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "In Dagster, you can use the `AssetSelection` syntax with `define_asset_job` to specify the assets you want to materialize. If you only want to materialize asset C and its dependencies, you can use `AssetSelection.keys(\"C\").upstream()`. However, keep in mind that in a graph, you can materialize each node, so you need to specify it.", "dagster_empl_first_target": "You can use the `AssetSelection` syntax with `define_asset_job` (<https://docs.dagster.io/_apidocs/assets#dagster.AssetSelection>). Something like the following should work:\n\n```define_asset_job(..., selection=AssetSelection.keys(\"C\").upstream())```", "dagster_empl_last_target": "My mental model was somewhat different, I thought that in a graph, only the terminal nodes are of interest, but within a job, I can basically materialize each node, and therefore, I need to specify it."}, {"question": "Hi, how does io managers work? Are the outputs deleted after the job finishes?", "replies": ["No, they are used for persistence? Or what do you mean exactly?", "Nevermind, I thought op outputs are just used during the pipeline run"], "is_dagster_empl": [false, false], "gpt4_replies_target": "IO Managers in Dagster are used for persistence and the outputs are not deleted after the job finishes.", "dagster_empl_first_target": "No, they are used for persistence? Or what do you mean exactly?", "dagster_empl_last_target": "Nevermind, I thought op outputs are just used during the pipeline run"}, {"question": "How do I get the partition key out of this? (Inside of a `handle_output` for a `MultiPartition`)\n`PartitionDimensionDefinition(name='dogs', partitions_def=DynamicPartitionsDefinition(partition_fn=None, name='doggo'))`", "replies": ["I ended up doing,\n```keys = [\ncontext.partition_key.keys_by_dimension[dim]\nfor dim in context.asset_partitions_def.partitions_defs\n]```"], "is_dagster_empl": [false], "gpt4_replies_target": "You can get the partition key out by using the following code: `keys = [context.partition_key.keys_by_dimension[dim] for dim in context.asset_partitions_def.partitions_defs]`.", "dagster_empl_first_target": "I ended up doing,\n```keys = [\ncontext.partition_key.keys_by_dimension[dim]\nfor dim in context.asset_partitions_def.partitions_defs\n]```", "dagster_empl_last_target": "I ended up doing,\n```keys = [\ncontext.partition_key.keys_by_dimension[dim]\nfor dim in context.asset_partitions_def.partitions_defs\n]```"}, {"question": "Hello, I'm trying to understand Resources conceptually. How are resources passed between dagster ops/assets? For example if i open a connection to an external service as part of the `__init__`  of a ConfigurableResource, will that connection be preserved for all nodes in the asset materialization? or will each asset that accesses the resource re-open that connection? What's the best way to only open a connection once and have each asset use the same connection?", "replies": ["the examples in this <https://dagster.io/blog/pythonic-config-and-resources|post> don't seem to use a persistent connection, but rather isolate the expensive logic into functions in the resource class...", "<@U0291UAGGCB> given your name is on the article, maybe you have some insight here?", "resources are not shared between processes, they are re-initialized for every node in the graph. a lot of this seems to comes down to the fact that it's really difficult to share data between processes in python, particularly non-serializable data.\n\nit's a pretty common ask here to have a database resource which preserves a connection across nodes in your execution graph but as far as I know it's not currently supported", "I think this is kind of possible using the in-memory executor, but then you lose parallelization", "how would you do this via the in-memory executor?", "I think it pretty much just works by default... if you make a database connection resource, and configure it on ops in a job which uses the in-memory executor then it'll only instantiate the resource once, because there's only one process being used. I don't think there's anything particularly special you have to do (aside from configuring the job to use the in-memory executor)", "cool, will give it a shot. Thanks!"], "is_dagster_empl": [false, false, false, false, false, false, false], "gpt4_replies_target": "In Dagster, resources are not shared between processes and are re-initialized for every node in the graph due to the difficulty of sharing data between processes in Python, especially non-serializable data. However, using the in-memory executor can allow a database connection resource to be instantiated only once, but this method sacrifices parallelization.", "dagster_empl_first_target": "the examples in this <https://dagster.io/blog/pythonic-config-and-resources|post> don't seem to use a persistent connection, but rather isolate the expensive logic into functions in the resource class...", "dagster_empl_last_target": "cool, will give it a shot. Thanks!"}, {"question": "*Legacy Resources*\n\nI notice that since dagster 1.3, use of `@resource` is now considered legacy. I have put considerable effort into creating a number of resources (which do great and amazing things) and I'm now fearful that I will need to migrate them over to the \"new order\". In general, I use jobs and ops to construct my workloads (because the software defined assets model doesn't map to my use case as well).\n\nHow long will legacy resources be available in dagster?\n\nSome references:\n\u2022 <https://docs.dagster.io/concepts/resources>\n\u2022 <https://docs.dagster.io/concepts/resources-legacy>\n\u2022 <https://docs.dagster.io/guides/dagster/migrating-to-pythonic-resources-and-config>", "replies": ["Existing resources and config APIs will continue to be supported for the foreseeable future."], "is_dagster_empl": [true], "gpt4_replies_target": "Existing resources and config APIs in dagster will continue to be supported for the foreseeable future.", "dagster_empl_first_target": "Existing resources and config APIs will continue to be supported for the foreseeable future.", "dagster_empl_last_target": "Existing resources and config APIs will continue to be supported for the foreseeable future."}, {"question": "Is there a way to set a tag on a job that can be set during execution of job or modified after job completion? e.g.\n```tag = ''\n\n@op\ndef my_op(context):\n  if a:\n    tag = 'a'\n  else:\n    tag = 'b'\n\n@job(\n  tags={\n    \"choice\": tag\n  }\n)\ndef my_job(context):\n  my_op()```\nI have a job with some branching behavior and want to more easily identify what path was chosen from the Run History UI where tags and metadata are exposed, is this possible via tags or is there some other approach to easily exposing this info in the UI?", "replies": ["Ah I see I can do this via custom config on the Schedule <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules#schedules-that-provide-custom-run-config-and-tags>"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, you can set or modify a tag during the execution of a job or after its completion by using custom config on the Schedule as per the Dagster documentation.", "dagster_empl_first_target": "Ah I see I can do this via custom config on the Schedule <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules#schedules-that-provide-custom-run-config-and-tags>", "dagster_empl_last_target": "Ah I see I can do this via custom config on the Schedule <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules#schedules-that-provide-custom-run-config-and-tags>"}, {"question": "Is there a way to add some metadata logic to all assets?\n\n I have a function `my_metadata(x)` that returns the metadata I want for each object type x:\n\nEg. if x is a table it gives me the column names\nEg. if x is an sklearn RandomForest it gives me the features hyper-parameters\nEg, if x is a LogisticRegression it gives me the features &amp; weights\n\nEtc...\n\nThe implementation is basically:\n\n```def my_metadata(asset, extra_metadata=dict()):\n    if isinstance(asset, pl.DataFrame):\n        return Output(asset, metadata=metadata_from_polars(asset)|extra_metadata)\n    elif isinstance(asset, list):\n        return  Output(asset, metadata=metadata_from_list(asset)|extra_metadata)\n    elif isinstance(asset, CatBoost):\n        return Output(asset, metadata=metadata_from_catboost(asset)|extra_metadata)\n    elif isinstance(asset, str):\n        return Output(asset, metadata=metadata_from_str(asset)|extra_metadata)\n    elif ...\n        ...\n    else:\n        print(f'No metadata will be written for asset of type {type(asset)}')\n        return asset```\nHowver, most of my software defined assets now have to call this function:\n\n```@asset(...)\ndef some_asset():\n   x_ans = ...\n   return my_metadata(x_ans)```\nIs there a way to make the `my_metadata` implicit? or to add it to the IO-manager or somewhere else?", "replies": ["Hi, yes you can use `IOManager` to catch automatically  `assets` metadata.\n\nHere an example of `IOmanager` class with `handle_output` method:\n\n```class MSSQLAlchemyIOManager(IOManager):\n  def _init_(self, connection_string):\n    self.engine = create_engine(connection_string, fast_executemany=True)\n  \n  def handle_output(self, context, obj):\n   \n    asset_key = context.asset_key\n    table_name = f\"{asset_key.path[-1]}\"\n    if isinstance(obj, pd.DataFrame):\n      context.add_output_metadata(\n        {\"num_rows\": len(obj), \n         \"table_name\": table_name,\n         \"preview\": MetadataValue.md(obj.head().to_markdown())})```", "Nice\nI really hope HTML is supported soon as a MetadataValue :')"], "is_dagster_empl": [false, false], "gpt4_replies_target": "Yes, you can use `IOManager` to automatically handle `assets` metadata. You can define a `handle_output` method in your `IOManager` class to add metadata to your assets based on their type.", "dagster_empl_first_target": "Hi, yes you can use `IOManager` to catch automatically  `assets` metadata.\n\nHere an example of `IOmanager` class with `handle_output` method:\n\n```class MSSQLAlchemyIOManager(IOManager):\n  def _init_(self, connection_string):\n    self.engine = create_engine(connection_string, fast_executemany=True)\n  \n  def handle_output(self, context, obj):\n   \n    asset_key = context.asset_key\n    table_name = f\"{asset_key.path[-1]}\"\n    if isinstance(obj, pd.DataFrame):\n      context.add_output_metadata(\n        {\"num_rows\": len(obj), \n         \"table_name\": table_name,\n         \"preview\": MetadataValue.md(obj.head().to_markdown())})```", "dagster_empl_last_target": "Nice\nI really hope HTML is supported soon as a MetadataValue :')"}, {"question": "Where do I need to set my environment variable so the docker run launcher picks it up?\n\nCaught an error for run 7a0a8223-1061-4974-9ab1-40de292e2b36 while removing it from the queue. Marking the run as failed and dropping it from the queue: Exception: Tried to load environment variable FIVETRAN_API_KEY, but it was not set\n\nI'm getting the following error in my docker setup:\n\n```Stack Trace:\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py\", line 283, in _dequeue_run_guarded\n    self._dequeue_run(workspace_process_context, run)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/run_coordinator/queued_run_coordinator_daemon.py\", line 323, in _dequeue_run\n    instance.launch_run(run.run_id, workspace_process_context.create_request_context())\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 1839, in launch_run\n    self.run_launcher.launch_run(LaunchRunContext(pipeline_run=run, workspace=workspace))\n  File \"/usr/local/lib/python3.7/site-packages/dagster_docker/docker_run_launcher.py\", line 153, in launch_run\n    self._launch_container_with_command(run, docker_image, command)\n  File \"/usr/local/lib/python3.7/site-packages/dagster_docker/docker_run_launcher.py\", line 95, in _launch_container_with_command\n    docker_env = dict([parse_env_var(env_var) for env_var in container_context.env_vars])\n  File \"/usr/local/lib/python3.7/site-packages/dagster_docker/docker_run_launcher.py\", line 95, in &lt;listcomp&gt;\n    docker_env = dict([parse_env_var(env_var) for env_var in container_context.env_vars])\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/utils.py\", line 107, in parse_env_var\n    raise Exception(f\"Tried to load environment variable {env_var_str}, but it was not set\")```\nI set the variable inside my dagster.yaml like so:\n\n```run_launcher:\n  module: dagster_docker\n  class: DockerRunLauncher\n  config:\n    env_vars:\n      - DAGSTER_POSTGRES_USER\n      - DAGSTER_POSTGRES_PASSWORD\n      - DAGSTER_POSTGRES_DB\n      - FIVETRAN_API_KEY\n      - FIVETRAN_API_SECRET```\nAlso set it docker-compose.yml for dagster_dagit and for dagster_user_code", "replies": ["Hi Benedikt - you'd want this environment value to be set as well in your daemon container - that's the container that pulls runs off of the queue and launches them", "<@U016C4E5CP8> thank you that worked."], "is_dagster_empl": [true, false], "gpt4_replies_target": "You need to set the environment variable in your daemon container, as it's the container that pulls runs off of the queue and launches them.", "dagster_empl_first_target": "Hi Benedikt - you'd want this environment value to be set as well in your daemon container - that's the container that pulls runs off of the queue and launches them", "dagster_empl_last_target": "Hi Benedikt - you'd want this environment value to be set as well in your daemon container - that's the container that pulls runs off of the queue and launches them"}, {"question": "Hello! I'm attempting to do something more complex than the initial tutorials with `branching graphs`\n\nI have two ops:\n```@op{out={'output_1' : Out(is_required=False)})\ndef get_parents_dataframe():\n    df = pd.read_sql('...')\n    data = df.groupby('parent')['id'].agg(list)\n    for item in data.items():\n        if item[0] == 'output_type_1':\n            yield Output(item, outname='output_1') ```\n```@op\ndef parse_file_output_1(context,_input) -&gt; pd.DataFrame:\n\n    return do_awesome_transforms()```\nand a graph\n\n```@graph\ndef my_first_branch():\n     output_1 = get_parents_dataframe() # as i understand this can have n outputs?\n     parse_file(output_1)```\nunfortunately this returns the following error after I use `AssetsDefinition.from_graph(my_first_graph)`  at my module level `__init___.py` (I'm using the project scaffold structure from the docs)\n`dagster._check.CheckError: Invariant failed. Description: All leaf nodes within graph 'my_first_branch' must generate outputs which are mapped to outputs of the graph, and produce assets. The following leaf node(s) are non-asset producing ops: {'parse_file_output_1'}. This behavior is not currently supported because these ops are not required for the creation of the associated asset(s).`", "replies": ["folder structure if that helps at all", "<@U04BB7HGNTB> I think including `return` at the beginning of the last line might fix this", "<@U011CET83FG> on the graph ?", "```@graph\ndef my_first_branch():\n     output_1 = get_parents_dataframe() # as i understand this can have n outputs?\n     return parse_file(output_1)```"], "is_dagster_empl": [false, true, false, true], "gpt4_replies_target": "The error you're encountering is because all leaf nodes within a graph must generate outputs which are mapped to outputs of the graph, and produce assets. To fix this, you need to include a `return` at the beginning of the last line in your graph definition. So your graph should look like this: \n```@graph\ndef my_first_branch():\n     output_1 = get_parents_dataframe() # as i understand this can have n outputs?\n     return parse_file(output_1)```", "dagster_empl_first_target": "<@U04BB7HGNTB> I think including `return` at the beginning of the last line might fix this", "dagster_empl_last_target": "```@graph\ndef my_first_branch():\n     output_1 = get_parents_dataframe() # as i understand this can have n outputs?\n     return parse_file(output_1)```"}, {"question": "question for anyone who's tried running dagit in a container:\nlet's say i do a brand new project (`dagit project scaffold --name xyz` ) usually if i `cd ./xyz` and run `dagit`, the server comes up.  but if i tried to run dagit from the original folder (without cding into the project file) i'm not able to do it with any of the flags (-w, -d, etc.)\n\ncan anyone clear up how i can do something like `dagit -w ./xyz`  and have the server run", "replies": ["```2023-01-11 19:59:46 +0000 - dagster - INFO - Started Dagster code server for module testing in process 230\n/usr/local/lib/python3.9/dist-packages/dagster/_core/workspace/context.py:548: UserWarning: Error loading repository location testing:dagster._core.errors.DagsterInvariantViolationError: No repositories, jobs, pipelines, graphs, asset groups, or asset definitions found in \"testing\".\n\nStack Trace:\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_grpc/server.py\", line 242, in __init__\n    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_grpc/server.py\", line 105, in __init__\n    loadable_targets = get_loadable_targets(\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_grpc/utils.py\", line 43, in get_loadable_targets\n    else loadable_targets_from_python_module(module_name, working_directory)\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_core/workspace/autodiscovery.py\", line 42, in loadable_targets_from_python_module\n    return loadable_targets_from_loaded_module(module)\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_core/workspace/autodiscovery.py\", line 134, in loadable_targets_from_loaded_module\n    raise DagsterInvariantViolationError(\n\n  warnings.warn(\n2023-01-11 19:59:46 +0000 - dagit - INFO - Serving dagit on <http://127.0.0.1:3000> in process 228```", "from the docs:\n  `1. dagit (works if .workspace.yaml exists)`\n\n  `2. dagit -w path/to/workspace.yaml`\nif i run dagit inside the project file it works -- but there is no `.workspace.yaml` file\nif i try to run `dagit -w ./projectfile` it doesn't work", "welp i'll just use `(cd ./testing ; dagit)`", "in case anyone needs:\n```podman rm dagit\npodman run \\\n--name dagit \\\n-p 3003:3000 \\\n-v ~/apps/dagster/:/app/ \\\n--entrypoint \"\" \\\ndag:bld /bin/sh -c \"(cd /app/$(cat ~/apps/dagster/PROJECT_NAME)/ ; dagit )\"\npodman ps```\n"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "To run dagit from the original folder without changing directories into the project file, you can use the command `(cd ./xyz ; dagit)`. If you want to run it in a container, you can use the command `podman run --name dagit -p 3003:3000 -v ~/apps/dagster/:/app/ --entrypoint \"\" dag:bld /bin/sh -c \"(cd /app/$(cat ~/apps/dagster/PROJECT_NAME)/ ; dagit )\"`", "dagster_empl_first_target": "```2023-01-11 19:59:46 +0000 - dagster - INFO - Started Dagster code server for module testing in process 230\n/usr/local/lib/python3.9/dist-packages/dagster/_core/workspace/context.py:548: UserWarning: Error loading repository location testing:dagster._core.errors.DagsterInvariantViolationError: No repositories, jobs, pipelines, graphs, asset groups, or asset definitions found in \"testing\".\n\nStack Trace:\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_grpc/server.py\", line 242, in __init__\n    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_grpc/server.py\", line 105, in __init__\n    loadable_targets = get_loadable_targets(\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_grpc/utils.py\", line 43, in get_loadable_targets\n    else loadable_targets_from_python_module(module_name, working_directory)\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_core/workspace/autodiscovery.py\", line 42, in loadable_targets_from_python_module\n    return loadable_targets_from_loaded_module(module)\n  File \"/usr/local/lib/python3.9/dist-packages/dagster/_core/workspace/autodiscovery.py\", line 134, in loadable_targets_from_loaded_module\n    raise DagsterInvariantViolationError(\n\n  warnings.warn(\n2023-01-11 19:59:46 +0000 - dagit - INFO - Serving dagit on <http://127.0.0.1:3000> in process 228```", "dagster_empl_last_target": "in case anyone needs:\n```podman rm dagit\npodman run \\\n--name dagit \\\n-p 3003:3000 \\\n-v ~/apps/dagster/:/app/ \\\n--entrypoint \"\" \\\ndag:bld /bin/sh -c \"(cd /app/$(cat ~/apps/dagster/PROJECT_NAME)/ ; dagit )\"\npodman ps```\n"}, {"question": "*How to manage +300 secrets?*\n\nI need to extract data from an API that have a separate token for each endpoint. :persevere:\nAnd I need to extract data from more that 300 endpoints.\n\nI\u2019m considering two options:\n1. create a key for environment variables for each token\n2. create one environment variable with all keys in json format\nI don\u2019t consider either option a good solution\n\nAny idea of how to handle this?", "replies": ["where are you hosting it? You could create a ressource that connects to some sort of secret manager. I.e an azure keyvault or similarly. Also makes it easier to control and see what access to what", "potentially even a bitwarden vault <https://pypi.org/project/bitwardentools/>", "or if it's not particularly sensitive just chuck the file with 300 api tokens on s3. then you're turning 300 secrets into one AWS key pair", "Hi <@U02DMJECQ68>, you have a couple of options, you can use an external secret manager like aws secret manager, azure, etc, as suggested by <@U04AELUGF6G>, or you can store all secrets in a encrypted file using a tool like ansible-vault, and pass the main password as an environment variable.", "Thanks <@U04AELUGF6G> and <@U04GCAR5WTH>\nWe are on AWS and it is unfortunately very sensitive data.\n\nIn development I will add all tokens to a json object and set that jsonobject as a environment variable.\n\nAnd then come production, we need to make a decision on what we will do. Most likely aws secret manager."], "is_dagster_empl": [false, false, false, false, false], "gpt4_replies_target": "You can manage +300 secrets by using an external secret manager like AWS Secret Manager or Azure, or store all secrets in an encrypted file using a tool like ansible-vault, and pass the main password as an environment variable.", "dagster_empl_first_target": "where are you hosting it? You could create a ressource that connects to some sort of secret manager. I.e an azure keyvault or similarly. Also makes it easier to control and see what access to what", "dagster_empl_last_target": "Thanks <@U04AELUGF6G> and <@U04GCAR5WTH>\nWe are on AWS and it is unfortunately very sensitive data.\n\nIn development I will add all tokens to a json object and set that jsonobject as a environment variable.\n\nAnd then come production, we need to make a decision on what we will do. Most likely aws secret manager."}, {"question": "Please help...\nHow do I get the branch deployment id (value between cloud/ and /runs) `<https://xxxxxxxx.dagster.cloud/57d2700a51ded0b52fa0a3475b78c31614bb8d54/runs>`\nUsing github actions, or the dagster-cloud CLI or  graphql\n\nThe idea is to run a job to create a preprod environment on a branch deployment. I have follow this guide <https://docs.dagster.io/guides/dagster/branch_deployments> and I have achieved run the job every time I create a pull request.\nThe only part missing is that the job is running in production, but instead I would like to run the job from the Branch deployment.\n\nThis is part of my github action\n```      - name: Build and deploy\n        id: deploy\n        if: env.ENABLE_FAST_DEPLOYS == 'true'\n        uses: dagster-io/dagster-cloud-action/actions/build_deploy_python_executable@pex-v0.1\n        with:\n          dagster_cloud_file: \"$GITHUB_WORKSPACE/project-repo/dagster_cloud.yaml\"\n          build_output_dir: \"$GITHUB_WORKSPACE/build\"\n          python_version: \"3.10\"\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n          \n\n      - name: Clone Snowflake\n        if: github.event.action != 'closed'\n        uses: dagster-io/dagster-cloud-action/actions/utils/run@v0.1\n        with:\n          location_name: data_analytics\n          deployment: ${{ steps.deploy.outputs.deployment }}\n          job_name: create_preprod\n        env:\n          DAGSTER_CLOUD_URL: \"<http://xxxxxx.dagster.cloud>\"\n          DAGSTER_CLOUD_API_TOKEN: ${{ secrets.DAGSTER_CLOUD_API_TOKEN }}```\nHow can I retrieve the branch deployment id from \"Build and deploy\" and pass it to the deployment variable in \"Clone Snowflake\" step. this value `${{ steps.deploy.outputs.deployment }}`  is coming empty\nthe graphql for the branch deployment is in the format <https://xxxxxxxx.dagster.cloud/9b864fc769056eb155f240a1f5fc77c2b6b1701b/graphql> so this is the id I am looking for 9b864fc769056eb155f240a1f5fc77c2b6b1701b\n\nI tried to use this action, but looks like it is not working\n`dagster-io/dagster-cloud-action/actions/utils/get_branch_deployment@v0.1`\nThank you!!", "replies": ["I see this conversation is continuing here: <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1686811684924829?thread_ts=1686807282.008969&amp;cid=C02LJ7G0LAZ> - will keep an eye on that thread"], "is_dagster_empl": [true], "gpt4_replies_target": "You can retrieve the branch deployment id from the \"Build and deploy\" step and pass it to the deployment variable in the \"Clone Snowflake\" step by using the output of the build_deploy_python_executable action, however, if it's coming out empty, it might be an issue with the action itself. You may want to check the ongoing conversation on this topic in the provided Slack thread for more information.", "dagster_empl_first_target": "I see this conversation is continuing here: <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1686811684924829?thread_ts=1686807282.008969&amp;cid=C02LJ7G0LAZ> - will keep an eye on that thread", "dagster_empl_last_target": "I see this conversation is continuing here: <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1686811684924829?thread_ts=1686807282.008969&amp;cid=C02LJ7G0LAZ> - will keep an eye on that thread"}, {"question": "What do I have to change to support the latest version of dagster. Now, I am getting:\n```IO manager on output relationships has returned value MetadataEntry(label='key', description=None, entry_data=IntMetadataValue(value=123)) of type MetadataEntry. The return type can only be one of AssetMaterialization, Dict[str, MetadataValue].```", "replies": ["<https://docs.dagster.io/concepts/assets/software-defined-assets#recording-materialization-metadata>", "My IO manager was `yield MetadataEntry(value=row_count, label=\"row_count\")` for the asset", "but this no longer seems to work.", "is the `context.add_output_metadata` defined in the IO manager?", "looks like it is <https://docs.dagster.io/_apidocs/io-managers>", "hi <@U01PP2ED4ES>, yes, try `context.add_output_metadata({\"row_count\": row_count})`. `MetadataEntry` can no longer be passed to any of our APIs.", "thx"], "is_dagster_empl": [false, false, false, false, false, true, false], "gpt4_replies_target": "To support the latest version of dagster, you need to replace `yield MetadataEntry(value=row_count, label=\"row_count\")` with `context.add_output_metadata({\"row_count\": row_count})` in your IO manager, as `MetadataEntry` can no longer be passed to any of the APIs.", "dagster_empl_first_target": "hi <@U01PP2ED4ES>, yes, try `context.add_output_metadata({\"row_count\": row_count})`. `MetadataEntry` can no longer be passed to any of our APIs.", "dagster_empl_last_target": "hi <@U01PP2ED4ES>, yes, try `context.add_output_metadata({\"row_count\": row_count})`. `MetadataEntry` can no longer be passed to any of our APIs."}, {"question": "*== self answered ==*\nwow lot of traffic here!\n\nI assume I miss just a little info, once I've started a run from Launchpad, I find it easy to click at the run-summary and re-start from any node from my graph\n\u2022 But how to do this for the first launch :thinking_face:\n\u2022 when I want to start for example from the 3rd node \"tr023d_ts_dev and all its decedents\" in my screenshot?\n\u2022 I assume maybe some configuration in Launchpad `/* Configure how steps are executed within a run. */`?\nappreciate a link to an example :slightly_smiling_face:", "replies": ["why is that so, that *after* asking I find it?\n\"Op selection syntax\" <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#op-selection-syntax|documentation link>\n\nin my case `tr023d_ts_dev*` is doing the job\n\u2022 to start with 23d and then all its descendants"], "is_dagster_empl": [false], "gpt4_replies_target": "You can use the \"Op selection syntax\" to start from a specific node in your graph. In your case, using `tr023d_ts_dev*` will start with the 23d node and then all its descendants. You can find more information in the [documentation](https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#op-selection-syntax).", "dagster_empl_first_target": "why is that so, that *after* asking I find it?\n\"Op selection syntax\" <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#op-selection-syntax|documentation link>\n\nin my case `tr023d_ts_dev*` is doing the job\n\u2022 to start with 23d and then all its descendants", "dagster_empl_last_target": "why is that so, that *after* asking I find it?\n\"Op selection syntax\" <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#op-selection-syntax|documentation link>\n\nin my case `tr023d_ts_dev*` is doing the job\n\u2022 to start with 23d and then all its descendants"}, {"question": "Hello everyone, could someone give me a pointer on how Dagster finds the content of a SourceAsset?  For example, if I have a pre-computed .csv file I want to make available as a SourceAsset, how do I tell Dagster where that file is located?", "replies": ["This would be based on the IO Manager attached to the SourceAsset:\n<https://docs.dagster.io/_apidocs/assets#dagster.SourceAsset>\n<https://docs.dagster.io/concepts/io-management/io-managers>", "Thanks <@U03RP112CAJ>!  This makes sense - is there an example that you know of anywhere?  The io managers that I\u2019ve been using for software-defined assets don\u2019t specify a file name or location, since that is determined by the asset, it\u2019s partitions, and DAGSTER_HOME.   So I am a little unclear on how to do this for SourceAssets.", "Warning: I\u2019m just spitballing since I never worked with this, but I\u2019m imagining it would look something like this:\n```class MySourceAssetIOManager(ConfigurableIOManager):\n    def load_input(self, context):\n        with open(\"my/asset/path\") as f:\n            return f.read()\n\nSourceAsset(\"my_source_asset\", io_manager_def=MySourceAssetIOManager)```\nBasically I assume you\u2019d have to find a way to tell your IO Manager where to find the source asset, this could be done with the asset keys or other information you can find within the `InputContext`(<https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/execution/context/input.py#L39>)", "ok, thanks!  I\u2019ll look into it and mess around"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "In Dagster, the content of a SourceAsset is found based on the IO Manager attached to the SourceAsset. You can specify the file location by creating a custom IO Manager that reads the file from the desired location.", "dagster_empl_first_target": "This would be based on the IO Manager attached to the SourceAsset:\n<https://docs.dagster.io/_apidocs/assets#dagster.SourceAsset>\n<https://docs.dagster.io/concepts/io-management/io-managers>", "dagster_empl_last_target": "ok, thanks!  I\u2019ll look into it and mess around"}, {"question": "Pythonic config - do I correctly understand that basically the pattern is for every different op to have a separate config class?", "replies": ["You might be able to reuse some, but I\u2019ve been creating new classes in 80-90% of my cases"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, you are correct. While some configurations can be reused, in most cases (80-90%), a new class is created for each different operation.", "dagster_empl_first_target": "You might be able to reuse some, but I\u2019ve been creating new classes in 80-90% of my cases", "dagster_empl_last_target": "You might be able to reuse some, but I\u2019ve been creating new classes in 80-90% of my cases"}, {"question": "I am trying to write an IO manager that \"wraps\" another IO manager (usecases I imagine could be things like asset \"dedup\" by changing keys and storing map in a database resources, anonymisation, change data representation, repartition of data, computation of analytics in way that is independent from underlying storage ). I have tried various mechanisms such as `required_resource_keys` - but always end up with problem having problem passing the correct context when calling `handle_output`  in the wrapped `io_manager` .  It seems context and configuration in the context needs to be duplicated / modified when you pass it to the sub resources which is a priori fine theoretically but complex as not much documented. I wonder if anyone has done something similar or  there are recommendation or any advice on the best way to achieve this.", "replies": ["Hi Bertrand, Interesting question-- could you provide more context on why you\u2019d like to use this \u201cwrapping\u201d pattern instead of just subclassing an IO manager?", "Hi Sean, thanks for the follow up. Actually I have managed to make the progress on this. The reason why I was considering this is the pattern of the data. I have a database that evolve through time that correspond to a list of financial assets. Most of the days it will be very similar so it seems not a good idea to save the entire asset over and over again in a file storage or in a database. At the same time, I want to be aware if that universe change due to change in our code or change providers, and I'd like to be able to go back to previous versions. Dagster has the right abstract concepts, code_version , data_version etc... However, if I am to store hundred of copies of the same data because I reprocess and I want to be able to go back through time that's not very clever. So we need the storage to undestand there is redundancy, and this problem is somehow indepdent of the abstract \"partiontion+asset-key-dataframe\" store that  I use to store my data. Currently, I am testing, I  not entirely sure that I want to store on s3 or in database, so it seem naturally to try to abstract the problem of deduplication from the underlying io_manager. I now have a very first version of hash-map that seems to seems to work in a dirty state, it requires an io_manager for storing blocks of data and a database connection for storing the hashmap.  Temporal diffs between consecutive partition look much harder. I have mentioned other examples of why I wanted to use this pattern, the main reason is that allow the problem solved by the wrapper io_manager is conceptually independent from what the underlying io_manager does. I was bit difficult to get started because, I had to get a good understanding how configuration was stored in this type of usecase. I have now accepted that I need a dict objects in my config/state to store the config of the wrapped io_manager. Naively at the beginning, I was hoping to be able reuse existing preconfigured io_manager and not to have to duplicate the config in an untyped dict.", "I see, thanks for the details. Yeah it sounds like there is a clean conceptual break between Dagster IO managers and this redundancy management you want to implement-- you can do anything you want inside `load_input` / `handle_output` of the IO manager, so that\u2019s where you\u2019ll need to implement or call out to some other resource that does the tricky redundancy stuff.\n\nSeems like you\u2019re in a good place though-- do you need any more help right now?", "Thanks, probably not just right now, but at some point, I am open to the idea to share the code to see if it can be improved / generalised."], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "The user is trying to create an IO manager in Dagster that wraps another IO manager for data redundancy management. They have made progress by implementing a hash-map that requires an IO manager for storing blocks of data and a database connection for storing the hashmap. They are open to sharing their code for improvements and generalization.", "dagster_empl_first_target": "Hi Bertrand, Interesting question-- could you provide more context on why you\u2019d like to use this \u201cwrapping\u201d pattern instead of just subclassing an IO manager?", "dagster_empl_last_target": "I see, thanks for the details. Yeah it sounds like there is a clean conceptual break between Dagster IO managers and this redundancy management you want to implement-- you can do anything you want inside `load_input` / `handle_output` of the IO manager, so that\u2019s where you\u2019ll need to implement or call out to some other resource that does the tricky redundancy stuff.\n\nSeems like you\u2019re in a good place though-- do you need any more help right now?"}, {"question": "I'm experiencing the below error when following <https://docs.dagster.io/tutorial/saving-your-data#choosing-an-io-manager-for-each-asset|this tutorial>. Are we expected to manually create the `topstories` table before materializing this asset? I'm using `dagster=1.3.2` and `dagster_duckdb_pandas=0.19.2`. Any input is appreciated!\n```dagster._core.errors.DagsterExecutionLoadInputError: Error occurred while loading input \"topstories\" of step \"topstories_word_cloud\":\n\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\execute_plan.py\", line 268, in dagster_event_sequence_for_step\n    for step_event in check.generator(step_events):\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\execute_step.py\", line 335, in core_dagster_event_sequence_for_step\n    for event_or_input_value in step_input.source.load_input_object(step_context, input_def):\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\inputs.py\", line 187, in load_input_object\n    yield from _load_input_with_input_manager(loader, load_input_context)\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\inputs.py\", line 801, in _load_input_with_input_manager\n    with op_execution_error_boundary(\n  File \"C:\\local\\Python\\lib\\contextlib.py\", line 153, in __exit__\n    self.gen.throw(typ, value, traceback)\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\utils.py\", line 84, in op_execution_error_boundary\n    raise error_cls(\n\nThe above exception was caused by the following exception:\nduckdb.CatalogException: Catalog Error: Table with name topstories does not exist!\nDid you mean \"information_schema.tables\"?\nLINE 1: SELECT * FROM public.topstories\n                      ^\n\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\utils.py\", line 54, in op_execution_error_boundary\n    yield\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\execution\\plan\\inputs.py\", line 808, in _load_input_with_input_manager\n    value = input_manager.load_input(context)\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster\\_core\\storage\\db_io_manager.py\", line 168, in load_input\n    return self._handlers_by_type[load_type].load_input(context, table_slice, conn)\n  File \"C:\\local\\Python\\lib\\site-packages\\dagster_duckdb_pandas\\duckdb_pandas_type_handler.py\", line 76, in load_input\n    return connection.execute(DuckDbClient.get_select_statement(table_slice)).fetchdf()```", "replies": ["i believe the `topstories` table will have been created in Step 4 here: <https://docs.dagster.io/tutorial/building-an-asset-graph>", "Thanks for the reply. I figured out where I went wrong. I had missed the step to drop the `kids` field from the DataFrame, which was causing problems when trying to write to DuckDB.  Things are working for me now after updating my `topstories` asset to include `df = pd.DataFrame(results).drop([\"kids\"], axis=1)` and manually deleting and recreating the `analytics.hackernews` DuckDB database."], "is_dagster_empl": [true, false], "gpt4_replies_target": "The `topstories` table should have been created in Step 4 of the tutorial and the error might be due to missing the step to drop the `kids` field from the DataFrame. Updating the `topstories` asset to include `df = pd.DataFrame(results).drop([\"kids\"], axis=1)` and manually deleting and recreating the `analytics.hackernews` DuckDB database should resolve the issue.", "dagster_empl_first_target": "i believe the `topstories` table will have been created in Step 4 here: <https://docs.dagster.io/tutorial/building-an-asset-graph>", "dagster_empl_last_target": "i believe the `topstories` table will have been created in Step 4 here: <https://docs.dagster.io/tutorial/building-an-asset-graph>"}, {"question": "Hi Team,\n\nJust trying to add a new code location from an existing repository to my primary deployment and getting an `unexpected error`. Working on the serverless cloud version. Error really isn't giving me any context as to what's going wrong.\n\nSeems to fail at the `Creating your repo` step which doesn't really make sense to me since I'm on the page to `Import an existing project`. If it helps I removed a previous code location for this deployment that was linked to the same repository which contained some sample code initially while I was getting started. Just trying to re-add the repository as a code location now that it's close to ready.", "replies": ["posting file in thread", "", "Note.. I'm not trying to deploy sample code, I'm trying to add an existing repository", "Hi Greg - I'll DM you about this, I think we could be showing a clearer error here"], "is_dagster_empl": [false, false, false, true], "gpt4_replies_target": "It seems like there might be an issue with the process of adding an existing repository as a new code location, possibly due to a previous code location linked to the same repository. A direct message will be sent to you for further assistance as the error message isn't clear.", "dagster_empl_first_target": "Hi Greg - I'll DM you about this, I think we could be showing a clearer error here", "dagster_empl_last_target": "Hi Greg - I'll DM you about this, I think we could be showing a clearer error here"}, {"question": "hey Dagster team, I am using Dagster Hybrid and Github Actions to deploy. the `DAGSTER_CLOUD_GIT_SHA` environment variable is perfect for my needs, except i need the temporary merge commit that is made against my base branch, rather than the commit i made and pushed. all my other deployment processes use `GITHUB_SHA` - <https://github.com/github/docs/issues/15302|this page> talks about how `GITHUB_SHA` is this merge commit.\n\ni have tried everything i can think of, including peeking inside the <https://github.com/dagster-io/dagster-cloud-action/blob/main/src/deploy_pex.py|deploy code called by the CLI>. but the `DAGSTER_CLOUD_GIT_SHA` is always the one I pushed - the one you can see on the PR page.\n\ni don\u2019t necessarily need to change `DAGSTER_CLOUD_GIT_SHA` - instead if you could help me pass `GITHUB_SHA` into my Dagster deployment, that\u2019d work too! But so far I have not had luck passing build arguments into my Docker image and setting them as env variables. (side question: should i expect that env variables set in my Docker image would be accessible in my `@repository` description?)\n\nfor reference, my deployment Github workflow looks very much like <https://github.com/dagster-io/hooli-data-eng-pipelines/blob/master/.github/workflows/deploy-dagster-cloud.yml|this one>. any ideas how i can access that sha and set it as an env variable? thanks!", "replies": ["Have you consider just not using the github action and just using the dagster cli to deploy your code? You'd have more control over the github sha", "I\u2019ve got a big enough team of data scientists who need to test their code on branch deployments that i need to automate these deployments. I personally wouldn\u2019t mind using the CLI, but that won\u2019t scale well :confused:", "I mean using the CLI from github actions", "just adding a step that does `dagster-cloud workspace update-location ...`", "oh gotcha. i have a command that says:\n```ci set-build-output --location-name=${{ matrix.location.name }} --image-tag=$IMAGE_TAG-${{ matrix.location.name }}```\ni don\u2019t know this command well - is there another argument i can pass that overrides the sha?", "that command ^ is passed to <https://github.com/dagster-io/dagster-cloud-action/blob/main/actions/utils/dagster-cloud-cli/action.yml|this action>", "re-read your question - it's your docker image that you're trying to insert a particular env var into?", "yeah, ultimately i want this env var accessible within my `@repository` and `@job` Python scripts. i\u2019m passing it into the Docker image and i think that should do the trick", "I think that command you pointing to is just telling the dagster cloud action what image to use and what location to update - you probably have a separate step where you're actually building your docker image right? Something like\n```      - name: Build and push fini Docker image\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          push: true\n          builder: ${{ steps.buildx.outputs.name }}\n          tags: \"${{ env.IMAGE_REGISTRY }}:${{ github.sha }}\"\n          labels: |\n            branch=${{ github.ref_name }}\n           ...```", "I don't think the Dagster action actually builds your image for you", "oh yeah, i\u2019ve got an image build step. i have been fighting the build arguments to try to get an env variable set in there\n```- name: Build and upload Docker image for ${{ matrix.location.name }}\n  if: steps.prerun.outputs.result != 'skip'\n  uses: docker/build-push-action@v4\n  with:\n    context: ./process\n    push: true\n    tags: ${{ env.IMAGE_REGISTRY }}:${{ env.IMAGE_TAG }}-${{ matrix.location.name }}\n    build-args: |\n      DYNAMO_TABLE_MAPPING={{ env.DYNAMO_TABLE_MAPPING }}```\n(Dynamo table mapping is an unrelated thing, but same concept - it requires passing an env var in)\n\nmy question on that note is, am i going down the right road? if i keep fighting the build arguments and get them to work, would my jobs be able to access it?", "I see but you're trying to rely on what's injected by Dagster as environment variables", "yeah I'd just do it that way, I do the same thing to pass in some environment variables via build-args", "your jobs should be able to access the environment variables if you assign the build-arg to an env var in the image", "perfect, thanks! i am just fighting some github workflow/Docker syntax cause it seems my argument is not being passed in correcctly. but once i get it working, i can use that as a template to also pass in the git sha. thanks <@U0300665S8G>!", "I suspect GITHUB_SHA is reserved by Github in some fashion which is why you're having trouble overriding it - setting some other env var on the Docker image as per Zach's suggestion seems reasonable to me", "I just got the env var working, so that's the way I'll go. Thanks guys!"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, true, false], "gpt4_replies_target": "The user can pass the `GITHUB_SHA` into their Dagster deployment by using the Dagster CLI from Github actions, adding a step that does `dagster-cloud workspace update-location ...`, and setting the `GITHUB_SHA` as an environment variable in their Docker image. This environment variable should then be accessible within their `@repository` and `@job` Python scripts.", "dagster_empl_first_target": "I suspect GITHUB_SHA is reserved by Github in some fashion which is why you're having trouble overriding it - setting some other env var on the Docker image as per Zach's suggestion seems reasonable to me", "dagster_empl_last_target": "I suspect GITHUB_SHA is reserved by Github in some fashion which is why you're having trouble overriding it - setting some other env var on the Docker image as per Zach's suggestion seems reasonable to me"}, {"question": "Hey yall, I have what I imagine is simple. I'm getting this error in dagster cloud after deploying via github actions, but everything runs fine locally. I'm pretty new to this and I assume I'm not pointing something to the right location.\n```dagster._core.errors.DagsterInvariantViolationError: No repositories, jobs, pipelines, graphs, or asset definitions found in bb-dev-tests```", "replies": ["I think I figured it out", "Nice, let us know if we can help with anything else!"], "is_dagster_empl": [false, true], "gpt4_replies_target": "It seems like you've resolved the issue with Dagster cloud deployment. If you encounter any more problems, feel free to ask for help.", "dagster_empl_first_target": "Nice, let us know if we can help with anything else!", "dagster_empl_last_target": "Nice, let us know if we can help with anything else!"}, {"question": "Hi, I have the following problem:\n\nI've created DBT project with Redshift, and I wanted to create downstream asset (plot) from the upstream DBT asset. When I execute my code, Dagster tries to read asset from the local folder, and not the Redshift table. I can't find references in docs on how to tackle this, since the examples use local DuckDB. Anyone know how to solve this? My error message is: FileNotFoundError: `[Errno 2] No such file or directory: '/repo/dagster_redshift_demo/tmp2sdfgg3/storage/dagster_demo_schema/customers'`\n\nI defined my asset as in tutorial <https://docs.dagster.io/integrations/dbt/using-dbt-with-dagster/part-four> :\n```@asset(\n    ins={\"customers\": AssetIn(key_prefix=[\"dagster_demo_schema\"])},\n    group_name=\"staging\",\n)```", "replies": ["Ok, I got it. I had to re-read the docs, and write custom IO manager.\n\n<https://docs.dagster.io/integrations/dbt/reference#defining-an-io-manager>"], "is_dagster_empl": [false], "gpt4_replies_target": "The user resolved their issue by re-reading the documentation and writing a custom IO manager as per the instructions in the Dagster documentation.", "dagster_empl_first_target": "Ok, I got it. I had to re-read the docs, and write custom IO manager.\n\n<https://docs.dagster.io/integrations/dbt/reference#defining-an-io-manager>", "dagster_empl_last_target": "Ok, I got it. I had to re-read the docs, and write custom IO manager.\n\n<https://docs.dagster.io/integrations/dbt/reference#defining-an-io-manager>"}, {"question": "Hi Dagster! Probably a simple one but what's the correct way to reference a dbt asset inside a `selection` clause for a job?\n\n```# dbt_project.yml\nmodels:\n  retail_pipeline_dbt:\n    stg_ulta:\n      +schema: stg_ulta```\n```# dagster_project/assets/dbt.py\n\ndbt_assets = load_assets_from_dbt_project(\n    project_dir=DBT_PROJECT_PATH,\n    profiles_dir=DBT_PROFILES,\n    key_prefix=[\"ulta_dbt\"],\n)```\n```# dagster_project/assets/__init__.py\n\nfrom dagster import define_asset_job\nfrom .dbt import dbt_assets \n\nasset_job = define_asset_job(name='dbt_job', selection=[?])```\n", "replies": ["looking inside my database when I materlize the job it has the following keys :", "I've tried various variations of `ulta_dbt/stg_ulta/ulta` with no luck scoured this resource as well : <https://docs.dagster.io/0.15.6/integrations/dbt> will happily edit the page with an example", "ah figured it out! looking through the db was useful `from dagster import AssetKey`\n\nthen\n```asset_job = define_asset_job(name='dbt_job', selection=[AssetKey(['ulta_dbt', 'stg_ulta', 'ulta']).to_user_string()] ```\nthe source code is bril, very self documenting! thanks", "glad that you were able to figure this out!", "Hi, is it also possible to include all dbt assets from `load_assets_from_dbt_project()` into `define_asset_job()` inside the `selection` clause?\nCouldn't figure it out yet. Thanks.", "This should work:\n```dbt_assets = load_assets_from_dbt_project(...)\nmy_job = define_asset_job(\"fdjskl\", selection=dbt_assets)```", "Definetely tried that before and it didn't worked. Anyway, works like expected now. Thanks sandy :slightly_smiling_face:"], "is_dagster_empl": [false, false, false, true, false, true, false], "gpt4_replies_target": "To reference a dbt asset inside a `selection` clause for a job, you can use the `AssetKey` from dagster. For example: `asset_job = define_asset_job(name='dbt_job', selection=[AssetKey(['ulta_dbt', 'stg_ulta', 'ulta']).to_user_string()])`. To include all dbt assets from `load_assets_from_dbt_project()` into `define_asset_job()` inside the `selection` clause, you can directly pass the `dbt_assets` to the `selection` parameter: `my_job = define_asset_job(\"fdjskl\", selection=dbt_assets)`.", "dagster_empl_first_target": "glad that you were able to figure this out!", "dagster_empl_last_target": "This should work:\n```dbt_assets = load_assets_from_dbt_project(...)\nmy_job = define_asset_job(\"fdjskl\", selection=dbt_assets)```"}, {"question": "How can I provide a description to an asset created by `AssetDefinition.from_graph` ? Given that the `@asset` decorator picks up docstring from the decorated function I expected the docstring of the graph to be \u201cpropagated\u201d as the description of the created asset, but it doesn\u2019t seem to be the case.", "replies": ["Hi Andras. Currently graph-backed assets use the description from the first op in the graph, but we do have an issue to fix this: <https://github.com/dagster-io/dagster/issues/9476>\n\nI think the ideal expected behavior would be to accept an argument in the `from_graph` method.", "Agree, a keyword argument is the cleanest. Thanks for pointing me to this."], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, graph-backed assets use the description from the first op in the graph, but there is an ongoing issue to fix this. The ideal solution would be to accept an argument in the `from_graph` method.", "dagster_empl_first_target": "Hi Andras. Currently graph-backed assets use the description from the first op in the graph, but we do have an issue to fix this: <https://github.com/dagster-io/dagster/issues/9476>\n\nI think the ideal expected behavior would be to accept an argument in the `from_graph` method.", "dagster_empl_last_target": "Hi Andras. Currently graph-backed assets use the description from the first op in the graph, but we do have an issue to fix this: <https://github.com/dagster-io/dagster/issues/9476>\n\nI think the ideal expected behavior would be to accept an argument in the `from_graph` method."}, {"question": "In `dagster-dbt`, can I instantiate assets from two different DBT projects? Problem seems to be that they both require a `dbt` resource, but different ones - I get an error if I try to supply two different resources with the same key in the same repository. Is the proper way to split it into two `repository`/`Definitions`?", "replies": ["Hi Rasmus. It is possible to instantiate assets from 2 dbt projects in the same `Definitions` / `repository` object.\n\nThe reason why you're seeing this error is because each resource key can only map to one unique resource object. You could change the resource key of one resource:\n<https://discuss.dagster.io/t/5160928/hi-all-we-want-to-load-the-assets-from-2-dbt-projects-vdbt-a>"], "is_dagster_empl": [true], "gpt4_replies_target": "Yes, you can instantiate assets from two different DBT projects in the same `Definitions` / `repository` object by changing the resource key of one resource.", "dagster_empl_first_target": "Hi Rasmus. It is possible to instantiate assets from 2 dbt projects in the same `Definitions` / `repository` object.\n\nThe reason why you're seeing this error is because each resource key can only map to one unique resource object. You could change the resource key of one resource:\n<https://discuss.dagster.io/t/5160928/hi-all-we-want-to-load-the-assets-from-2-dbt-projects-vdbt-a>", "dagster_empl_last_target": "Hi Rasmus. It is possible to instantiate assets from 2 dbt projects in the same `Definitions` / `repository` object.\n\nThe reason why you're seeing this error is because each resource key can only map to one unique resource object. You could change the resource key of one resource:\n<https://discuss.dagster.io/t/5160928/hi-all-we-want-to-load-the-assets-from-2-dbt-projects-vdbt-a>"}, {"question": "Hi all. Might be a bug on my end but usually when I click on an asset in the Global Asset Lineage Graph I get some asset metadata on the right (in the dagit ui). Now nothing is happening when I click on an asset. I'm on dagit 1.1.7\n\nAlso runIds are nowhere to be found", "replies": ["It was an issue with chrome; it works on Edge :shrug:", "Hi Mycchaka. Do the errors still persist after hard refreshing / clearing cache on Chrome?", "I've not tried - will check"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "The issue might be with Chrome, try using Edge or hard refreshing/clearing cache on Chrome to see if the problem persists.", "dagster_empl_first_target": "Hi Mycchaka. Do the errors still persist after hard refreshing / clearing cache on Chrome?", "dagster_empl_last_target": "Hi Mycchaka. Do the errors still persist after hard refreshing / clearing cache on Chrome?"}, {"question": "Hi all! I'm trying to create a sensor to terminate stuck jobs, and I get unexpected behavior. When I try to cancel a running job using context.instance:\n```\n            cancel_result = instance.run_coordinator.cancel_run(run_id)\n\n            if not cancel_result:\n                # just mark run as canceled\n                instance.report_run_canceled(run)```\ni get this messages in logs:\n```[DockerRunLauncher] Unable to get docker container to send termination request to.\nThis run has been marked as canceled from outside the execution context.```\nand run's execution continues (the first screenshot)\nbut when I terminate a run via dagit interface or via graphql everything works as expected (the second screenshot)", "replies": ["Hi Yevhen - sensors run on your grpc server container so this error makes me think that that container may not have access to the same Docker environment as dagit. Fixing that could involve adding the docker socket as a volume to that container like this: <https://github.com/dagster-io/dagster/blob/master/examples/deploy_docker/docker-compose.yml#L61>", "Yep, now it works! Thank you, <@U016C4E5CP8>!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The issue arises because the sensor runs on your grpc server container which may not have access to the same Docker environment as dagit. To fix this, you can add the docker socket as a volume to that container.", "dagster_empl_first_target": "Hi Yevhen - sensors run on your grpc server container so this error makes me think that that container may not have access to the same Docker environment as dagit. Fixing that could involve adding the docker socket as a volume to that container like this: <https://github.com/dagster-io/dagster/blob/master/examples/deploy_docker/docker-compose.yml#L61>", "dagster_empl_last_target": "Hi Yevhen - sensors run on your grpc server container so this error makes me think that that container may not have access to the same Docker environment as dagit. Fixing that could involve adding the docker socket as a volume to that container like this: <https://github.com/dagster-io/dagster/blob/master/examples/deploy_docker/docker-compose.yml#L61>"}, {"question": "Where can I find a list of supported `MetadataValue` subtypes?", "replies": ["The methods of MetadataValue seem to work", "These are listed on the api docs here: <https://docs.dagster.io/_apidocs/ops#metadata-types>"], "is_dagster_empl": [false, true], "gpt4_replies_target": "You can find a list of supported `MetadataValue` subtypes in the API documentation on this page: <https://docs.dagster.io/_apidocs/ops#metadata-types>.", "dagster_empl_first_target": "These are listed on the api docs here: <https://docs.dagster.io/_apidocs/ops#metadata-types>", "dagster_empl_last_target": "These are listed on the api docs here: <https://docs.dagster.io/_apidocs/ops#metadata-types>"}, {"question": "Is it possible (and a good idea) to create a new class, called DynamicPartitionsDefinition, where I implement `get_partitions()` to get the list of partitions at runtime? In my use case, all my tables have a \u201cdataset_id\u201d column, that identify the source dataset used to generate the row. I want to have a partition per dataset_id, but the list is not known beforehand, so that I cannot use `StaticPartitionsDefinition`.\nSome details:\n\u2022 there would be O(10,000) partitions (or more)\n\u2022 I would also implement a Sensor to run the adequate partition job when needed\n\u2022 the list of partitions (ie the list of datasets) would be obtained at runtime by an API call to an external service (<https://huggingface.co/api/datasets>)", "replies": ["Oh... it already exists :slightly_smiling_face:\n\n<https://github.com/dagster-io/dagster/blob/78df951f1f0d3934a5ca3ea34a84036f31d3525c/python_modules/dagster/dagster/_core/definitions/partition.py#L437|https://github.com/dagster-io/dagster/blob/78df951f1f0d3934a5ca3ea34a84036f31d3525[\u2026]c/python_modules/dagster/dagster/_core/definitions/partition.py>", "But it does not seem to be available for now :blob_sad:\n\nI get\n```dagster._core.errors.DagsterInvalidDefinitionError: Only static, time window, and multi-dimensional partitions are currently supported. ```\nwhen instancing a DynamicPartitionsDefinition object", "Hi Sylvain. Unfortunately Dagit doesn't support dynamic partitions at the moment, because calling out to user code to fetch the list of partition keys can be very slow and costly.\n\nWe are considering supporting a similar <https://github.com/dagster-io/dagster/issues/7943|runtime asset partitions> feature, which adds a partition to an asset from within job runs rather than at definition time. Let us know if you have thoughts on this feature compared to dynamic partitions definitions!\n\nNot a great workaround, but for now you could define a function that makes the API call to fetch the list of partitions, then define a static partitions definition with the partitions list. Every time you reload your workspace, your static partitions definition would be updated.", "Yes, changing the static partitions list could be an option... Beside it, I'm also wondering how I could manage the assets that disappear (in our context, when a dataset is deleted). I tried, and missing partitions that previously existed seem to just be ignored", "Yep, as long as any deleted partition key is no longer part of the partitions definition, Dagster will just ignore it and not display that partition's data in Dagit"], "is_dagster_empl": [false, false, true, false, true], "gpt4_replies_target": "While Dagster currently doesn't support dynamic partitions due to potential slowness and cost, a workaround could be to define a function that makes an API call to fetch the list of partitions, then define a static partitions definition with the partitions list. Every time you reload your workspace, your static partitions definition would be updated. As for managing disappearing assets, as long as any deleted partition key is no longer part of the partitions definition, Dagster will just ignore it and not display that partition's data in Dagit.", "dagster_empl_first_target": "Hi Sylvain. Unfortunately Dagit doesn't support dynamic partitions at the moment, because calling out to user code to fetch the list of partition keys can be very slow and costly.\n\nWe are considering supporting a similar <https://github.com/dagster-io/dagster/issues/7943|runtime asset partitions> feature, which adds a partition to an asset from within job runs rather than at definition time. Let us know if you have thoughts on this feature compared to dynamic partitions definitions!\n\nNot a great workaround, but for now you could define a function that makes the API call to fetch the list of partitions, then define a static partitions definition with the partitions list. Every time you reload your workspace, your static partitions definition would be updated.", "dagster_empl_last_target": "Yep, as long as any deleted partition key is no longer part of the partitions definition, Dagster will just ignore it and not display that partition's data in Dagit"}, {"question": "Hey all -\n\nAny pointers around database infrastructure requirements for hosting Dagster? I think I remember reading some posts about the storage filling up quickly, but wondering if anyone has any hard numbers here", "replies": ["alas we haven't compiled great guidelines on this\n\none datapoint is that we run our dogfood deployment (~500 runs a day) on a single `db.t3.medium` postgres instance"], "is_dagster_empl": [true], "gpt4_replies_target": "Dagster doesn't have specific guidelines for database infrastructure requirements, but as a reference, a single `db.t3.medium` postgres instance is used for their deployment which handles around 500 runs a day.", "dagster_empl_first_target": "alas we haven't compiled great guidelines on this\n\none datapoint is that we run our dogfood deployment (~500 runs a day) on a single `db.t3.medium` postgres instance", "dagster_empl_last_target": "alas we haven't compiled great guidelines on this\n\none datapoint is that we run our dogfood deployment (~500 runs a day) on a single `db.t3.medium` postgres instance"}, {"question": "if I have a bunch of assets tagged as 'upstream changed', is there a single-action option to rerun all of the outdated assets in one go? I seem to have to click each one and rerun it. I haven't used dagit for a few months and thought there was a one-click option, but I can't seem to find it. using 1.0.15.", "replies": ["in Dagster 1.1+, these are now called \"Stale\", and you can click the dropdown on the Materialize button to only materialize stale and missing assets", "Thanks! I'll try that out"], "is_dagster_empl": [true, false], "gpt4_replies_target": "In Dagster 1.1 and later versions, the 'upstream changed' assets are now referred to as 'Stale', and you can use the dropdown on the Materialize button to only materialize stale and missing assets.", "dagster_empl_first_target": "in Dagster 1.1+, these are now called \"Stale\", and you can click the dropdown on the Materialize button to only materialize stale and missing assets", "dagster_empl_last_target": "in Dagster 1.1+, these are now called \"Stale\", and you can click the dropdown on the Materialize button to only materialize stale and missing assets"}, {"question": "hoping someone can point me in the right direction of docs as well as just conceptually explain:\n\u2022 i have an op made with the purpose of reusing it (it takes a query string as the argument, then hits a redshift database for a result)\n\u2022 the op uses a resource\n\u2022 i want to create a job where the op above is used as the first step\nhow can i pass that query and make this work:\n```@op\ndef qry_redshift():\n   result = my_reusable_op(my_query_string)\n   get_dagster_logger().info(f\"*******{result}\")\n   return \"done\"\n\n\n@op\ndef qry_bigquery(qry_redshift):\n   result = op_bigquery_query()\n   get_dagster_logger().info(f\"*******{result}\")\n   return \"done\"\n\n\n@job(resource_defs={\"rsrc_bigquery\": rsrc_bigquery,\n                    \"rsrc_redshift\": rsrc_redshift})\ndef one_two_punch():\n   qry_bigquery(qry_redshift())```\nthanks!", "replies": ["these docs around <https://docs.dagster.io/concepts/configuration/config-schema#defining-and-accessing-configuration-for-an-op-asset-or-resource|op configuration> might help", "the <https://docs.dagster.io/concepts/configuration/config-schema#specifying-runtime-configuration|next section> below the one I linked has a good example", "you code would be something like\n\n```@op(config_schema={\"query_string\": str}\ndef qry_redshift(context):\n   result = my_reusable_op(context.op_config[\"query_string\")\n   get_dagster_logger().info(f\"*******{result}\")\n   return \"done\"\n\n\n@op\ndef qry_bigquery(qry_redshift):\n   result = op_bigquery_query()\n   get_dagster_logger().info(f\"*******{result}\")\n   return \"done\"\n\n\n@job(resource_defs={\"rsrc_bigquery\": rsrc_bigquery,\n                    \"rsrc_redshift\": rsrc_redshift})\ndef one_two_punch():\n   qry_bigquery(qry_redshift())\n\none_to_punch.execute_in_process(run_config={\"ops\": {\"config\": { \"qry_redshift\": \"SELECT * FROM blah\"}}})```", "in a unit testing context you can build the context object using `build_op_context`\n<https://docs.dagster.io/concepts/testing#testing-ops>", "thank you for taking the time!! my only issue is the resuable op takes a context argument because there is a `config_schema` there, and if i do that i get `UserWarning: Error loading repository location etl_dev:dagster._core.errors.DagsterInvalidInvocationError: Comput`\n`e function of op 'my_reusable_op' has context argument, but no context was provided when invoking.`\nso i tried to pass the context but it doesnt register...", "yeah you shouldn't need to pass the context argument explicitly. your code looks like the example I posted?", "yep this is the reusable op:\n```\n@op(required_resource_keys={\"rsrc_redshift\"},\n   config_schema={\"execute_many\": Field(bool, default_value=False),\n               \"qry_str\": str,\n               \"qry_vals\": Field(list, default_value=[]),\n               \"qry_type\": Field(str, default_value='DQL')\n               })\ndef my_reusable_op(context):\n   with context.resources.rsrc_redshift(placeholder=None).client as redshift_rsrc:```\nso maybe this one needs to have the context removed?", "ohh I missed the part where you're calling one op from inside another op. that'd kind of a no-no in dagster. you either need to use a vanilla python function for your reusable function, or define the reusable op as part of your graph, with it taking the output value of the `qry_redshift` function as an argument", "ahhhh got it", "i wonder if u could use a vanilla function that returns the reusable op", "if you try to call anything that's decorated with @op within another op you're going to get the missing context error, because Dagster implicitly provides the context for you when calling ops on the graph level and isn't able to do that from within the op", "ok ok i think im getting it, thank you!", "no problem!", "if you dont mind i have a more execution related question... is there a workaround to running dagit, while also having the custom run configuration set in the python script?  meaning, id like to keep `one_to_punch.execute_in_process(run_config={\"ops\": {\"config\": { \"qry_redshift\": \"SELECT * FROM blah\"}}})` in the python file, while also having the job be able to launch through dagit", "from the docs it seems mutually exclusive", "i end up having to scaffold the config in the ui and enter the query again", "you can pre-configure jobs / assets using the <https://docs.dagster.io/concepts/configuration/configured|configured api>. the `execute_in_process` method will actually execute the job when dagster loads your code (if it's defined at the top-level anyway), which I don't think is what you want", "just tested this and it seems like the configured api is actually a great use case for piggy backing on those shared ops", "thank you so much this was a major breakthrough", "Another design choice is you not use a reusable op but a resource that accepts a query string and hits the database for a result. You could let the query execute in  the `load_input` section of a custom io manager or you could let any op that uses the resource execute the query. If the query string is not going to change, you can use use the asset api instead of op and just load the value of the asset anytime you call it. This is where the the magic of the custom io manager comes in. Because every time the asset is loaded, the results are updated and not stale.", "<@U02P92K2FRQ> hey this is super interesting. it seems like im having a lot of issues with pickling when i try to ouput the result of an op to another.  for example:\n\ni have a resource that gives me a bigquery client, and use it in an op that i want to run queries, and return that \"cursor\" object\n```@op(required_resource_keys={\"rsrc_bigquery\"},\n   config_schema={\"qry_str\": str})\ndef op_bigquery_query(context):\n   with context.resources.rsrc_bigquery(placeholder=None).client as bigquery_rsrc:\n      qry_str = context.op_config[\"qry_str\"]\n      return bigquery_rsrc.query(qry_str)```\nwhen i try to use this in another op:\n```bq_test = configured(op_bigquery_query, name=\"bq_test\")(\n   {\"qry_str\": \"SELECT * FROM blah;}\n)\n@op\ndef data_to_dataframe(bq_test):\n\tbq_test.to_csv(\"bq_test.csv\", index=False)\n\treturn```\ni get told that i cant use `\"io_manager\": fs_io_manager`"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "To pass a query to an op in Dagster, you can use the op configuration. You can define the configuration schema for the op, which allows you to pass the query string as a configuration. Then, you can access this configuration in the op using the context argument. However, calling one op from inside another op is not recommended in Dagster. Instead, you can use a vanilla python function for your reusable function, or define the reusable op as part of your graph, with it taking the output value of the `qry_redshift` function as an argument.", "dagster_empl_first_target": "these docs around <https://docs.dagster.io/concepts/configuration/config-schema#defining-and-accessing-configuration-for-an-op-asset-or-resource|op configuration> might help", "dagster_empl_last_target": "<@U02P92K2FRQ> hey this is super interesting. it seems like im having a lot of issues with pickling when i try to ouput the result of an op to another.  for example:\n\ni have a resource that gives me a bigquery client, and use it in an op that i want to run queries, and return that \"cursor\" object\n```@op(required_resource_keys={\"rsrc_bigquery\"},\n   config_schema={\"qry_str\": str})\ndef op_bigquery_query(context):\n   with context.resources.rsrc_bigquery(placeholder=None).client as bigquery_rsrc:\n      qry_str = context.op_config[\"qry_str\"]\n      return bigquery_rsrc.query(qry_str)```\nwhen i try to use this in another op:\n```bq_test = configured(op_bigquery_query, name=\"bq_test\")(\n   {\"qry_str\": \"SELECT * FROM blah;}\n)\n@op\ndef data_to_dataframe(bq_test):\n\tbq_test.to_csv(\"bq_test.csv\", index=False)\n\treturn```\ni get told that i cant use `\"io_manager\": fs_io_manager`"}, {"question": "The problem above with input managers seems to be a problem with ops sharing resources. I define a iomanager and use it in an op with a `io_manager_key`. Then I use it in another `Out` in another op, and this is where things blow up. Here is my code:\n```target_extractor_op = define_dagstermill_op(\n    name=\"target_extractor_op\",\n    notebook_path=file_relative_path(__file__, \"../notebooks/target_extractor.ipynb\"),\n    output_notebook_name=\"output_target_extractor\",\n    outs={\"target\": Out(pd.DataFrame, io_manager_key=\"lake_io_manager\")},\n    ins={\"df_train\": In(pd.DataFrame)}\n)\n\n\nlocal_target_extractor_job = target_extractor_graph.to_job(\n    name=\"target_extractor_job\",\n    resource_defs={\n        \"output_notebook_io_manager\": local_output_notebook_io_manager,\n        \"training_data\": current_training_data,\n        \"lake_io_manager\": local_pandas_parquet_io_manager,\n    }\n)\n\ntransformer_op = define_dagstermill_op(\n    name=\"transformer_op\",\n    notebook_path=file_relative_path(__file__, \"../notebooks/transform.ipynb\"),\n    output_notebook_name=\"output_transform\",\n    outs={\"transformed_data\": Out(pd.DataFrame, io_manager_key=\"lake_io_manager\")},\n    ins={\"df\": In(pd.DataFrame), \"encoders\": In(dict), \"datatype\": In(str)}\n)\n\n\nlocal_train_transformer_job = transformer_graph.to_job(\n    name=\"train_transformer_job\",\n    resource_defs={\n        \"output_notebook_io_manager\": local_output_notebook_io_manager,\n        \"data_file\": current_training_data, \n        \"data_type\": train_type,\n        \"encoder_file\": encoder_file,\n        \"lake_io_manager\": local_pandas_parquet_io_manager,\n    }\n)```\n", "replies": ["this is pretty weird -- what does your graph code look like? is it just those single ops? and this shouldn't be necessary but what happens if you add \"lake_io_manager\" to the `reuired_resource_keys` argument of `define_dagstermill_op`?", "also it's not a big deal but do you mind keeping related messages together in a thread? makes it easier for us to track :pray:", "yes sorry for that i should have threaded. My bad!!!!", "note that i get no error for the target extractor (perhaps because its defined first...)", "IO manager:\n```class PandasParquetIOManager(UPathIOManager):\n    extension: str = \".parquet\"\n\n    def _get_path(self, context) -&gt; str:\n        <http://context.log.info|context.log.info>(context.resource_config)\n        <http://context.log.info|context.log.info>(type(context))\n        return UPath(f\"{context.resource_config['base_path']}/{context.name}{PandasParquetIOManager.extension}\")\n\n    def dump_to_path(self, context: OutputContext, obj: pd.DataFrame, path: UPath):\n        with path.open(\"wb\") as file:\n            obj.to_parquet(file)\n\n    def load_from_path(self, context: InputContext, path: UPath) -&gt; pd.DataFrame:\n        with path.open(\"rb\") as file:\n            return pd.read_parquet(file)\n\n    def load_input(self, context):\n        <http://context.log.info|context.log.info>(\"=============================in load input\")\n        if context.upstream_output is None: # input manager\n            path = self._get_path(context)\n        else:\n            <http://context.log.info|context.log.info>(\"===========================upstream path\")\n            path = self._get_path(context.upstream_output)\n        with path.open(\"rb\") as file:\n            return pd.read_parquet(file)\n\n@io_manager(config_schema={\"base_path\": Field(str, is_required=False)})\ndef local_pandas_parquet_io_manager(\n    init_context: InitResourceContext,\n) -&gt; PandasParquetIOManager:\n    assert init_context.instance is not None  # to please mypy\n    base_path = UPath(\n        init_context.resource_config.get(\n            \"base_path\", init_context.instance.storage_directory()\n        )\n    )\n    return PandasParquetIOManager(base_path=base_path)```\n", "and graph defs:\n```@graph(out = {'target': GraphOut()},\n)\ndef target_extractor_graph():\n    df_train = read_train_data()\n    target, _ = target_extractor_op(df_train)\n    return target\n\n\n@graph(out = {'transformed_data': GraphOut()},\n)\ndef transformer_graph():\n    df = read_data_file()\n    datatype = read_data_type()\n    edict = read_encoder_file()\n    transformed_data, _ = transformer_op(datatype = datatype, df = df, encoders = edict)\n    #transformed_data, _ = transformer_op(datatype = datatype, df = df)\n    return transformed_data```\n", "As you suspected, no change on adding `required_resource_keys={\"lake_io_manager\"},`as an argument to the ops...", "hey Rahul, what error are you getting? im trying to repro this on my end for troubleshooting.", "ah nvm - saw it in <https://dagster.slack.com/archives/C01U954MEER/p1676576119918849|the message below> :blob_smile_sweat:", "Yeah sorry for messing up the thread!!!", "you can get the code on github at", "univai-community/mlops2-with-dagster", "```@@ -373,6 +373,7 @@ local_test_transformer_job = transformer_graph.to_job(\n     name=\"test_transformer_job\",\n     resource_defs={\n         \"output_notebook_io_manager\": local_output_notebook_io_manager,\n+        \"lake_io_manager\": local_pandas_parquet_io_manager,\n         \"data_file\": current_testing_data,\n         \"data_type\": test_type,\n         \"encoder_file\": encoder_file\n@@ -383,6 +384,7 @@ local_dataset_transformer_job = transformer_graph.to_job(\n     name=\"dataset_transformer_job\",\n     resource_defs={\n         \"output_notebook_io_manager\": local_output_notebook_io_manager,\n+        \"lake_io_manager\": local_pandas_parquet_io_manager,\n         \"data_file\": current_dataset_data,\n         \"data_type\": dataset_type,\n         \"encoder_file\": encoder_file\n@@ -481,10 +483,11 @@ local_inference_from_data_job = inference_from_data_graph.to_job(\n     name=\"inference_from_data_job\",\n     resource_defs={\n         \"output_notebook_io_manager\": local_output_notebook_io_manager,\n         \"data_file\": current_dataset_data,\n+        \"lake_io_manager\": local_pandas_parquet_io_manager,       \n         \"data_type\": dataset_type,\n         \"encoder_file\": encoder_file,\n         \"model_file\": model_file,\n         \"infer_type\": dataset_type\n     }\n )\n@@ -507,10 +510,11 @@ local_inference_from_data_job_scratch = inference_from_data_graph_scratch.to_job\n     name=\"inference_from_data_job_scratch\",\n     resource_defs={\n         \"output_notebook_io_manager\": local_output_notebook_io_manager,\n         \"data_file\": current_dataset_data,\n+        \"lake_io_manager\": local_pandas_parquet_io_manager,\n         \"data_type\": dataset_type,\n         \"encoder_file\": encoder_file,\n         \"model_file\": model_file,\n         \"infer_type\": dataset_type\n     }\n )```", "i cloned your repo and did those changes^ and then the code location could be successfully loaded.", "so the problem is as you are reusing the notebook op in multiple jobs, you would need to specify the resource def, i.e. `\"lake_io_manager\": local_pandas_parquet_io_manager` in all the jobs.", "we are working on improving the error message as well as making the resource definition/specification more intuitive to use.\n\none improvement you could do is to move the common resource_defs to Definitions, i.e. specifying the resources at definition level across all jobs, so you won\u2019t forget to include any for new jobs.", "Wow! Thank you!! In retrospect this makes a lot of sense, since we describe resources at op level, those resources need to be on the jobs that use the ops\u2026", "yeah exactly.. for exactly this type of inconvenience, we introduced the definition-level resources: <https://github.com/dagster-io/dagster/discussions/11167>", "i do see you have some here: <https://github.com/univai-community/mlops2_with_dagster/blob/master/mlops2_with_dagster/__init__.py#L8> so moving the other common ones here should make it easier to maintain", "My strategy is to bind things as much to outer layers (jobs ) as possible. And so love the config format for inward communication.", "yea exactly Definition would be an even outer layer than jobs :blob_smiley:", "and if you ever need to bind things early, you can specify them on jobs or graphs", "Yes I tried the commented lines there but it somehow didn\u2019t work. But I shoukd try again..I was trying to many things to keep my head bolted on straight :rolling_on_the_floor_laughing: ", "so just to keep this straight in my head, i could do `lake_io_manager = participants.local_pandas_parquet_io_manager` in resource definitions and then not include it in any job, just using the key in ops?", "I think i am misunderstanding <@UULA0R2LV>. I tried moving all redource definitions out of the job and into dunder init.py but that gives me the same error, only now on target_op as thats the first one in the file.", "like so `lake_io_manager = participants.local_pandas_parquet_io_manager,` and so:\n```local_target_extractor_job = target_extractor_graph.to_job(\n    name=\"target_extractor_job\",\n    resource_defs={\n        \"output_notebook_io_manager\": local_output_notebook_io_manager,\n        \"training_data\": current_training_data,\n        #\"lake_io_manager\": local_pandas_parquet_io_manager,\n    }\n)```\nin the participants file. The op definition remains the same...", "oh sorry my bad - i got it wrong myself. jobs currently don\u2019t respect the `resources` arg to `Definitions` - only assets do.", "so in your case, because it seems like many jobs are sharing the same set of resources based on test/dataset. my suggestion would be to maintain a set of resources per dataset or environment, similar to this example here: <https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/resources/__init__.py#L55>", "Aha so it makes sense to double them into the resources definitions anyway, for asset based jobs. For example, metrics can be calculated as a downstream job after training is completed. The env based idea is super! I was thinking of that as a dev to production thing, but maintaining these dictionaries more as a \"config\" of sorts and passing them into jobs based on need makes sense!", "Thank you!", "anytime! speaking of dev to prod, here\u2019s a guide for that, in case you aren\u2019t aware : ) <https://docs.dagster.io/guides/dagster/transitioning-data-pipelines-from-development-to-production>"], "is_dagster_empl": [true, true, false, false, false, false, false, true, true, false, false, false, true, true, true, true, false, true, true, false, true, true, false, false, false, false, true, true, false, false, true], "gpt4_replies_target": "The issue arises from reusing the notebook op in multiple jobs, hence the resource def, i.e. `\"lake_io_manager\": local_pandas_parquet_io_manager` needs to be specified in all the jobs. A possible improvement is to move the common resource_defs to Definitions, specifying the resources at definition level across all jobs, so you won\u2019t forget to include any for new jobs.", "dagster_empl_first_target": "this is pretty weird -- what does your graph code look like? is it just those single ops? and this shouldn't be necessary but what happens if you add \"lake_io_manager\" to the `reuired_resource_keys` argument of `define_dagstermill_op`?", "dagster_empl_last_target": "anytime! speaking of dev to prod, here\u2019s a guide for that, in case you aren\u2019t aware : ) <https://docs.dagster.io/guides/dagster/transitioning-data-pipelines-from-development-to-production>"}, {"question": "Hello!\n\nWe just added a payment method to Dagster cloud, but our org is still in read-only mode. Can someone help with opening it up again?", "replies": ["<#C02LJ7G0LAZ|dagster-cloud>", "Hi <@U04TLNKNQ4F> this should be resolved for you.", "Thank you!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "The issue with your Dagster cloud being in read-only mode should now be resolved.", "dagster_empl_first_target": "Hi <@U04TLNKNQ4F> this should be resolved for you.", "dagster_empl_last_target": "Hi <@U04TLNKNQ4F> this should be resolved for you."}, {"question": "hi all, i've built a job status sensor and am seeing that occasionally, the value of `context.dagster_run.asset_selection` is `None` - under what circumstance would this happen?", "replies": ["The `asset_selection` field is only populated when a job is created as a subset-- it doesn\u2019t reflect the set of assets used to create the job with `define_asset_job`. So you will see this pretty frequently. (We realize this is confusing and are looking at improving this).", "hmm, i see - any fish-hooks to be aware of if I fall back to looking up the job by name and using `JobDefinition.asset_layer.asset_keys`?", "experiementally, this one _is_ populated on runs initiated by a schedule."], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "The `context.dagster_run.asset_selection` is `None` when a job is not created as a subset, it doesn't reflect the set of assets used to create the job with `define_asset_job`. However, it is populated on runs initiated by a schedule. If you look up the job by name and use `JobDefinition.asset_layer.asset_keys`, there shouldn't be any major issues.", "dagster_empl_first_target": "The `asset_selection` field is only populated when a job is created as a subset-- it doesn\u2019t reflect the set of assets used to create the job with `define_asset_job`. So you will see this pretty frequently. (We realize this is confusing and are looking at improving this).", "dagster_empl_last_target": "The `asset_selection` field is only populated when a job is created as a subset-- it doesn\u2019t reflect the set of assets used to create the job with `define_asset_job`. So you will see this pretty frequently. (We realize this is confusing and are looking at improving this)."}, {"question": "can someone help me troubleshoot why I'm getting an Internal Server Error when I try to execute this GraphQL mutation?\n```mutation(\n  $repositoryLocationName: String!\n  $parentRunId: String!\n  $rootRunId: String!\n) {\n  launchRunReexecution(\n    executionParams: {\n      selector: {\n        repositoryName: \"__repository__\"\n        repositoryLocationName: $repositoryLocationName\n      }\n      executionMetadata: {\n        rootRunId: $rootRunId\n        parentRunId: $parentRunId\n        tags: [{ key: \"dagster/is_resume_retry\", value: \"true\" }]\n      }\n    }\n    reexecutionParams: { parentRunId: $parentRunId, strategy: FROM_FAILURE }\n  ) {\n    __typename\n    ... on PythonError {\n      message\n      className\n      stack\n    }\n  }\n}```\n", "replies": ["the error doesn't give me any more information than this\n```{\n  \"error\": {\n    \"data\": {\n      \"launchRunReexecution\": {\n        \"__typename\": \"PythonError\",\n        \"message\": \"Internal Server Error (Trace ID: 5387314698807084639)\",\n        \"className\": \"Exception\",\n        \"stack\": []\n      }\n    }\n  }\n}```", "and the playground linter isn't flagging any issues", "I don't think you need executionParams set if reexecutionParams is set - try that?", "wow nice", "the docs made it look like it was required", "ah which docs? we can fix that up", "actually maybe I just misread them, the graphql docs", "I saw some `!` fields under that and figured they were required", "but they're only required if you include that param"], "is_dagster_empl": [false, false, true, false, false, true, false, false, false], "gpt4_replies_target": "The Internal Server Error you're experiencing might be due to having both executionParams and reexecutionParams set in your GraphQL mutation. Try removing the executionParams to see if that resolves the issue.", "dagster_empl_first_target": "I don't think you need executionParams set if reexecutionParams is set - try that?", "dagster_empl_last_target": "ah which docs? we can fix that up"}, {"question": "With regards to the k8s executor and user deployments, is there a way to change the run pod/job names from \"dagster-run-xxx\" to a name that would help a user determine which user deployment that pod is coming from?", "replies": ["There\u2019s not. Would a k8s label suffice? We could stick code location name in with job name here <https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/launcher.py#L228>", "That might be good to add, but it will still be hard to tell which pod is running what job just looking at pods", "What interface are you using for that? `kubectl get pods`?", "k9s, lense", "currently don't have lense running, not sure if there is a good way to use that to filter by tags", "Would you mind filing an issue for this?", "I don\u2019t think we\u2019ll be able to get to it immediately. We do take pr\u2019s though :slightly_smiling_face:"], "is_dagster_empl": [true, false, true, false, false, true, true], "gpt4_replies_target": "Currently, there is no way to change the run pod/job names from \"dagster-run-xxx\" to a name that would help a user determine which user deployment that pod is coming from. However, adding a k8s label might be a possible solution. This issue has been noted for future improvements.", "dagster_empl_first_target": "There\u2019s not. Would a k8s label suffice? We could stick code location name in with job name here <https://github.com/dagster-io/dagster/blob/master/python_modules/libraries/dagster-k8s/dagster_k8s/launcher.py#L228>", "dagster_empl_last_target": "I don\u2019t think we\u2019ll be able to get to it immediately. We do take pr\u2019s though :slightly_smiling_face:"}, {"question": "Running user pipeline with dagster v1.3.4 and when I click on a materialized asset, I get a graphql error:\n\n```Operation name: AssetEventsQuery\n\nMessage: Cannot return null for non-nullable field Run.mode.\n\nPath: [\"assetOrError\",\"assetMaterializations\",0,\"runOrError\",\"mode\"]\n\nLocations: [{\"line\":191,\"column\":7}]\n\nStack Trace:\n  File \"/usr/local/lib/python3.7/site-packages/graphql/execution/execute.py\", line 541, in execute_field\n    return_type, field_nodes, info, path, result\n  File \"/usr/local/lib/python3.7/site-packages/graphql/execution/execute.py\", line 621, in complete_value\n    \"Cannot return null for non-nullable field\"```", "replies": ["is the `dagit` server on the same version? I believe this was a bug fixed a few releases a go. You should be able to see the version of dagit by hovering on the dagster logo", "<@UH3RM70A2> Thanks! dagit is v1.2.2. Will upgrade!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The error might be due to a version mismatch between your dagster and dagit server. This issue was reportedly fixed in recent releases, so upgrading your dagit server to match the dagster version might resolve the problem.", "dagster_empl_first_target": "is the `dagit` server on the same version? I believe this was a bug fixed a few releases a go. You should be able to see the version of dagit by hovering on the dagster logo", "dagster_empl_last_target": "is the `dagit` server on the same version? I believe this was a bug fixed a few releases a go. You should be able to see the version of dagit by hovering on the dagster logo"}, {"question": "Hello! I am doing a POC of dagster open source on ECS using Fargate. When executing a job it take about a minute to start up the job instance on fargate. Is there a similar start up time for jobs in Dagster Cloud?", "replies": ["Hi Noam, Dagster Cloud Serverless uses Fargate and yes, the typical startup time is about a minute. This is just how long Fargate seems to take to provision and launch a container. We do provide an alternative <https://docs.dagster.io/dagster-cloud/deployment/serverless#non-isolated-runs|Non-isolated runs> option in Cloud that is faster, but not recommended if you have compute or memory intensive jobs."], "is_dagster_empl": [true], "gpt4_replies_target": "Yes, Dagster Cloud also uses Fargate and typically has a startup time of about a minute, but they offer a faster alternative with Non-isolated runs, although it's not recommended for compute or memory intensive jobs.", "dagster_empl_first_target": "Hi Noam, Dagster Cloud Serverless uses Fargate and yes, the typical startup time is about a minute. This is just how long Fargate seems to take to provision and launch a container. We do provide an alternative <https://docs.dagster.io/dagster-cloud/deployment/serverless#non-isolated-runs|Non-isolated runs> option in Cloud that is faster, but not recommended if you have compute or memory intensive jobs.", "dagster_empl_last_target": "Hi Noam, Dagster Cloud Serverless uses Fargate and yes, the typical startup time is about a minute. This is just how long Fargate seems to take to provision and launch a container. We do provide an alternative <https://docs.dagster.io/dagster-cloud/deployment/serverless#non-isolated-runs|Non-isolated runs> option in Cloud that is faster, but not recommended if you have compute or memory intensive jobs."}, {"question": "```\n@asset\ndef my_asset():\n    return \"my_asset\"\n\n@op\ndef write(csv_file):\n    print(csv_file)\n    return 'ok'\n\n\n@graph_asset\ndef my_graph_asset():\n    return write(my_asset.to_source_asset())\n\nmy_job = define_asset_job(name=\"my_job\", selection=\"my_graph_asset\")```\nI'm trying to define `my_job` but I'm getting following error in execution plan creation -\n\n```dagster._core.errors.DagsterInvariantViolationError: In job 'my_job' op 'write', input csv_file must get a value either (a) from a dependency or (b) from the inputs section of its configuration.```\nAny tips on how do I pass `my_asset` to `write` op?", "replies": ["Never mind. I figured that it's possible to do it in a job, not graph."], "is_dagster_empl": [false], "gpt4_replies_target": "The user realized that passing `my_asset` to `write` op is possible in a job, not in a graph, resolving the DagsterInvariantViolationError they were encountering.", "dagster_empl_first_target": "Never mind. I figured that it's possible to do it in a job, not graph.", "dagster_empl_last_target": "Never mind. I figured that it's possible to do it in a job, not graph."}, {"question": "Hello. We are using the ECS agent, each run creates an ECS task, each op launches a subprocess to execute. Is it normal for each subprocess to take ~20 seconds to start up? This is quickly multiplying by the large number of ops we have.", "replies": ["Never mind, increasing the ECS resources reduced this greatly :slightly_smiling_face:"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, it is normal for each subprocess to take some time to start up, but you can greatly reduce this time by increasing the ECS resources.", "dagster_empl_first_target": "Never mind, increasing the ECS resources reduced this greatly :slightly_smiling_face:", "dagster_empl_last_target": "Never mind, increasing the ECS resources reduced this greatly :slightly_smiling_face:"}, {"question": "I have an asset that is partitioned. I would like to create another asset that aggregates data from all of the partitions. Is there a way of defining the asset so that Dagster would pass to it something like a list of all the assets?\n\nI tried defining a second asset that is without partitions and having the first asset as an argument. I get the following error:\n\ndagster._check.CheckError: Failure condition: Tried to access partition key for asset \u2019AssetKey([\u2018asset__number_one\u2019])\u2019, but the number of input partitions =! 1", "replies": ["I think you should use `AllPartitionMapping` in the second asset's `AssetIn`\n\nIf your IOManager supports loading multiple partitions (the default one and any that's based on `UPathIOManager` does) you would be able to get a dictionary with mapping between partition keys and your asset's python objects", "Wonderful, thank you for the suggestion!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "To aggregate data from all partitions of an asset, you should use `AllPartitionMapping` in the second asset's `AssetIn`. Ensure your IOManager supports loading multiple partitions.", "dagster_empl_first_target": "I think you should use `AllPartitionMapping` in the second asset's `AssetIn`\n\nIf your IOManager supports loading multiple partitions (the default one and any that's based on `UPathIOManager` does) you would be able to get a dictionary with mapping between partition keys and your asset's python objects", "dagster_empl_last_target": "Wonderful, thank you for the suggestion!"}, {"question": "I took the PartitionedParquetIOManager and was testing it out with an HourlyPartitioned definition and when I run a backfill as a single job, I just get spammed with `ASSET_MATERIALIZATION - Materialized value component myasset.` I only see it calling `handle_output` on the IO manager once with a range, IO manager has saved it as a single xxxxxxx_xxxxxx.pq with all assets inside of it. But the asset job keeps repeating the debug log (possibly for each of the thousands of partitions). Any thoughts?", "replies": ["Where did you get `PartitionedParquetIOManager` from? Is it from the github example? Just interested\n\nAlso seems like you're materializing a range of partitions in the same run, which is not the same as backfilling with multiple runs (one per partition).", "Yes, it's from the project fully featured example.", "Could you send me the link please?", "<https://github.com/dagster-io/dagster/blob/master/examples/project_fully_featured/project_fully_featured/resources/parquet_io_manager.py>", "ok so this example is a bit outdated", "And, yes, when I go to the UI to materialize, I select to backfill in a single run.", "just curious, why do you need to do that?", "As opposed to multiple runs?", "Yeah", "I have no good reason. It seemed like the right thing to do. I set up my asset to handle a range and then load the data to materialize from the start time (for an hourly partition).", "I mean it's not really outdated, just now Dagster has `UPathIOManager` that supports loading multiple partitions with partition mapping out of the box. I'm not sure if it supports partitions range runs tho. maybe <@U011CET83FG> can clarify this.\n\nI think the best practice right now would be to write such IOManagers using the `UPathIOManager`. (<@U011CET83FG> correct me if I'm wrong)", "I am just playing with things to see how it works. :smile:", "```@asset(\n    io_manager_key=\"parquet_io_manager\",\n    key_prefix=[\"component\"],\n    metadata={\n        \"partition_expr\": \"time\",\n    },\n    partitions_def=hourly_partitions_def,\n    required_resource_keys={\"ods_client\"},\n)\ndef ods_components(context) -&gt; pd.DataFrame:\n    \"\"\"Components from the ODS.\"\"\"\n    start, end = context.asset_partitions_time_window_for_output()\n    # partition_date_str = context.asset_partition_key_for_output()\n    get_dagster_logger().info(f\"Materializing ODS assets between: {start} {end}\")\n    return context.resources.ods_client.extract_between(start, end)```", "I see. It doesn't seem like your case actually needs the range so I would suggest just using normal runs.\nYou could use the range if your code could handle multiple partitions efficiently (like it's some SQL that doesn't care how many partitions does it work with). Or perhaps if you are using PySpark.\nProbably with DataFrames you would be safer by launching multiple runs. Less risk to OOM too in case of a lot of data.", "Yeah, that's a good consideration.", "here is my IOManager for parquet (using Polars). It's a bit fancy because it's returning either `pl.DataFrame` or `pl.LazyFrame` based on how did you type the input. It will also work with `gcs`, `s3`, etc.\n\nPerhaps you want to write something like this.\n\n```\nclass PolarsParquetIOManager(UPathIOManager):\n    \"\"\"\n    Will return polars.DataFrame or polars.LazyFrame based on the type hint or dagster_type.typing_type\n    \"\"\"\n\n    extension: str = \".parquet\"\n\n    def dump_to_path(self, context: OutputContext, obj: pl.DataFrame, path: UPath):\n        with path.open(\"wb\") as file:\n            obj.write_parquet(file)\n\n    def load_from_path(self, path: UPath, context: InputContext) -&gt; pl.DataFrame | pl.LazyFrame:\n        fs: fsspec.AbstractFileSystem | None = None\n\n        try:\n            fs = path._accessor._fs\n        except AttributeError:\n            pass\n\n        ldf = pl.scan_ds(ds.dataset(str(path), filesystem=fs))\n\n        columns = context.metadata.get(\"columns\")\n        if columns is not None:\n            context.log.debug(f\"Loading {columns=}\")\n            ldf = ldf.select(columns)\n\n        if context.dagster_type.typing_type in (\n            pl.DataFrame,\n            Dict[str, pl.DataFrame],\n            Mapping[str, pl.DataFrame],\n            None,\n        ):\n            return ldf.collect(streaming=True)\n        elif context.dagster_type.typing_type in (\n            pl.LazyFrame,\n            Dict[str, pl.LazyFrame],\n            Mapping[str, pl.LazyFrame],\n        ):\n            return ldf\n        else:\n            raise NotImplementedError(f\"Can't load object for type annotation {context.dagster_type.typing_type}\")```\n", "Cool, let me have a look and see what I get rewriting this to handle the multi-run.", "It would be great if you could contribute the partitions range run logic to the `UPathIOManager`", "Hi Daniel, you have used `ds.dataset` in your code, what is the `ds` alias to ?", "`import pyarrow.dataset as ds`", "ah .. okay  Thanks", "this allows reading not only single parquet files but also for example partitioned datasets produced by spark", "just out of curiosity, so this is compatible with partitions on parquet file", "if yes then just wondering why you said \u201cIt would be great if you could contribute the partitions range run logic to the `UPathIOManager` \u201d", "So the partitions range run is a new thing in Dagster. I don't use it myself. This is when you combine all backfill runs into a single run. There is a checkbox for it in Dagit.\nBy supporting partitions I mean supporting loading multiple partitions with partitions mapping. This is for situations when for example a daily asset depends on 24 partitions of an upstream hourly asset.\nSo just a single run for this asset would require loading multiple partitions. `UPathIOManager` supports exactly this. The partitions would be then returned in a dictionary with partition_key-&gt;obj mapping.", "Oh okay, got it", "And it obviously supports normal partitioned assets without mappings", "When a daily asset depends on another daily asset", "Just yesterday I was wondering how can I re-use my kuberntes pod to do more work instead of having more pods generated.. since I already have spare CPU.. this answers my question, do you by any chance recall  what that feature is called..", "I think it's called partitions range run. Here is what comes up in Dagit. I don't think there are a lot of docs about this available right now.\n\nI also don't see how this will help you in kubernetes, you will still use the same compute.\n\nAlso, at best you would be able to run the same thing in parallel (by writing some parallel code and using more CPUs for your single pod), otherwise you will run it sequentially and it will take more time than launching multiple pods at the same time.\n\nPartitions range run can help when the compute happens outside, can be stretched - like spark, big query, etc, and when running all the partitions at the same time can provide some sort of optimization over running them individually in parallel jobs (I can't provide any examples right now).\n\nLike I said, I'm not using it personally.", "Ooh.. Interesting.. Thank you Daniel", "I am playing with it now and a multi-run over 10k extract load operations has a lot of overhead in the spin-up of a run, where if I can get a range run to process, it'll be to load the entire range of datasets in a single run, with controlled parallelism or even just a looped iteration and not add the extra latency of the spin-up for each run to the time to process. Though I imagine for a use case like this, I could have a job for an entire one time load, and then partitioned job take over after that time period but I'm just exploring right now.", "One last thing <@U03JEQ2PRRV> would you mind sharing your `@resource` definition for the above Polars IO Manager ?", "```from upath import UPath\n\n@io_manager(config_schema={\"base_path\": Field(str, is_required=False)})\ndef polars_parquet_io_manager(\n    init_context: InitResourceContext,\n) -&gt; PolarsParquetIOManager:\n    assert init_context.instance is not None\n    base_path = UPath(init_context.resource_config.get(\"base_path\", init_context.instance.storage_directory()))\n    return PolarsParquetIOManager(base_path=base_path)```\n`base_path` can be a remote storage uri like `<s3://bucket/directory>`", "wonderful so universal-pathlib takes in default aws creds to connect", "yes"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "The `PartitionedParquetIOManager` you're using is a bit outdated, and it's recommended to use `UPathIOManager` that supports loading multiple partitions with partition mapping out of the box. The repeated debug log might be due to materializing a range of partitions in the same run, which is not the same as backfilling with multiple runs. It's suggested to use normal runs unless your code can handle multiple partitions efficiently.", "dagster_empl_first_target": "Where did you get `PartitionedParquetIOManager` from? Is it from the github example? Just interested\n\nAlso seems like you're materializing a range of partitions in the same run, which is not the same as backfilling with multiple runs (one per partition).", "dagster_empl_last_target": "yes"}, {"question": "If I need to temporarily store data during an op (retrieve files, sort them individually, write them to files, then merge the sorted files into the final result), what's the canonical way to do it? just use some temp directory or is there a special temp directory in dagster? Make the intermediates op outputs (I don't need them later)? If I want to have a file as an dagster output, which (python) type would I use for this?", "replies": ["If I understand the question correctly, you can use a custom/different IO Manager for specific ops. If you\u2019re using them to to materialize assets instead of jobs, you can look into <https://docs.dagster.io/concepts/assets/graph-backed-assets|Graph-Backed Assets>", "How do I hand over sth. to an IO Manager without loading it into memory first?", "As far as I know the default behavior in (all?) out of the box IO managers is to load the data into memory for processing, e.g. the S3 IO Manager will save the output of `op_1` into S3 and load it into memory for processing by `op_2`. You could of course just return the file paths if you don\u2019t need to process the underlying data. What are the ops/steps doing exactly? I might be able to sketch out something more appropriate", "I've got blob storage, where files are stored as a/b/c{1,...n}. I generate a DynamicPartition with keys \"a/b\" for all a &amp; b.  Now I need to fetch all c1..n, which are large files that need to be sorted and appended into a resulting \"file\".", "I am at the point where I have the resulting file on my hard drive (below some temporary directory) and I want to hand it back to dagster for storage, best without loading it into memory completely.", "I think if you really want to leverage the IOManager concept one way would be to compile your files into a fileobject handle / BytesIO stream from within your op, then return that from the op. Then a custom IOManager could pick up the output from memory and store it. But you could also just pass the IOManager the path to where you wrote the aggregate file, and have it just store the path in memory / in a database / on disk as a small file that just contains the path to your larger file. Then if you need to load that output from another op the IOManager can use that to find your large file.\n\nIf you don't need to load the large aggregate file from a downstream op / asset then you don't really need the IOManager to do anything", "So passing around fileboject handles as assets is ok in dagster? Will buildin io managers handle that as well?", "Depends on the IO Manager, e.g. if you use the `fs_io_manager`, <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/fs_io_manager.py#L146|this logic> will be executed, calling `pickle.dump` on the returned object. I don\u2019t know off the top of my mind if that would support a BytesIO/Fileobj, but you could adapt the logic into your own IO Manager that will."], "is_dagster_empl": [false, false, false, false, false, false, false, false], "gpt4_replies_target": "You can use a custom IO Manager for specific ops in Dagster to temporarily store data. If you don't need to load the large aggregate file from a downstream op/asset, you don't really need the IOManager to do anything. You can compile your files into a fileobject handle / BytesIO stream from within your op, then return that from the op. A custom IOManager could then pick up the output from memory and store it. However, whether built-in IO managers can handle fileobject handles as assets depends on the specific IO Manager.", "dagster_empl_first_target": "If I understand the question correctly, you can use a custom/different IO Manager for specific ops. If you\u2019re using them to to materialize assets instead of jobs, you can look into <https://docs.dagster.io/concepts/assets/graph-backed-assets|Graph-Backed Assets>", "dagster_empl_last_target": "Depends on the IO Manager, e.g. if you use the `fs_io_manager`, <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/fs_io_manager.py#L146|this logic> will be executed, calling `pickle.dump` on the returned object. I don\u2019t know off the top of my mind if that would support a BytesIO/Fileobj, but you could adapt the logic into your own IO Manager that will."}, {"question": "Hi, dagster team!\n\nIs there a way to possibly connect an asset with an op? I have generated an asset that reads a csv file from s3 and I want to use greatexpectations (dagster-ge) op to test this one out. Thanks :slightly_smiling_face:", "replies": ["One idea would be to model the `dagster-ge`  op as a <https://docs.dagster.io/concepts/assets/graph-backed-assets#graph-backed-assets|graph-backed asset>, with the results of the QC checks being your asset. Then the GreatExpectations asset graph could depend on the asset you want to QC.\n\nAnother idea (might be a little more of a hack) that I personally haven't tried before would be to use the <https://docs.dagster.io/_apidocs/io-managers#dagster.InputManager|new >`root_input_manager`<https://docs.dagster.io/_apidocs/io-managers#dagster.InputManager| abstraction> to load the asset by just embedding the same logic the upstream asset's IOManager uses to load the asset's value. This wouldn't be recognized as a dependency between the op and the asset by Dagster, which may or may not be what you want (the graph-backed asset approach would show your `dagster-ge` asset as stale every time you materialized the upstream, which may or may not be what you want).\n\nOther than that I don't think there's a Dagster-native way to load an asset from an op, but I think it's a feature that's on the Dagster team's radar as it comes up pretty regularly.", "A variant on Zach's second suggestion that might be a little easier is this capability that we recently added: <https://docs.dagster.io/guides/dagster/how-assets-relate-to-ops-and-graphs#op-graphs-that-read-from-an-asset>", "Hey Zach, Sandy, thank you so much for your inputs. After quite a lot of internalization, I just converted the dagster-ge op into an asset using `AssetsDefinition.from_op` . I'll see if this will work :slightly_smiling_face:", "I am trying something similar, but I did try and incorporate a ge op made with get_validation_op_factory in a graph_asset as Sandy suggested.  However when the op is executed it looks like it is erroring out because resource_defs does not have ge_data_context defined.  Which is true because I haven't figured out how to pass resource_defs into the asset job definition which calls the graph_asset.  Am I missing something key to get the ge_op to work?"], "is_dagster_empl": [false, true, false, false], "gpt4_replies_target": "You can connect an asset with an op in Dagster by modeling the `dagster-ge` op as a graph-backed asset, with the results of the QC checks being your asset. Alternatively, you can use the `root_input_manager` abstraction to load the asset by embedding the same logic the upstream asset's IOManager uses to load the asset's value.", "dagster_empl_first_target": "A variant on Zach's second suggestion that might be a little easier is this capability that we recently added: <https://docs.dagster.io/guides/dagster/how-assets-relate-to-ops-and-graphs#op-graphs-that-read-from-an-asset>", "dagster_empl_last_target": "A variant on Zach's second suggestion that might be a little easier is this capability that we recently added: <https://docs.dagster.io/guides/dagster/how-assets-relate-to-ops-and-graphs#op-graphs-that-read-from-an-asset>"}, {"question": "~*[dagster-ge]* Another question: it has been mentioned that to save data docs, I need to set `validation_operator_name='action_list_operator'` . Can I specify where to put this data docs?~\n~Context: I want to save it into a temporary folder, upload the data docs to s3, host it using s3. And since it's in a temporary folder, after uploading to s3, it will be deleted automatically.~  \nEdit: Figured it out", "replies": ["Can you share the solution?", "This seems to be on the greatexpectations side. You would need to modify the great_expectations.yml to use s3 store in the action_list_operator. I haven't tested this out yet and I should be headed to the right direction"], "is_dagster_empl": [false, false], "gpt4_replies_target": "To save data docs in a specific location, you need to modify the great_expectations.yml to use s3 store in the action_list_operator.", "dagster_empl_first_target": "Can you share the solution?", "dagster_empl_last_target": "This seems to be on the greatexpectations side. You would need to modify the great_expectations.yml to use s3 store in the action_list_operator. I haven't tested this out yet and I should be headed to the right direction"}, {"question": "Is it hard to migrate an Airflow dag to Dagster?", "replies": ["There's been a ton of work by the Dagster team to provide some pretty magical tools for migrating Airflow DAGs to Dagster. Here's a couple links to get you started:\n<https://docs.dagster.io/integrations/airflow/migrating-to-dagster#migrating-airflow-to-dagster>\n<https://www.youtube.com/watch?v=YDO400lQhL4>\n<https://docs.dagster.io/integrations/airflow>\n<https://github.com/dagster-io/dagster/tree/1.1.20/examples/with_airflow>", "Thanks you!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "\"Migrating an Airflow dag to Dagster is made easier by the tools provided by the Dagster team, with resources available online to guide you through the process.\"", "dagster_empl_first_target": "There's been a ton of work by the Dagster team to provide some pretty magical tools for migrating Airflow DAGs to Dagster. Here's a couple links to get you started:\n<https://docs.dagster.io/integrations/airflow/migrating-to-dagster#migrating-airflow-to-dagster>\n<https://www.youtube.com/watch?v=YDO400lQhL4>\n<https://docs.dagster.io/integrations/airflow>\n<https://github.com/dagster-io/dagster/tree/1.1.20/examples/with_airflow>", "dagster_empl_last_target": "Thanks you!"}, {"question": "I'm getting the following error when attempting to yield an AssetMaterialization.\n```TypeError: SqlEventLogStorage._get_asset_entry_values() missing 1 required positional argument: 'has_asset_key_index_cols'```\nThis is the code that yields the asset materialization\n```        yield dagster.AssetMaterialization(\n            asset_key=dagster.AssetKey([google_sheet.title, sheet_name]),\n            metadata={\"url\": dagster.MetadataValue.url(google_sheet.url)},\n        )```\nI can see the ASSET_MATERIALIZATION event in the log (see screenshot). But immediately after that I see a STEP_FAILURE.\n\nAny ideas on what could be causing this problem?", "replies": ["I'm now suspecting a version mismatch between the repo (1.1.20) and dagit (1.1.3).", "I have upgraded dagit to 1.1.20 and the problem is gone."], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue was caused by a version mismatch between the repo (1.1.20) and dagit (1.1.3). Upgrading dagit to 1.1.20 resolved the problem.", "dagster_empl_first_target": "I'm now suspecting a version mismatch between the repo (1.1.20) and dagit (1.1.3).", "dagster_empl_last_target": "I have upgraded dagit to 1.1.20 and the problem is gone."}, {"question": "Hi Everyone,\nI'm able to pass arguments when invoking an op as per examples.\n```# pass config as arg\nmy_op(build_op_context(), 1, 2, MyConfig(z=3))```\nBut how to do this when calling the op from a job?\nWhen attempting to do the above in a job I get an error when starting up Dagster:\n```Must pass the output from previous node invocations or inputs to the composition function as inputs when invoking nodes during composition.```\n:thinking_face:", "replies": ["could you provide an example of what you're trying to do when you get this error? generally, when passing inputs to an op they need to be passed from another op's outputs. in Dagster you don't hard-code arguments in your graph, and the `context` is passed automatically under the hood - you won't need to use `build_op_context` when constructing a graph or calling ops (it's generally used for testing ops that rely on an `OpExecutionContext`. to pass arguments to your job to kick it off you'll generally use <https://docs.dagster.io/concepts/configuration/config-schema|run config>, which will allow you to pass initial configuration to each op that defines a config schema or pythonic config.", "What Zach said is correct-- have you done the Tutorial? Highly recommended to get acquainted with the basics:\n\n<https://docs.dagster.io/tutorial>", "thanks for the feedback guys, the code example above is from <https://github.com/dagster-io/dagster/pull/13459>", "just thinking if I have an op using a config, is there a way I can call the op from a job using different config values? (example below)\n```class MyConfig(Config):\n    z: int\n\n@op\ndef my_op(context, config: MyConfig):\n    return config.z\n\n# pass config as arg --&gt; works\nmy_op(MyConfig(z=3))\nmy_op(MyConfig(z=4))\n\n# how to invoke op from job with different config values? --&gt; doesn't work\n@job\ndef my_job():\n    my_op(MyConfig(z=3))\n    my_op(MyConfig(z=4))```", "using a hardcoded configuration for example works but then you can obviously only call the op once in the job with one set of configuration values\n```my_config = RunConfig(\n    ops={\"my_op\": MyConfig(z=3)}\n)\n\n@job(config=my_config)\ndef my_job():\n  my_op()```", "I'm busy with a POC and I mainly use SDAs, but in this case I've got an op that's basically doing a reverse ETL, preparing &amp; posting journals from a Data Warehouse to Oracle NetSuite via REST API", "Multiple invocations of the same op get aliased automatically (you can also choose the aliases). See here: <https://docs.dagster.io/concepts/ops-jobs-graphs/graphs#that-re-use-ops>\n\nThis allows you to set separate config for each call:\n\n```from dagster import Config, RunConfig, job, op\n\n\nclass AddNConfig(Config):\n    n: int\n\n\n@op\ndef return_one(context) -&gt; int:\n    return 1\n\n\n@op\ndef add_n(context, number, config: AddNConfig) -&gt; int:\n    return number + config.n\n\n\n@op\ndef adder(context, a: int, b: int) -&gt; int:\n    result = a + b\n    print(result)\n    return result\n\n\n@job\ndef inputs_and_outputs():\n    value = return_one()\n    a = add_n(value)\n    b = add_n(value)\n    adder(a, b)\n\n\ninputs_and_outputs.execute_in_process(\n    run_config=RunConfig({\n        \"add_n\": AddNConfig(n=1),\n        \"add_n_2\": AddNConfig(n=1)\n    })\n)```", "didn't know about the aliasing, I'll give that a go thank you"], "is_dagster_empl": [false, true, false, false, false, false, true, false], "gpt4_replies_target": "In Dagster, you don't hard-code arguments in your graph, instead, you pass inputs from another op's outputs. To pass arguments to your job, you use run config which allows you to pass initial configuration to each op that defines a config schema. If you want to invoke the same op multiple times with different config values in a job, you can use aliasing which allows you to set separate config for each call.", "dagster_empl_first_target": "What Zach said is correct-- have you done the Tutorial? Highly recommended to get acquainted with the basics:\n\n<https://docs.dagster.io/tutorial>", "dagster_empl_last_target": "Multiple invocations of the same op get aliased automatically (you can also choose the aliases). See here: <https://docs.dagster.io/concepts/ops-jobs-graphs/graphs#that-re-use-ops>\n\nThis allows you to set separate config for each call:\n\n```from dagster import Config, RunConfig, job, op\n\n\nclass AddNConfig(Config):\n    n: int\n\n\n@op\ndef return_one(context) -&gt; int:\n    return 1\n\n\n@op\ndef add_n(context, number, config: AddNConfig) -&gt; int:\n    return number + config.n\n\n\n@op\ndef adder(context, a: int, b: int) -&gt; int:\n    result = a + b\n    print(result)\n    return result\n\n\n@job\ndef inputs_and_outputs():\n    value = return_one()\n    a = add_n(value)\n    b = add_n(value)\n    adder(a, b)\n\n\ninputs_and_outputs.execute_in_process(\n    run_config=RunConfig({\n        \"add_n\": AddNConfig(n=1),\n        \"add_n_2\": AddNConfig(n=1)\n    })\n)```"}, {"question": "Is there a limit to asset key lengths?  My asset keys seem to be truncated to the last 44 characters.", "replies": ["Hmm I don\u2019t think so-- what does one of these truncated keys look like? Does it have any special chars?", "Did you resolve this? Was the issue special characters?", "thanks for following up - yes it was a weird case in our code.  thanks for commenting - it wasn't long after that we discovered it."], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "There doesn't seem to be a limit to asset key lengths in dagster, the issue of keys being truncated could be due to special characters or a specific case in your code.", "dagster_empl_first_target": "Hmm I don\u2019t think so-- what does one of these truncated keys look like? Does it have any special chars?", "dagster_empl_last_target": "Did you resolve this? Was the issue special characters?"}, {"question": "dagster.daemon - ERROR - Stopping dagster-daemon process since the following threads are no longer sending heartbeats: ['SENSOR', 'SCHEDULER']\n\nEDIT--\nI checked for more recent versions of dagster and there is a bugfix for this in a newer version 1.1.13 - I will try the newest version and report back in this thread.\n--\n\n\nMy dagster-daemon is dying fairly regularly now and it's distrupting my day-to-day operations, we end up losing ~8 hours of overnight processing when this happens.\n\nI need to figure out why dagster-daemon is dying and address the root cause.\n\nPlease help!\nThanks,\nNick\n\n```dagster.daemon - ERROR - Stopping dagster-daemon process since the following threads are no longer sending heartbeats: ['SENSOR', 'SCHEDULER']\ndagster.daemon - INFO - Shutting down daemon threads...\ndagster.daemon - INFO = Daemon threads shut down.```\n", "replies": ["Hi Nicholas, Good that you\u2019ve identified an update may fix the issue-- did you try it yet?", "I did apply the update, but because this issue was occurring a couple times a week, I have to wait and see if the issue is fixed. So far things are good but I am getting an unrelated error : unexpected graphql  - it doesn't appear to be affecting the rest of dagster - my scheduler seems to be working fine.\n```sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: kvs\n[SQL: SELECT kvs.\"key\", kvs.value \nFROM kvs \nWHERE kvs.\"key\" IN (?)]\n[parameters: ('ASSET_DAEMON_PAUSED',)]\n(Background on this error at: <https://sqlalche.me/e/14/e3q8>)```", "", "when i try to turn off auto-materializing i get the error above", "hmmm, cc <@U015C9U9RLK> as the auto-materialize daemon guru", "The KVS table that you\u2019re missing was introduced in dagster 0.15- you can run `dagster instance migrate` in the same environment that you run dagit or use <https://docs.dagster.io/deployment/guides/kubernetes/how-to-migrate-your-instance> if you\u2019re on K8s. It\u2019s never a bad idea to make a db backup first", "got it , ty , will try it out after my in-process runs", "Hey Nicholas, just checking if you\u2019ve resolved the issue?", "Hi sean, yes the errors related to\n```sqlalchemy.exc.OperationalError: (sqlite3.OperationalError) no such table: kvs\n[SQL: SELECT kvs.\"key\", kvs.value \nFROM kvs \nWHERE kvs.\"key\" IN (?)]\n[parameters: ('ASSET_DAEMON_PAUSED',)]\n(Background on this error at: <https://sqlalche.me/e/14/e3q8>)```\nHave been resolved ! :+1:", "It has been about a week now, I am feeling confident that the dagster update resolved the scheduler daemon issue I was having. Tyvm for your help!"], "is_dagster_empl": [true, false, false, false, true, true, false, true, false, false], "gpt4_replies_target": "The issue with the dagster-daemon dying was resolved by updating to a newer version of dagster (1.1.13) which contained a bugfix for this issue. An additional error related to a missing KVS table was resolved by running `dagster instance migrate`.", "dagster_empl_first_target": "Hi Nicholas, Good that you\u2019ve identified an update may fix the issue-- did you try it yet?", "dagster_empl_last_target": "Hey Nicholas, just checking if you\u2019ve resolved the issue?"}, {"question": "Hey there,\n\nI\u2019m having some trouble with asset dependencies between multiple modules.\n\nI have a central `Definitions` which looks like this:\n\n```from dagster import Definitions, load_assets_from_modules\n\nfrom .assets import (\n  pipeline_classifications,\n  pipeline_emissions,\n  pipeline_valuations,\n)\n\ndefs = Definitions(\n  assets=load_assets_from_modules([\n    pipeline_classifications,\n    pipeline_emissions,\n    pipeline_valuations,\n  ])\n)```\nBut I\u2019m getting the error:\n\n```UserWarning: Error loading repository location spectacle:dagster._core.errors.DagsterInvalidDefinitionError: Input asset '[\"company_emissions_cost\"]' for asset '[\"archetype_financials\"]' is not produced by any of the provided asset ops and is not one of the provided sources```\n`company_emissions_cost` is defined in `pipeline_emissions` as:\n\n```@asset()\ndef company_emissions_cost(clean_cdp_emissions, companies_w_dtics):```\n`archetype_financials` is defined in `pipeline_classifications` as:\n\n```@asset(ins={'company_emissions_cost': AssetIn(key='company_emissions_cost')})\ndef archetype_financials(companies_w_dtics, company_emissions_cost):```\nWhat am I doing wrong? Help / guidance much appreciated! Thanks.", "replies": ["Hi Sim, I think we should confirm that the assets you think are being loaded by `load_assets_from_module` are actually being loaded. Can you try this:\n\n```assets=load_assets_from_modules([\n    pipeline_classifications,\n    pipeline_emissions,\n    pipeline_valuations,\n])\n\nfor defn in assets:\n    if isinstance(defn, AssetsDefinition):\n        print(defn.keys)```\nAnd confirm that what you expect is printed?", "thanks sean will try it and get back to you", "Thanks for your help on this. For some reason I forgot that at the end of the day these are all just python objects which can be inspected :slightly_smiling_face:\n\nI actually realised the problem was from `pipeline_classifcations` having its own Definitions variable. Which was getting read for some reason. Not sure if this is expected behaviour, but I've removed that extra Definitions and it's working now. Cheers! Refactor continues :slightly_smiling_face:"], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "The issue was due to `pipeline_classifications` having its own Definitions variable which was getting read for some reason. Removing the extra Definitions resolved the problem.", "dagster_empl_first_target": "Hi Sim, I think we should confirm that the assets you think are being loaded by `load_assets_from_module` are actually being loaded. Can you try this:\n\n```assets=load_assets_from_modules([\n    pipeline_classifications,\n    pipeline_emissions,\n    pipeline_valuations,\n])\n\nfor defn in assets:\n    if isinstance(defn, AssetsDefinition):\n        print(defn.keys)```\nAnd confirm that what you expect is printed?", "dagster_empl_last_target": "Hi Sim, I think we should confirm that the assets you think are being loaded by `load_assets_from_module` are actually being loaded. Can you try this:\n\n```assets=load_assets_from_modules([\n    pipeline_classifications,\n    pipeline_emissions,\n    pipeline_valuations,\n])\n\nfor defn in assets:\n    if isinstance(defn, AssetsDefinition):\n        print(defn.keys)```\nAnd confirm that what you expect is printed?"}, {"question": "What is the recommended pattern for detecting a materialization failure associated to an asset that is being materialized by an asset reconciliation sensor? There is no \"job\" that can be monitored using a job failure sensor, and an asset sensor only listens for successful materializations", "replies": ["Use case is to send a notification when the materialization fails for a subset of assets.", "Oh, i see that this may not be possible - <https://github.com/dagster-io/dagster/discussions/13958>\n\nIs there any downside to having multiple asset reconciliation sensors? It seems like the best way for now would be to configure an asset reconciliation sensor with specific tags and use the Dagster Cloud notification tooling", "hi <@U03V3QRND0U>! a few dimensions to this, but the first is that the regular `run_failure_sensor` should work for your purposes, as it (by default) monitors for failures of any of your jobs. You can check which asset keys a job was intended to materialize by looking at the `context.dagster_run.asset_selection` . You can then compare this to the set of assets that the run did materialize via\n```context.instance.get_records_for_run(run_id=run_id, of_type=DagsterEventType.ASSET_MATERIALIZATION).records```\nThe second dimension is that we recommend you use the new Auto Materialize Daemon, rather than `build_asset_reconciliation_sensor`, especially with Dagster Cloud (it works across repositories and is more performant). You can detect runs that were launched by the daemon by looking for the `\"dagster/auto_materialize\": \"true\"` tag on the run.", "ah, thanks! Yeah, we'll be moving towards the Daemon for sure.... Is there a way to access the Definition Metadata for an asset from an AssetKey? The idea here is that we could configure destinations/interested parties for a notification in the definition metadata for an asset.", "sorry, <@U01J51Y6B9D> - is it possible to access definition metadata from the sensor context?", "hi <@U03V3QRND0U>! yep this is possible -- you can always access your `Definitions` (or repository) object by importing it in your run status sensor code, i.e.\n```@run_status_sensor(...)\ndef my_sensor(...):\n    from my_stuff import defs\n    asset_graph = defs.get_repository_def().asset_graph\n    all_asset_defs = asset_graph.assets # list of all your AssetsDefinitions```", "from there, you can search that list for the key you're interested in"], "is_dagster_empl": [false, false, true, false, false, true, true], "gpt4_replies_target": "The recommended pattern for detecting a materialization failure associated with an asset is to use the regular `run_failure_sensor` which monitors for failures of any jobs. You can check which asset keys a job was intended to materialize by looking at the `context.dagster_run.asset_selection` and compare this to the set of assets that the run did materialize. Additionally, it's recommended to use the new Auto Materialize Daemon, rather than `build_asset_reconciliation_sensor`. You can detect runs that were launched by the daemon by looking for the `\"dagster/auto_materialize\": \"true\"` tag on the run.", "dagster_empl_first_target": "hi <@U03V3QRND0U>! a few dimensions to this, but the first is that the regular `run_failure_sensor` should work for your purposes, as it (by default) monitors for failures of any of your jobs. You can check which asset keys a job was intended to materialize by looking at the `context.dagster_run.asset_selection` . You can then compare this to the set of assets that the run did materialize via\n```context.instance.get_records_for_run(run_id=run_id, of_type=DagsterEventType.ASSET_MATERIALIZATION).records```\nThe second dimension is that we recommend you use the new Auto Materialize Daemon, rather than `build_asset_reconciliation_sensor`, especially with Dagster Cloud (it works across repositories and is more performant). You can detect runs that were launched by the daemon by looking for the `\"dagster/auto_materialize\": \"true\"` tag on the run.", "dagster_empl_last_target": "from there, you can search that list for the key you're interested in"}, {"question": "Hey guys, I am still struggling with this one. I've created a stackoverflow issue <https://stackoverflow.com/questions/76173666/how-to-implement-io-manager-that-have-a-parameter-at-asset-level/76203434#76203434|here>:\n\nNow to solve the issue that I find in the proposed answer, I want to give the asset a specific io_manager.\nIs it possible to achieve this without subscribing them to the `Definition(resources={...})` ?\n\n```\n@asset(io_manager_def='parquet_io_manager',\n       partitions_def=hourly_partitions)\ndef my_custom_df(context) -&gt; pd.DataFrame:\n    start, end = context.asset_partitions_time_window_for_output()\n\n    df = pd.DataFrame({'timestamp': pd.date_range(start, end, freq='5T')})\n    df['count'] = df['timestamp'].map(lambda a:  random.randint(1, 1000))\n    return df\n\n\n\n# @asset(io_manager_def=PartitionedParquetIOManager(base_path=os.getenv('DAGSTER_ROOT_DATA_FOLDER', '/tmp'), time_column='timestamp'),\n#        partitions_def=daily_partitions)\n\n# @asset(io_manager_def={'resource_fn': PartitionedParquetIOManager,\n#                        'config_schema': {'base_path': os.getenv('DAGSTER_ROOT_DATA_FOLDER', '/tmp'), 'time_column': 'timestamp'}},\n#        partitions_def=daily_partitions)\n\n@asset(io_manager_def=IOManagerDefinition(resource_fn=PartitionedParquetIOManager,\n                                          config_schema={'base_path': os.getenv('DAGSTER_ROOT_DATA_FOLDER', '/tmp'), 'time_column': 'timestamp'}),\n       partitions_def=daily_partitions)\ndef another_custom_df(context, my_custom_df: pd.DataFrame) -&gt; pd.DataFrame:\n    return my_custom_df.set_index('timestamp').resample('H').agg({'count': 'mean'})```\nI am getting a `DagsterInvalidConfigDefinitionError` because a value cannot be resolved", "replies": ["OK I think that I've figured it out. I need to use the `@io_manager` decorator then I can provide an object that can call the `.configured()`  method.\n\nI've added another answer to the stackoverflow post.\nAny comment would be appreciated.", "Hi Guillaume, Trying to wrap my head around what you\u2019re trying to do here and make sure you\u2019re using the latest APIs.\n\nConfused by this part of your SO post:\n\n&gt; Now I have an issue if the same asset is used with different paritions sizes because the names not necesserally are the same during the reading and writing of the files. e.g. I have some 1h partitions asset and some 1d partitions asset using the same base asset.\nCould you elaborate on this? In Dagster an asset can have one and only one partitions definition.", "Firstly thank you for looking at my problem, the way you are asking for more information about the issue makes me believe that I am trying to force pushing a square piece through a circle hole...\n\nI am using dagster 1.3.2\n\n&gt;  In Dagster an asset can have one and only one partitions definition.\nImagine you have an asset that have an hourly partition. I am using the start &amp; end of the partition to write an unique file per partition\n```# in my io_manager\ndef handle_output(self, context: OutputContext, obj: pandas.DataFrame):\n    path = self._get_path(self, context)\n    obj.to_parquet(path=path, index=False)\ndef _get_path(self, context) -&gt; Path:\n        path = self._base_path / '/'.join(context.asset_key.path)\n    start, end = context.asset_partitions_time_window\n            dt_format = \"%Y-%m-%d_%H%M%S\"\n            partition_str = start.strftime(dt_format) + \"_\" + end.strftime(dt_format)\n    path = path / f'{partition_str}.parquet'\n    return path```\nAs long as I am using the same time window, I can read the same file from `_get_path()`.\n\nThen imagine that we want the values contained in this asset but in a different granularity. Lets say daily.\nThen when I am in the context of the new asset and I try to read the files, since I have a different time window, the file generated by the `_get_path()` does not exist and I have an error saying exactly that.\n\nStarting from that, I've tried to use the `filters` parameter from `pd.read_parquet` .\n\nDoes it makes any sense or even there I should have done differently?", "I see-- so I don\u2019t exactly understand what \u201cfiltering\u201d has to do with the problem, but IIUC your IO manager needs, given the start and end of the time window, to figure out some set of files to write/read specific slices to/from.\n\nIn the case where your partitions definition is aligned with the scheme that you are using to write files, then that\u2019s straightforward-- you just resolve the start/end to a path and read/write the whole file.\n\nIn the case where the partitions definition is _not_ aligned, then one file does not necessarily correspond to the data you want to load. So in that case, `_get_path` doesn\u2019t really make sense, you really need to get a set of (path, slice) pairs.\n\nYou can do this with custom logic, an IO manager does not need to implement `_get_path`-- just `load_input` and `handle_output`.", "&gt;  In the case where the partitions definition is _not_ aligned, then one file does not necessarily correspond to the data you want to load. So in that case, `_get_path` doesn\u2019t really make sense, you really need to get a set of (path, slice) pairs.\nyes that's it.\n\n&gt; You can do this with custom logic, an IO manager does not need to implement `_get_path`-- just `load_input` and `handle_output`.\nThat's what I did, using the `filters` parameter (`pd.read_parquet(folder_path, filters=[('timestamp', '&gt;=', start), ('timestamp', '&lt;', end)])`), `start` and `end` being the partition thresholds.\n\nAt this point I wanted to implement it in a way to parameterize the column on which the comparison is done.\n\nAnd I had a tough time understanding how it is possible to communicate from the asset to the io_manager.\nI mean it is still not perfectly clear but I manage to do it in two different ways.\nThe first is using a `resource_def` and a custom `Config` class.\nThe second by call the `@io_manager` decorator to create an object that can be provided to the `@asset` decorator.\n\nI guess that the second method is preferred.", "have you looked at `ConfigurableIOManager`: <https://docs.dagster.io/concepts/io-management/io-managers#defining-an-io-manager>", "the `@io_manager` decorator pattern is older and will eventually be deprecated/deleted", "ty, i'll look at that to avoid using deprecated features."], "is_dagster_empl": [false, true, false, true, false, true, true, false], "gpt4_replies_target": "The user was trying to implement an IO manager with a parameter at the asset level. They were advised to use the `@io_manager` decorator and the `.configured()` method. They were also informed that an asset can only have one partitions definition in Dagster. The user was also advised to look at `ConfigurableIOManager` and informed that the `@io_manager` decorator pattern is older and will eventually be deprecated.", "dagster_empl_first_target": "Hi Guillaume, Trying to wrap my head around what you\u2019re trying to do here and make sure you\u2019re using the latest APIs.\n\nConfused by this part of your SO post:\n\n&gt; Now I have an issue if the same asset is used with different paritions sizes because the names not necesserally are the same during the reading and writing of the files. e.g. I have some 1h partitions asset and some 1d partitions asset using the same base asset.\nCould you elaborate on this? In Dagster an asset can have one and only one partitions definition.", "dagster_empl_last_target": "the `@io_manager` decorator pattern is older and will eventually be deprecated/deleted"}, {"question": "I\u2019m having an issue with hightouch integration, I\u2019m getting this error:\n```dagster._core.errors.DagsterExecutionStepExecutionError: Error occurred while executing op \"hightouch_deal_sync\":\nThe above exception was caused by the following exception:\nAttributeError: 'tuple' object has no attribute 'sync_and_poll'```\nAs far as I can tell, I\u2019ve followed the documentation - I have my hightouch API key set up and configured for the branch deployment, and I\u2019ve provided the configurations that I see in the docs. It seems like there\u2019s something missing from my configuration, but I can\u2019t seem to find what it is", "replies": ["It looks like a syntax issue in your op - can you share any of that code?", "wow - just going through again I found a small syntax error in the config object, thanks for the hint on where to look!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue seems to be a syntax error in your op, you should review your code to identify and correct it.", "dagster_empl_first_target": "It looks like a syntax issue in your op - can you share any of that code?", "dagster_empl_last_target": "wow - just going through again I found a small syntax error in the config object, thanks for the hint on where to look!"}, {"question": "I have noticed that staleness isn't always properly detected. I think it's a recent regression. I'm only on 1.3.1, so might be fixed, but cannot see any related bugfixes in the changelogs.\nFrom the screenshot, the materialization data of the upstream is more recent. The downstream asset actually had its dependency changed, and as far as I can tell its system tags are still referencing the old dependency. Both the new and the old upstream asset was part of a multi-asset.", "replies": ["likely an important detail: these two assets are from different code locations", "Thanks for the report and seems like a genuine bug. Staleness has been getting overhauled recently. Would you mind opening a Github issue? That\u2019s probably a better forum for this.", "<https://github.com/dagster-io/dagster/issues/14181>"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "The issue you're experiencing with staleness not being properly detected seems like a genuine bug, and it's recommended that you open a Github issue to address this problem.", "dagster_empl_first_target": "Thanks for the report and seems like a genuine bug. Staleness has been getting overhauled recently. Would you mind opening a Github issue? That\u2019s probably a better forum for this.", "dagster_empl_last_target": "Thanks for the report and seems like a genuine bug. Staleness has been getting overhauled recently. Would you mind opening a Github issue? That\u2019s probably a better forum for this."}, {"question": "Hi <@U011CET83FG> Good Morning! here again to ask about anti patterns! :sweat_smile:\n\nso I finally implemented a pipeline using assets.... quick question... By merely looking at this diagram would you say my use case for dagster is more suited for task based orchestration (i.e. using ops and creating a job out of them)?\n\n*CONTEXT*\n\n\u2022 The first step materializes the data from the api\n\u2022 The second step cleans the data and (explicitly) writes it to an s3 bucket, the second step only returns/ materializes an s3 URL (wrong use case of asset??)  \n\u2022 The third step picks up the s3 url from the second step and basically stages the data on snowflake and eventually  writes/copy the data into a Snowflake table\nI should also note that i'm using s3 IO manager (can that just be my datalake or I should actually be writing data myself into another s3 bucket as I'm currently doing in my second step?)\n\nThanks in advance!", "replies": ["Hi Emmanuel - how do you decide what S3 bucket the second step is going to write to? If you know it ahead of time, I think it's a good use case for assets. If you decide it dynamically at runtime, ops are likely the best way to go", "Hey Sandy, I know the bucket name ahead of time.\n\nI only generate the filename dynamically at runtime e.g.:\n\n\u2022  if the run is for `2022-12-01` \n\u2022  The file name will be `2022-12-01.parquet` \n\u2022 The s3 url will become `<s3://known_Bucket_Name/2022-12-01.parquet>`\n", "<@U011CET83FG> also, is it okay if an asset represents an S3 url like in my case? I guess a more general way of putting it is can a file path string be an asset or the file itself? (in some sense, they both represent the file I guess?)\n\nwhat are your thoughts?", "if you have a run for every day, I'd recommend using partitioned assets for this. have you had a chance to look at those before?", "Yup, they're currently partitioned actually...just wanted to be sure I wasn't misusing assets here :raised_hands::skin-tone-4:", "would something like this work?\n\n```from dagster import asset, DailyPartitionsDefinition\n\n\npartitions_def = DailyPartitionsDefinition(start_date=\"2020-01-01\")\n\n\ndef _make_file_path(context, asset_name):\n    return f\"<s3://my_bucket_name/{asset_name}/{context.partition_key}>\"\n\n\n@asset(partitions_def=partitions_def)\ndef asset1(context) -&gt; None:\n    output_path = _make_file_path(context, \"asset1\")\n    # write to S3\n\n\n@asset(partitions_def=partitions_def, non_argument_deps={\"asset1\"})\ndef asset2(context) -&gt; None:\n    input_path = _make_file_path(context, \"asset1\")\n    # read from S3```\nwe usually recommend against `@asset`-decorated functions returning URLs and paths, because, for it to be a good use case for software-defined assets, normally the path is known ahead of time, so there's no need for the downstream step to find it out dynamically", "yes, this makes perfect sense! the \"private\" function can pick up the actual context from within the asset's compute_fn. thank you!!!"], "is_dagster_empl": [true, false, false, true, false, true, false], "gpt4_replies_target": "Based on the context provided, your use case for Dagster seems to be more suited for task-based orchestration using ops and creating a job out of them. However, if you know the S3 bucket name ahead of time and only generate the filename dynamically at runtime, using partitioned assets could be a good approach. It's generally recommended against having `@asset`-decorated functions return URLs and paths, as the path is usually known ahead of time.", "dagster_empl_first_target": "Hi Emmanuel - how do you decide what S3 bucket the second step is going to write to? If you know it ahead of time, I think it's a good use case for assets. If you decide it dynamically at runtime, ops are likely the best way to go", "dagster_empl_last_target": "would something like this work?\n\n```from dagster import asset, DailyPartitionsDefinition\n\n\npartitions_def = DailyPartitionsDefinition(start_date=\"2020-01-01\")\n\n\ndef _make_file_path(context, asset_name):\n    return f\"<s3://my_bucket_name/{asset_name}/{context.partition_key}>\"\n\n\n@asset(partitions_def=partitions_def)\ndef asset1(context) -&gt; None:\n    output_path = _make_file_path(context, \"asset1\")\n    # write to S3\n\n\n@asset(partitions_def=partitions_def, non_argument_deps={\"asset1\"})\ndef asset2(context) -&gt; None:\n    input_path = _make_file_path(context, \"asset1\")\n    # read from S3```\nwe usually recommend against `@asset`-decorated functions returning URLs and paths, because, for it to be a good use case for software-defined assets, normally the path is known ahead of time, so there's no need for the downstream step to find it out dynamically"}, {"question": "does anyone know how to cache or keep in memory an io manager handled output?\n```class IOBigQToDF(IOManager):\n\n\tdef handle_output(self, context: OutputContext, obj: str) -&gt; None:\n\t\tdf: pl.DataFrame = context.resources.rsrc_bigquery_runner(obj).bigquery_to_df()\n\t\tfilter_func = context.config[\"filter_func\"]\n\t\tif filter_func is None:\n\t\t\tget_dagster_logger().info(\n\t\t\t\tf'DATAFRAME WAS NOT ADJUSTED, PER OP: {str(df.shape)} columns: {str(df.shape[1])}, rows: {str(df.shape[0])}')\n\t\t\tself.result = df\n\t\telse:\n\t\t\tfiltered_df = filter_func(df)\n\t\t\tget_dagster_logger().info(\n\t\t\t\tf'ADJUSTED DATAFRAME SHAPE: {str(filtered_df.shape)} columns: {str(filtered_df.shape[1])}, rows: {str(filtered_df.shape[0])}')\n\t\t\tself.result = filtered_df\n\n\tdef load_input(self, context: InputContext) -&gt; pl.DataFrame:\n\t\treturn self.result```\ni thought that i could keep it in the `self.result` attribute but i get an error saying `AttributeError: 'IOBigQToDF' object has no attribute 'result'` when i try to use the io manager as an input.\n\nwhen it's used as an output handler, the logger prints out the line above, and that's how i know it works.  but i guess after that, the instance is no longer used??\n\nthis doesn't work:\n```@op#(ins={\"bq_to_df_io\": In(input_manager_key=\"io_bigq_to_df\")})\ndef some_op(bq_to_df_io):\n   get_dagster_logger().info(bq_to_df_io)```\n", "replies": ["for example if i use this:\n```def load_input(self, context: InputContext):\n   return(context.upstream_output.get_output_identifier())```\ni will get the following:\n`['10b2c5a6-ea86-43b6-9889-e150a161ad09', 'upstream_op', 'result']`  but how do i tap into the value of result", "ok this answer it:\n```To store intermediate values in memory, instead of on-disk, you can use mem_io_manager. For mem_io_manager to work, all the ops within an execution need to share the same process, which requires using the in_process_executor.```\ngot it <https://github.com/dagster-io/dagster/discussions/6899|here>"], "is_dagster_empl": [false, false], "gpt4_replies_target": "To store intermediate values in memory, you can use `mem_io_manager` which requires all the ops within an execution to share the same process, using the `in_process_executor`.", "dagster_empl_first_target": "for example if i use this:\n```def load_input(self, context: InputContext):\n   return(context.upstream_output.get_output_identifier())```\ni will get the following:\n`['10b2c5a6-ea86-43b6-9889-e150a161ad09', 'upstream_op', 'result']`  but how do i tap into the value of result", "dagster_empl_last_target": "ok this answer it:\n```To store intermediate values in memory, instead of on-disk, you can use mem_io_manager. For mem_io_manager to work, all the ops within an execution need to share the same process, which requires using the in_process_executor.```\ngot it <https://github.com/dagster-io/dagster/discussions/6899|here>"}, {"question": "Is Dagster Serverless down ?\n_Could not reach user code server. gRPC Error code: UNKNOWN_", "replies": ["this seems have been resolved in <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1674786421169709>"], "is_dagster_empl": [true], "gpt4_replies_target": "The issue with Dagster Serverless being down appears to have been resolved as per the discussion in this thread: <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1674786421169709>.", "dagster_empl_first_target": "this seems have been resolved in <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1674786421169709>", "dagster_empl_last_target": "this seems have been resolved in <https://dagster.slack.com/archives/C02LJ7G0LAZ/p1674786421169709>"}, {"question": "Hi team,\nI migrated dagster from 0.15.7 to 1.0.6, making the change from pipelines/solids to jobs/ops.\nNow each op takes way longer than before to start, so a pipeline that used to take one minute now as a job take x10 that.\nAny idea why it now takes so long from \"launching process\" to \"executing step\" ?", "replies": ["In case anyone faces this, because the default executor changed from `in_process` to `multi_or_in_process_executor` , it was running in multiprocess mode"], "is_dagster_empl": [false], "gpt4_replies_target": "The increase in execution time could be due to the default executor changing from `in_process` to `multi_or_in_process_executor`, causing it to run in multiprocess mode.", "dagster_empl_first_target": "In case anyone faces this, because the default executor changed from `in_process` to `multi_or_in_process_executor` , it was running in multiprocess mode", "dagster_empl_last_target": "In case anyone faces this, because the default executor changed from `in_process` to `multi_or_in_process_executor` , it was running in multiprocess mode"}, {"question": "I have some failed partitions in my past runs and it makes this screen look very bad. Big swaths of the graph are red, but the latest partitions are fine. I am unable to find a way in the UI to clear this failed partition status and I will be presenting our dagster implementation to our company soon. It's not a good look at the moment. I deleted the failed run, but the failed partition remains with no obvious way to clear it. As it so happens that particular partition will never succeed, so i'd also be interested in solution for that, but mostly i need to get rid of the red. :smile:\n\n(Sorry for the redactions)", "replies": ["Is fixing it locally and showing them that an option?", "yep, that could work", "although, in the long run I'll have to fix it in real env", "it _seems_ like i might have to get into the db or maybe into s3 where dagster is storing some state", "i'm just not sure where this record of failure is stored", "Oof... look like it's stored in `cached_status_data` from `asset_keys` table as json but in text.\n\n```                                                  Table \"public.asset_keys\"\n             Column             |            Type             | Collation | Nullable |                Default\n--------------------------------+-----------------------------+-----------+----------+----------------------------------------\n id                             | integer                     |           | not null | nextval('asset_keys_id_seq'::regclass)\n asset_key                      | character varying(512)      |           |          |\n last_materialization           | text                        |           |          |\n last_run_id                    | character varying(255)      |           |          |\n asset_details                  | text                        |           |          |\n wipe_timestamp                 | timestamp without time zone |           |          |\n last_materialization_timestamp | timestamp without time zone |           |          |\n tags                           | text                        |           |          |\n create_timestamp               | timestamp without time zone |           |          | CURRENT_TIMESTAMP\n cached_status_data             | text                        |           |          |```\nI had tried clearing status cache with dagster cli, but it did not seem to change anything. text column makes it hard to query the values in this json. Still not 100% sure that this is the source/cause of what i'm seeing though.", "Ok, got it. CLI did it. `dagster asset wipe-partitions-status-cache --all`", "This should probably be an option in the UI either for all assets or when viewing individual", "Ohh good find!", "Being able to manually flag partitions as materialised/failed/clear would be extremely useful. It's actually a rare offering airflow's gui supports that dagster doesn't! This would also allow us to note e.g. manually materialised assets as materialised."], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "To clear the failed partition status in Dagster, you can use the Dagster CLI command `dagster asset wipe-partitions-status-cache --all`. However, this feature is currently not available in the UI.", "dagster_empl_first_target": "Is fixing it locally and showing them that an option?", "dagster_empl_last_target": "Being able to manually flag partitions as materialised/failed/clear would be extremely useful. It's actually a rare offering airflow's gui supports that dagster doesn't! This would also allow us to note e.g. manually materialised assets as materialised."}, {"question": "Hey, i'm running into an error when trying to use RepositoryData with partitioned assets. Whenever try to run a job i get \"Unable to find partition set on {job name}\" (see pic). I've replicated the issue with a simple example:\n```def partitioned_asset(partitions: List[str]) -&gt; AssetsDefinition:\n    @asset(partitions_def=StaticPartitionsDefinition(partitions))\n    def some_asset():\n        logger = get_dagster_logger()\n        <http://logger.info|logger.info>(\"running asset\")\n    return some_asset\n\nclass ReloadableRepository(RepositoryData):\n    def get_all_pipelines(self) -&gt; Sequence[PipelineDefinition]:\n        some_asset = partitioned_asset([\"partition1\", \"partition2\"])\n        unresolved_asset_job = define_asset_job(name=\"asset_job\", selection=[some_asset])\n        resovled_asset_job = unresolved_asset_job.resolve(asset_graph=AssetGraph.from_assets([some_asset]))\n        return [resovled_asset_job]\n\n    def get_top_level_resources(self) -&gt; Mapping[str, ResourceDefinition]:\n        return {}\n\n@repository(name=\"test\")\ndef repo():\n    return ReloadableRepository()```\nAny help would be much appreciated :slightly_smiling_face:", "replies": ["Ah, just found this api which fixed it :slightly_smiling_face:\n`def get_all_partition_sets(self) -&gt; Sequence[PartitionSetDefinition]:`"], "is_dagster_empl": [false], "gpt4_replies_target": "The issue can be resolved by using the API `def get_all_partition_sets(self) -> Sequence[PartitionSetDefinition]:`.", "dagster_empl_first_target": "Ah, just found this api which fixed it :slightly_smiling_face:\n`def get_all_partition_sets(self) -&gt; Sequence[PartitionSetDefinition]:`", "dagster_empl_last_target": "Ah, just found this api which fixed it :slightly_smiling_face:\n`def get_all_partition_sets(self) -&gt; Sequence[PartitionSetDefinition]:`"}, {"question": "Hi all,\nI've been playing around with adding metadata to the output of my io manager and I managed to get things working, I now have nice graphs and markdowns showing in Dagit! :dagster-spin:\nWhile testing I've created a few test metadata outputs that are no longer being created on new asset materializations, however I still see these old values in Dagit (although they are correctly not being updated anymore for new runs).\nIs there an easy way to remove any metadata that is no longer being generated? If that means losing my historic runs, I'm perfectly fine with that.", "replies": ["Found it!\nIn Dagit -&gt; Assets -&gt; select assets and wiping materializations erases the history including the old metadata.", "This is lovely! Do you have any examples one can copy?", "I'm afraid I may have set the expectations a bit high, but I simply adjusted the default io manager by copying its entire contents and adding:\n\n```context.add_output_metadata(\n    {\n        \"Rows\": obj.shape[0],\n        \"Columns\": obj.shape[1],\n        \"Sample\": MarkdownMetadataValue(obj.head(10).to_markdown()),\n    }\n)```\nto the output.\n\nI've tried the csv io manager of <https://github.com/dagster-io/dagster/tree/master/examples/assets_pandas_type_metadata> as well, but it expects an AssetKey on every input/output, which I couldn't get to work when not working with assets.\n\nThe full code of my io manager is the following (a direct copy paste of the default one, but with the above code block added, as well as an import for MarkdownMetadataValue)):\n```import pickle\nfrom typing import Any\n\nfrom upath import UPath\n\nimport dagster._check as check\nfrom dagster import DagsterInvariantViolationError\nfrom dagster._core.definitions.metadata import (\n    MarkdownMetadataValue,\n)\nfrom dagster._core.execution.context.input import InputContext\nfrom dagster._core.execution.context.output import OutputContext\nfrom dagster._core.storage.upath_io_manager import UPathIOManager\nfrom dagster._utils import PICKLE_PROTOCOL\n\n\nclass PickledObjectFilesystemIOManager(UPathIOManager):\n    \"\"\"Built-in filesystem IO manager that stores and retrieves values using pickling.\n    Is compatible with local and remote filesystems via `universal-pathlib` and `fsspec`.\n    Learn more about how to use remote filesystems here: <https://github.com/fsspec/universal_pathlib>\n\n    Args:\n        base_dir (Optional[str]): base directory where all the step outputs which use this object\n            manager will be stored in.\n        **kwargs: additional keyword arguments for `universal_pathlib.UPath`.\n    \"\"\"\n\n    extension: str = (\n        \"\"  # TODO: maybe change this to .pickle? Leaving blank for compatibility.\n    )\n\n    def __init__(self, base_dir=None, **kwargs):\n        self.base_dir = check.opt_str_param(base_dir, \"base_dir\")\n\n        super().__init__(base_path=UPath(base_dir, **kwargs))\n\n    def dump_to_path(self, context: OutputContext, obj: Any, path: UPath):\n\n        try:\n            with path.open(\"wb\") as file:\n                pickle.dump(obj, file, PICKLE_PROTOCOL)\n\n        except (AttributeError, RecursionError, ImportError, pickle.PicklingError) as e:\n            executor = context.step_context.pipeline_def.mode_definitions[\n                0\n            ].executor_defs[0]\n\n            if isinstance(e, RecursionError):\n                # if obj can't be pickled because of RecursionError then __str__() will also\n                # throw a RecursionError\n                obj_repr = f\"{obj.__class__} exceeds recursion limit and\"\n            else:\n                obj_repr = obj.__str__()\n\n            raise DagsterInvariantViolationError(\n                f\"Object {obj_repr} is not picklable. You are currently using the \"\n                f\"fs_io_manager and the {executor.name}. You will need to use a different \"\n                \"io manager to continue using this output. For example, you can use the \"\n                \"mem_io_manager with the in_process_executor.\\n\"\n                \"For more information on io managers, visit \"\n                \"<https://docs.dagster.io/concepts/io-management/io-managers> \\n\"\n                \"For more information on executors, vist \"\n                \"<https://docs.dagster.io/deployment/executors#overview>\"\n            ) from e\n\n        context.add_output_metadata(\n            {\n                \"Rows\": obj.shape[0],\n                \"Columns\": obj.shape[1],\n                \"Sample\": MarkdownMetadataValue(obj.head(10).to_markdown()),\n            }\n        )\n\n    def load_from_path(self, context: InputContext, path: UPath) -&gt; Any:\n        with path.open(\"rb\") as file:\n            return pickle.load(file)```\nThere may be easier/better solutions, but hey, it works. :slightly_smiling_face:"], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "To remove metadata that is no longer being generated, you can go to Dagit -> Assets, select the assets and wipe materializations. This will erase the history including the old metadata.", "dagster_empl_first_target": "Found it!\nIn Dagit -&gt; Assets -&gt; select assets and wiping materializations erases the history including the old metadata.", "dagster_empl_last_target": "I'm afraid I may have set the expectations a bit high, but I simply adjusted the default io manager by copying its entire contents and adding:\n\n```context.add_output_metadata(\n    {\n        \"Rows\": obj.shape[0],\n        \"Columns\": obj.shape[1],\n        \"Sample\": MarkdownMetadataValue(obj.head(10).to_markdown()),\n    }\n)```\nto the output.\n\nI've tried the csv io manager of <https://github.com/dagster-io/dagster/tree/master/examples/assets_pandas_type_metadata> as well, but it expects an AssetKey on every input/output, which I couldn't get to work when not working with assets.\n\nThe full code of my io manager is the following (a direct copy paste of the default one, but with the above code block added, as well as an import for MarkdownMetadataValue)):\n```import pickle\nfrom typing import Any\n\nfrom upath import UPath\n\nimport dagster._check as check\nfrom dagster import DagsterInvariantViolationError\nfrom dagster._core.definitions.metadata import (\n    MarkdownMetadataValue,\n)\nfrom dagster._core.execution.context.input import InputContext\nfrom dagster._core.execution.context.output import OutputContext\nfrom dagster._core.storage.upath_io_manager import UPathIOManager\nfrom dagster._utils import PICKLE_PROTOCOL\n\n\nclass PickledObjectFilesystemIOManager(UPathIOManager):\n    \"\"\"Built-in filesystem IO manager that stores and retrieves values using pickling.\n    Is compatible with local and remote filesystems via `universal-pathlib` and `fsspec`.\n    Learn more about how to use remote filesystems here: <https://github.com/fsspec/universal_pathlib>\n\n    Args:\n        base_dir (Optional[str]): base directory where all the step outputs which use this object\n            manager will be stored in.\n        **kwargs: additional keyword arguments for `universal_pathlib.UPath`.\n    \"\"\"\n\n    extension: str = (\n        \"\"  # TODO: maybe change this to .pickle? Leaving blank for compatibility.\n    )\n\n    def __init__(self, base_dir=None, **kwargs):\n        self.base_dir = check.opt_str_param(base_dir, \"base_dir\")\n\n        super().__init__(base_path=UPath(base_dir, **kwargs))\n\n    def dump_to_path(self, context: OutputContext, obj: Any, path: UPath):\n\n        try:\n            with path.open(\"wb\") as file:\n                pickle.dump(obj, file, PICKLE_PROTOCOL)\n\n        except (AttributeError, RecursionError, ImportError, pickle.PicklingError) as e:\n            executor = context.step_context.pipeline_def.mode_definitions[\n                0\n            ].executor_defs[0]\n\n            if isinstance(e, RecursionError):\n                # if obj can't be pickled because of RecursionError then __str__() will also\n                # throw a RecursionError\n                obj_repr = f\"{obj.__class__} exceeds recursion limit and\"\n            else:\n                obj_repr = obj.__str__()\n\n            raise DagsterInvariantViolationError(\n                f\"Object {obj_repr} is not picklable. You are currently using the \"\n                f\"fs_io_manager and the {executor.name}. You will need to use a different \"\n                \"io manager to continue using this output. For example, you can use the \"\n                \"mem_io_manager with the in_process_executor.\\n\"\n                \"For more information on io managers, visit \"\n                \"<https://docs.dagster.io/concepts/io-management/io-managers> \\n\"\n                \"For more information on executors, vist \"\n                \"<https://docs.dagster.io/deployment/executors#overview>\"\n            ) from e\n\n        context.add_output_metadata(\n            {\n                \"Rows\": obj.shape[0],\n                \"Columns\": obj.shape[1],\n                \"Sample\": MarkdownMetadataValue(obj.head(10).to_markdown()),\n            }\n        )\n\n    def load_from_path(self, context: InputContext, path: UPath) -&gt; Any:\n        with path.open(\"rb\") as file:\n            return pickle.load(file)```\nThere may be easier/better solutions, but hey, it works. :slightly_smiling_face:"}, {"question": "Hey team, I need some assistance with the issue I'm having, seemingly related to my user_code container and grpc.\n\nI am deploying Dagster in Docker with Docker Compose, running on a single EC2 instance. I've had a working instance for some time now, but after EC2 instance had some down time today, I cannot get my dagster project to start again (when using docker compose The error occurs almost instantly, just after the postgres container starts up.\n\nI conducted a rebuild of my containers (with no code changes), and I am starting to receve a GRPC Unavailable error for my user code repository container on startup. I'm wondering if pip installing a newer dependency version of something is causing this error or maybe changes on the EC2 instance are affecting this? (The downtime was to change from a private IP instance to a public IP instance.)\n\nSpecific Error:\n`/usr/local/lib/python3.10/site-packages/dagster/_core/workspace/context.py:602: UserWarning: Error loading repository location repository:dagster._core.errors.DagsterUserCodeUnreachableError: Could not reach user code server. gRPC Error code: UNAVAILABLE`\n\n\n`The above exception was caused by the following exception:`\n`daemon      | grpc._channel._InactiveRpcError: &lt;_InactiveRpcError of RPC that terminated with:`\n`daemon      |   status = StatusCode.UNAVAILABLE`\n`daemon      |   details = \"failed to connect to all addresses\"`\n`daemon      |   debug_error_string = \"{\"created\":\"@1676946492.319074179\",\"description\":\"Failed to pick subchannel\",\"file\":\"src/core/ext/filters/client_channel/client_channel.cc\",\"file_line\":3260,\"referenced_errors\":[{\"created\":\"@1676946492.319073239\",\"description\":\"failed to connect to all addresses\",\"file\":\"src/core/lib/transport/error_utils.cc\",\"file_line\":167,\"grpc_status\":14}]}\"`\n\n\n1x Container - UI\n1x Container - PostgreSQL DB\n1x Container - Daemon\n1x Container - User Code (I called it repository)\n\nHere is a github gist to all of the following files: <https://gist.github.com/daniel-daum/6d57b3c9f129e1f9f53985d7d14d35dc>\n\nDockerfile_dagster\nDockerfile_repo\ndocker-compose.yaml\ndagster.yaml\nworkspace.yaml\nsetup.py\ndocker compose stack trace\n\nNotes:\n\n- Docker compose works locally on my windows machine with &amp; without docker(from source), but not on my linux instance (docker). (I did run docker compose on my local windows machine, and recieved this same error, but the use_code container still started up properly a second later.)\n\n- This is also printed by the repo container: the AWS EC2 instance metadata IP appears for some reason?\n\n`repository  |     raise ReadTimeoutError(`\n`repository  | urllib3.exceptions.ReadTimeoutError: HTTPConnectionPool(host='169.254.169.254', port=80): Read timed out. (read timeout=5.0)`", "replies": ["Please ignore, it took some time but I figured out the issue", "It was not dagster related. It was related to running Docker on EC2, and it running into EC2 hop limits.\n\nI bind my aws credentials on startup, and periodically after that. When my EC2 instance restarted, it reset the network Hop Limit back to 1. Meaning, while I can access my AWS credentials via instance metadata on the host, I could not do so from inside a docker container on that same host (as its considered as another network, the request would have to hop). This was causing an error on startup, and explains why I was receiving the main AWS IP Addr in my stack trace.\n\nI set my hop limit back to 3, and was good to go."], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue was not related to Dagster but was due to the network Hop Limit being reset to 1 on the EC2 instance after a restart, which prevented accessing AWS credentials from inside a Docker container on the same host. The user resolved the issue by setting the hop limit back to 3.", "dagster_empl_first_target": "Please ignore, it took some time but I figured out the issue", "dagster_empl_last_target": "It was not dagster related. It was related to running Docker on EC2, and it running into EC2 hop limits.\n\nI bind my aws credentials on startup, and periodically after that. When my EC2 instance restarted, it reset the network Hop Limit back to 1. Meaning, while I can access my AWS credentials via instance metadata on the host, I could not do so from inside a docker container on that same host (as its considered as another network, the request would have to hop). This was causing an error on startup, and explains why I was receiving the main AWS IP Addr in my stack trace.\n\nI set my hop limit back to 3, and was good to go."}, {"question": "Hi all! Is there a way to restrict the number of <https://docs.dagster.io/concepts/ops-jobs-graphs/dynamic-graphs|DynamicOutput>  concurrently running on a machine?\nI have tried the <https://docs.dagster.io/deployment/run-coordinator#queuedruncoordinator|QueueRunCoordinator>, but the coordinator only controls the number of `jobs` being executed not the number of `ops`.\nMany thanks in advance.", "replies": ["I think I've found the answer. The number of processes that can be run concurrently can be set in the <https://docs.dagster.io/_apidocs/execution#dagster.multiprocess_executor|job config>"], "is_dagster_empl": [false], "gpt4_replies_target": "The number of processes that can run concurrently can be set in the job config as per the dagster multiprocess executor documentation.", "dagster_empl_first_target": "I think I've found the answer. The number of processes that can be run concurrently can be set in the <https://docs.dagster.io/_apidocs/execution#dagster.multiprocess_executor|job config>", "dagster_empl_last_target": "I think I've found the answer. The number of processes that can be run concurrently can be set in the <https://docs.dagster.io/_apidocs/execution#dagster.multiprocess_executor|job config>"}, {"question": "I am trying to load dbt assets by using `select`\n\nI am trying to build the `select` as string dynamically but it fails\n\nIf I make it static string then it works. Can anybody explain why ?", "replies": ["can you print both the static and dynamic string and see if there is any difference?", "I didnt see any difference visually, I dont currently have it running to print it, but when I last saw them it was the same during declaration, but while discovering dbt assets it failed", "if they are byte for byte equal then I don't see how the behaviour could be different when you call load_assets_from_dbt_project", "I'll find out and respond back"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "The issue might be due to some hidden differences between the static and dynamic strings, even if they appear the same visually. If they are byte for byte equal, there should be no difference in behavior when calling load_assets_from_dbt_project.", "dagster_empl_first_target": "can you print both the static and dynamic string and see if there is any difference?", "dagster_empl_last_target": "I'll find out and respond back"}, {"question": "Good morning!  I have been testing out dagster deployed on ec2 (trying to switch to ecs but ec2 for now). I have a very small cluster for testing, but keep running into issues with running out of space and I think it is because of log files being written (I am using postgres for metadata and right now am only materializing assets to from api to s3 and reading in one snowflake table as a dependency with the i/o manager). The problem I am having is that when I add `dagster_aws.s3.compute_log_manager` to my `dagster.yaml` file and restart my docker containers, the dagit immediately crashes and the daemon crashes and tries to restart - this happens both locally and in the cloud.  Do I need to do anything additional to make this work?", "replies": ["```compute_logs:\n  module: dagster_aws.s3.compute_log_manager\n  class: S3ComputeLogManager\n  config:\n    bucket: \"my-bucket-name\"\n    local_dir: \"/tmp/compute-logs-stg\"\n    prefix: \"dagster-compute-logs\"\n    skip_empty_files: false\n    upload_interval: 30```", "Here is the error I get (please let me know if this is something other than what I think - log files)", "```OSError: [Errno 28] No space left on device: '/opt/dagster/dagster_home/storage/partitioned-asset-name/2022-12-06'\n File \"/usr/local/lib/python3.7/site-packages/dagster/_core/execution/plan/utils.py\", line 52, in solid_execution_error_boundary\n  yield\n File \"/usr/local/lib/python3.7/site-packages/dagster/_utils/__init__.py\", line 460, in iterate_with_context\n  next_output = next(iterator)\n File \"/usr/local/lib/python3.7/site-packages/dagster/_core/execution/plan/execute_step.py\", line 626, in _gen_fn\n  gen_output = output_manager.handle_output(output_context, output.value)\n File \"/usr/local/lib/python3.7/site-packages/dagster/_core/storage/upath_io_manager.py\", line 231, in handle_output\n  self.dump_to_path(context=context, obj=obj, path=path)\n File \"/usr/local/lib/python3.7/site-packages/dagster/_core/storage/fs_io_manager.py\", line 148, in dump_to_path\n  with path.open(\"wb\") as file:\n File \"/usr/local/lib/python3.7/pathlib.py\", line 1208, in open\n  opener=self._opener)\n File \"/usr/local/lib/python3.7/pathlib.py\", line 1063, in _opener\n  return self._accessor.open(self, flags, mode)```", "Hi Linday - I think this is coming from your IO manager actually - it looks like the outputs from your assets/ops are what's filling up the filesystem. Is switching to the S3 IO manager an option here? <https://docs.dagster.io/deployment/guides/aws#using-s3-for-io-management>", "HI Daniel, makes sense.  I wonder what the difference between the s3 io manager and using boto to write to s3?  I actually have been meaning to refactor and use the built in i/o manager, but I am just curious if this is really the problem or if I might just not have enough memory.  Does the built in manager do something under the hood to avoid running out of space?", "If you have large outputs from your assets/ops, the built-in fs_io_manager (which I see in that stack trace) pickles them up and writes them to disk. It's possible that it's other things that filled up the disk originally, but it's the writing of outputs that's hitting the disk errors now.\n\nI'm not aware of anything that the built-in IO manager does to avoid running out of space, but it's a lot harder to run out of space in S3", "i see.  right now I am chunking the job and writing to s3 in batches, but I guess i'm testing on such a tiny system it just is too small.  thanks, this makes sense.,"], "is_dagster_empl": [false, false, false, true, false, true, false], "gpt4_replies_target": "The issue you're experiencing is likely due to your IO manager writing large outputs from your assets/ops to disk, causing it to run out of space. Switching to the S3 IO manager could help, as it's harder to run out of space in S3.", "dagster_empl_first_target": "Hi Linday - I think this is coming from your IO manager actually - it looks like the outputs from your assets/ops are what's filling up the filesystem. Is switching to the S3 IO manager an option here? <https://docs.dagster.io/deployment/guides/aws#using-s3-for-io-management>", "dagster_empl_last_target": "If you have large outputs from your assets/ops, the built-in fs_io_manager (which I see in that stack trace) pickles them up and writes them to disk. It's possible that it's other things that filled up the disk originally, but it's the writing of outputs that's hitting the disk errors now.\n\nI'm not aware of anything that the built-in IO manager does to avoid running out of space, but it's a lot harder to run out of space in S3"}, {"question": "Hi Dagster, we\u2019re trying out the `@asset` decorators at the moment. With all of our `jobs` in the config we include a `k8s_executor` config as well. How can the we include the k8s_executor in our assets as well - as when I try to materialise an asset it fails due to not having the correct executor config?", "replies": ["Hi Jamie - take a look at this: <https://docs.dagster.io/deployment/executors#for-a-code-location>"], "is_dagster_empl": [true], "gpt4_replies_target": "You can include the `k8s_executor` in your assets by referring to the Dagster documentation on how to set up executors for a code location.", "dagster_empl_first_target": "Hi Jamie - take a look at this: <https://docs.dagster.io/deployment/executors#for-a-code-location>", "dagster_empl_last_target": "Hi Jamie - take a look at this: <https://docs.dagster.io/deployment/executors#for-a-code-location>"}, {"question": "Who is the best person to nudge to review/merge a PR for dagster-azure?", "replies": ["you can request a review from me! i\u2019m `jamiedemaria` on github", "And what about Vercel and buildkite? I don't have permissions to see anything related to those failing steps so I am stuck in the state where it can't be merged and I cannot see what needs to be fixed :disappointed:", "I can kick off buildkite for you (I don\u2019t think vercel will be necessary since there aren\u2019t docs changes). Once BK runs I\u2019ll let you know what errors emerge (unfortunately our permissions don\u2019t allow external contributors to see the BK results, but we\u2019re trying to fix that)", "Can you take a look again <@U030M2AL48M>, I see the build failed. Sorry for the hassle, hope the fix to make it easier for contributors happens soon", "yep i\u2019m taking a look! unfortunately i don\u2019t know much about the helm test suite that\u2019s failing, so i\u2019m having another engineer take a look. sorry this is such a hassle", "thanks for looking into it", "merged your PR in! thanks for the contribution! we aren\u2019t doing a release next week, but it will be out the following week", "Can you please take a look at the other extremely similarly named PR? it does basically the same but for the ADLS2 resource authentication, it seems green"], "is_dagster_empl": [true, false, true, false, true, false, true, false], "gpt4_replies_target": "The best person to nudge to review/merge a PR for dagster-azure is `jamiedemaria` on Github.", "dagster_empl_first_target": "you can request a review from me! i\u2019m `jamiedemaria` on github", "dagster_empl_last_target": "merged your PR in! thanks for the contribution! we aren\u2019t doing a release next week, but it will be out the following week"}, {"question": "Hey everyone, I\u2019m wondering, would it possible to add a configuration or wrap an my warehouse IO Manager in another one so I would be able to save each assets in S3 as well as where the file should be stored define in the IO Manager?\n\nSo instead of creating 2 assets (One with IO Manager linked to S3) and one with the IO Manager link to Snowflake), only create 1 assets for Snowflake, but save it to S3 too at the same time, for backup purpose?", "replies": ["Hi Jacob, to implement this, you would need to write an IO manager that stores the data in both places\n\nThis might be a useful example of how to write an IO manager that delegates to another IO manager: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/branching/branching_io_manager.py>", "Thank you very much :slightly_smiling_face:"], "is_dagster_empl": [true, false], "gpt4_replies_target": "To achieve this, you would need to create an IO manager that stores the data in both S3 and Snowflake. You can refer to this example on how to write an IO manager that delegates to another IO manager: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/branching/branching_io_manager.py>.", "dagster_empl_first_target": "Hi Jacob, to implement this, you would need to write an IO manager that stores the data in both places\n\nThis might be a useful example of how to write an IO manager that delegates to another IO manager: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/branching/branching_io_manager.py>", "dagster_empl_last_target": "Hi Jacob, to implement this, you would need to write an IO manager that stores the data in both places\n\nThis might be a useful example of how to write an IO manager that delegates to another IO manager: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/branching/branching_io_manager.py>"}, {"question": "I\u2019m using Dagster Cloud Serverless, and I recently tried making a branch deployment which updated the runtime to Python 3.10 in my GitHub Action as shown <https://docs.dagster.io/dagster-cloud/deployment/serverless#using-a-different-python-version|here>, but now my branch deployments are getting the below error message when I try to deploy. Could I get some help figuring out why this won\u2019t deploy? Running the commit locally with dagit (and local Python 3.10 of course) works fine.\ncc from earlier convo: <@U03HP2GNN7Q>", "replies": ["Hey <@U04J3F8RJQH> - mind posting this in <#C02LJ7G0LAZ|dagster-cloud> channel?", "Sure thing!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "It seems like you're having issues with deploying your branch after updating the runtime to Python 3.10 in your GitHub Action. Please post this issue in the dagster-cloud channel for further assistance.", "dagster_empl_first_target": "Hey <@U04J3F8RJQH> - mind posting this in <#C02LJ7G0LAZ|dagster-cloud> channel?", "dagster_empl_last_target": "Hey <@U04J3F8RJQH> - mind posting this in <#C02LJ7G0LAZ|dagster-cloud> channel?"}, {"question": "This message was deleted.", "replies": ["I put the different Dockerfiles in their own directories and pointed `dagster-cloud.yaml` to the respective directories for each location, but they all fail with an error along the lines of\n```Error: buildx failed with: ERROR: failed to solve: failed to compute cache key: failed to calculate checksum of ref femc2novkejrbkoun33sp29rh::ljzx0b7tyxk77r80nfm2x6j88: failed to walk /tmp/buildkit-mount2643354055/src/teamster: lstat /tmp/buildkit-mount2643354055/src/teamster: no such file or directory```\n", "Hey Charlie - mind posting this in the <#C02LJ7G0LAZ|dagster-cloud> channel?", "yup will do!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "The user encountered an error while trying to build Dockerfiles in different directories using `dagster-cloud.yaml`, and was advised to post the issue in the dagster-cloud channel for further assistance.", "dagster_empl_first_target": "Hey Charlie - mind posting this in the <#C02LJ7G0LAZ|dagster-cloud> channel?", "dagster_empl_last_target": "Hey Charlie - mind posting this in the <#C02LJ7G0LAZ|dagster-cloud> channel?"}, {"question": "Hey - I have an asset with a DynamicPartitionsDefinition and an AutoMaterializePolicy, I was expecting a materialization to kick off whenever a new partition was added but it's not happening. Am I missing something here?\n\n```@asset(\n    partitions_def=dynamic_partition_defs,\n    auto_materialize_policy=AutoMaterializePolicy.eager(),\n)\ndef foo(context):\n    ...```", "replies": ["D'oh. Didn't realize I had to toggle the auto-materializing daemon on :facepalm:", "All works OK once that's turned on (in deployment/daemons for anyone else who has the same issue)"], "is_dagster_empl": [false, false], "gpt4_replies_target": "You need to ensure that the auto-materializing daemon is turned on in the deployment/daemons for the DynamicPartitionsDefinition and AutoMaterializePolicy to work as expected.", "dagster_empl_first_target": "D'oh. Didn't realize I had to toggle the auto-materializing daemon on :facepalm:", "dagster_empl_last_target": "All works OK once that's turned on (in deployment/daemons for anyone else who has the same issue)"}, {"question": "Does Dagster have any utilities for merging run databases? I've got two separate history/storage folders and i'd like to merge them into one pool of runs if possible. Any tips?", "replies": ["What storage backend are you using? The logic for how it'd work would be dependent on that. As far as I know, we don't have utilities to merge databases together though.", "sqlite backend", "What exactly does dagster rely on for populating the `runs` page? If i just merge the `runs` table for runs.db,\nand the `event_logs` table within index.db, do you think i'll be OK? by merge i mean simple UNION", "Looking for some more help here. I've gotten pretty far merging the two tables that I mentioned above. However, i think im still missing some data, because runs that i've imported via the merge still say: \"Unable to build execution plan\" when loading their run page. Are there any docs about the various tables available within the sqlite storage backend? And what data is required to reconstruct an execution plan?", "at the very least a schema of the expected data when loading a run details page would be helpful", "Is there a way to view the dagit logs? running `docker logs` on my dagster-dev container doesn't actually show me errors from Dagit. Specifically looking to find the error message that indicates why my UI is `Unable to build execution plan`", "Hey there. thanks for asking these questions. I've triaged this over to the team to help out.", "Thank you!"], "is_dagster_empl": [true, false, false, false, false, false, true, false], "gpt4_replies_target": "Dagster does not currently have utilities to merge databases together, and the process would depend on the storage backend you are using. Merging the `runs` and `event_logs` tables might work, but you may still miss some data, causing issues like \"Unable to build execution plan\". There are no specific documents about the tables within the sqlite storage backend or the expected data schema. Viewing Dagit logs might help identify the issue.", "dagster_empl_first_target": "What storage backend are you using? The logic for how it'd work would be dependent on that. As far as I know, we don't have utilities to merge databases together though.", "dagster_empl_last_target": "Hey there. thanks for asking these questions. I've triaged this over to the team to help out."}, {"question": "Hello,\nI just noticed that with the latest version (`1.3.3`), the \"backfill only failed and missing partitions\" option is unchecked by default.\nIn previous version, it was checked by default to avoid/prevent rematerializing existing partitions by mistake. Is this new default the expected behavior ?", "replies": ["Hi Lucas, this is indeed the expected new behavior", "Hello <@U01GTMVMGQH>,\nThank you for the response. Is there a reason behind this choice ?\n\nAs we start working on time-partitioned assets with growing number of partitions (hundreds or thousands), this option being unchecked means more chance of rerunning all thousands of partitions at once by mistake if we trigger backfill manually from Dagit.\n\nIt felt safer to have it checked by default. Will it be possible to set this option to be checked by default (as I think it was the behavior in previous  versions) ?", "cc <@U011CET83FG> on ^^"], "is_dagster_empl": [true, false, true], "gpt4_replies_target": "The new default behavior of the \"backfill only failed and missing partitions\" option being unchecked in the latest version (`1.3.3`) of Dagster is indeed expected, despite the increased chance of rerunning all partitions at once if backfill is triggered manually from Dagit.", "dagster_empl_first_target": "Hi Lucas, this is indeed the expected new behavior", "dagster_empl_last_target": "cc <@U011CET83FG> on ^^"}, {"question": "I am new to Dagster and getting started on this amazing project. I am using `1.3.3` with Conda and Windows to run locally. The Conda is set up correctly. I am trying to run the `hello_dagster.py` from the Dagster docs, but no luck thus far.\nWhen I try this - `dagster dev --code-server-log-level DEBUG -p 6050 -f hello_dagster.py` - Dagster starts &amp; shutsdown immediately. However, when I try this - `dagster dev --code-server-log-level trace -p 6050 -f hello_dagster.py` - the Dasgster at least comes up! But the code does not execute with an error on the GRPC-Server.\nIt is rather strange that `DEBUG` &amp; `trace` in the command can make so much difference...\nAny tips or pointers would be really helpful. Thanks again.\n```DEBUG\n===\n(dagster_demo) C:\\some\\path&gt;dagster dev --code-server-log-level DEBUG -p 6050 -f hello_dagster.py\n2023-05-07 22:08:24 +0800 - dagster - INFO - Launching Dagster services...\n2023-05-07 22:08:38 +0800 - dagster - INFO - Started Dagster code server for file hello_dagster.py on port 52818 in process 12132\n2023-05-07 22:08:38 +0800 - dagster - INFO - Started Dagster code server for file hello_dagster.py on port 52820 in process 980\n2023-05-07 22:08:44 +0800 - dagster - INFO - Shutting down Dagster services...\n2023-05-07 22:08:44 +0800 - dagster - INFO - Dagster services shut down.\n\ntrace\n===\nException: gRPC server exited with return code 1 while starting up with the command: \n&lt;stack trace&gt;\nlib\\site-packages\\dagster\\_core\\host_representation\\grpc_server_registry.py\", line 194, in _get_grpc_endpoint\n    server_process = GrpcServerProcess(...```", "replies": ["I tried to run the `grpc Code Server` separately and then tired to connect the Dasgter - but when I do that both the `Code Server` and `Dagster` exit now  -\n```GRPC Code Server\n===\ndagster api grpc --python-file hello_dagster.py --host 0.0.0.0 --port 7001\n\nDagster Services\n===\ndagster dev --code-server-log-level trace -w workspace.yaml```", "I have tried to run the following command (as a python process) to see if the `grpc` servers exits but it seems to be ok -\n```(dagster_demo) C:\\some\\path\\miniforge3\\envs\\dagster_demo&gt;python.exe -m dagster api grpc --lazy-load-user-code --port 52707```", "Hi, I have tried with Python versions - `3.9, 3.10,  3.11` - still no luck.  Still the same - `Exception: gRPC server exited with return code 1 while starting up with the command:`", "Hi Ramkumar, can you provide more of the backtrace shown when you use trace?\n\n&gt; However, when I try this - dagster dev --code-server-log-level trace -p 6050 -f hello_dagster.py - the Dasgster at least comes up! But the code does not execute with an error on the GRPC-Server.\nWhat did you mean here by \u201cthe Dagster at least comes up\u201d?", "<@U01GTMVMGQH> - thank you for the reply. What I meant is that the Dagster UI comes up. What is really a mystery is that `--code-server-log-level debug` starts and shuts down the Dagster, whilst the `--code-server-log-level trace``  bring the Dagster UI. In the UI i can see a message saying that the `grpc server exited with 1`.  Change of log-levels has an impact on how the Dagster behaves?", "I also noticed a strange line in the console - why do we have 2 lines for the Code Server (looks like 1st one started and failed? So a 2nd attempt was made and that also failed?) -\n```(dagster_39) C:\\some\\path\\dagster_poc\\hello_dagster&gt;dagster dev --code-server-log-level info -p 4001 -f hello-dagster.py\n2023-05-09 12:22:11 +0800 - dagster - INFO - Launching Dagster services...\n2023-05-09 12:22:27 +0800 - dagster - INFO - Started Dagster code server for file hello-dagster.py on port 53588 in process 18928\n2023-05-09 12:22:27 +0800 - dagster - INFO - Started Dagster code server for file hello-dagster.py on port 53590 in process 24708\n2023-05-09 12:22:31 +0800 - dagster - INFO - Shutting down Dagster services...\n2023-05-09 12:22:31 +0800 - dagster - INFO - Dagster services shut down.```", "<@U01GTMVMGQH> - here is the backtrace from the `trace` level in the log -\n```C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\lib\\site-packages\\dagster\\_core\\workspace\\context.py:589: UserWarning: Error loading repository location hello-dagster.py:Exception: gRPC server exited with return code 1 while starting up with the command: \"C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\python.exe -m dagster api grpc --lazy-load-user-code --port 53139 --heartbeat --heartbeat-timeout 120 --fixed-server-id c4d859cc-f13f-49c3-859d-3a431989701f --log-level trace --inject-env-vars-from-instance --instance-ref {\"__class__\": \"InstanceRef\", \"compute_logs_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"LocalComputeLogManager\", \"config_yaml\": \"base_dir: C:\\\\Users\\\\someuser\\\\dev\\\\pythonprojects\\\\dagster_poc\\\\dagster_home2\\\\storage\\n\", \"module_name\": \"dagster.core.storage.local_compute_log_manager\"}, \"custom_instance_class_data\": null, \"event_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"SqliteEventLogStorage\", \"config_yaml\": \"base_dir: C:\\\\Users\\\\someuser\\\\dev\\\\pythonprojects\\\\dagster_poc\\\\dagster_home2\\\\history\\\\runs\\\\\\n\", \"module_name\": \"dagster.core.storage.event_log\"}, \"local_artifact_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"LocalArtifactStorage\", \"config_yaml\": \"base_dir: C:\\\\Users\\\\someuser\\\\dev\\\\pythonprojects\\\\dagster_poc\\\\dagster_home2\\n\", \"module_name\": \"dagster.core.storage.root\"}, \"run_coordinator_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DefaultRunCoordinator\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster.core.run_coordinator\"}, \"run_launcher_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DefaultRunLauncher\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster\"}, \"run_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"SqliteRunStorage\", \"config_yaml\": \"base_dir: C:\\\\Users\\\\someuser\\\\dev\\\\pythonprojects\\\\dagster_poc\\\\dagster_home2\\\\history\\\\\\n\", \"module_name\": \"dagster.core.storage.runs\"}, \"schedule_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"SqliteScheduleStorage\", \"config_yaml\": \"base_dir: C:\\\\Users\\\\someuser\\\\dev\\\\pythonprojects\\\\dagster_poc\\\\dagster_home2\\\\schedules\\n\", \"module_name\": \"dagster.core.storage.schedules\"}, \"scheduler_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DagsterDaemonScheduler\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster.core.scheduler\"}, \"secrets_loader_data\": null, \"settings\": {\"code_servers\": {\"local_startup_timeout\": 120}, \"telemetry\": {\"enabled\": false}}, \"storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DagsterSqliteStorage\", \"config_yaml\": \"base_dir: C:\\\\Users\\\\someuser\\\\dev\\\\pythonprojects\\\\dagster_poc\\\\dagster_home2\\n\", \"module_name\": \"dagster.core.storage.sqlite_storage\"}} --location-name hello-dagster.py -f hello-dagster.py -d C:\\Users\\someuser\\dev\\pythonprojects\\dagster_poc\\hello_dagster\"\n\nStack Trace:\n  File \"C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\lib\\site-packages\\dagster\\_core\\host_representation\\grpc_server_registry.py\", line 194, in _get_grpc_endpoint\n    server_process = GrpcServerProcess(\n  File \"C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\lib\\site-packages\\dagster\\_grpc\\server.py\", line 1281, in __init__\n    server_process, self.port = _open_server_process_on_dynamic_port(\n  File \"C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\lib\\site-packages\\dagster\\_grpc\\server.py\", line 1214, in _open_server_process_on_dynamic_port\n    server_process = open_server_process(\n  File \"C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\lib\\site-packages\\dagster\\_grpc\\server.py\", line 1185, in open_server_process\n    wait_for_grpc_server(server_process, client, subprocess_args, timeout=startup_timeout)\n  File \"C:\\Users\\someuser\\apps\\miniforge3\\envs\\dagster_39\\lib\\site-packages\\dagster\\_grpc\\server.py\", line 1119, in wait_for_grpc_server\n    raise Exception(```", "<@U01GTMVMGQH> - I am still at this debugging - am working behind a corporate firewall and there is no proxy set up. I have `telemetry` to `false` in the `dagster.yaml` - Does the `grpc` server need a proxy to start ?", "<@U01GTMVMGQH> - I saw some earlier chats which mentions that `set no_proxy=127.0.0.1` environment variable (however that seems to be for a separate instances of the code server &amp; dagster) - anyway I set that also if that makes any difference but still no luck.\n\nI tried this in another machine (outside the corporate wall) - `dagster dev --code-server-log-level debug -f hello-dagster.py` - this came up properly, however - `dagster dev --code-server-log-level trace -f hello-dagster.py` - gave the same `grpc` server error - `gRPC server exited with return code 1 while starting up with the command:`\n\nStill confused why changing the log-levels of the code server would have such an impact to the outcomes... Any tips here would be super helpful!", "Hi Ramkumar:\n\n&gt; Still confused why changing the log-levels of the code server would have such an impact to the outcomes... Any tips here would be super helpful!\nWhat\u2019s going on here is that `trace` is actually not a valid log level and an error is being thrown, which you should be able to find in the backtrace:\n\n```dagster._check.CheckError: Invariant failed. Description: Bad value for log level trace: permissible values are 'INFO', 'CRITICAL', 'WARN', 'WARNING', 'DEBUG', 'FATAL', 'ERROR'. ```\nThis is causing the grpc process to be instantly terminated. The `dagster dev` command actually launches two processes (the grpc and the webserver) and the webserver is continuing to launch normally, but then can\u2019t find the grpc process. No doubt we should handle this better (an invald log level or instantly failng grpc should also shut down the webserver). I will open an issue.\n\nSince this seems to be an issue with your corporate firewall, I am going to reach out to some team members more knowledgable about networking stuff.", "<@U01GTMVMGQH> - Thanks for the reply and explanation. I also tried to run the `gRPC` Code Server and Dagster UI in separate processes - but they both shutdown as soon as the Dagster UI tries to come up. There are no logs anywhere to find why this is happening. I am trying the `Dagster OSS`  - so ideally, it should be nothing with the coporate firewall -\n```1st Process - gRPC Code Server (standalone)\n(dagster_39) C:\\Users\\someuser\\dev\\pythonprojects\\dagster_poc\\hello_dagster&gt;dagster api grpc --python-file hello-dagster-cs.py --port 7001\n2023-05-09 21:14:30 +0800 - dagster - INFO - Started Dagster code server for file hello-dagster-cs.py on port 7001 in process 20528\n==&gt; This shutsdown with no logs at all, once the 2nd process tries to connect to it\n\n2nd Process - Dagster UI (pointing to a workspace.yaml)\n(dagster_39) C:\\Users\\someuse\\dev\\pythonprojects\\dagster_poc\\hello_dagster&gt;dagster dev -w workspace.yaml\n2023-05-09 21:15:32 +0800 - dagster - INFO - Launching Dagster services...\n2023-05-09 21:15:42 +0800 - dagster - INFO - Shutting down Dagster services...\n2023-05-09 21:15:42 +0800 - dagster - INFO - Dagster services shut down.\n\nworkspace.yaml\n# workspace.yaml\nload_from:  - grpc_server:      host: 127.0.0.1      port: 7001      location_name: \"my_grpc_code_server\"```", "When I bring up the 1st process (`gRPC Code Server`) - at least it comes up and does not shutdown. But as soon as the Dagster UI tries to connect to it - both the processes shutdown.\nThese are server processes - am expecting them to throw errors in some logs &amp; continue serving - why shutdown?", "I also noticed that the process is starting the `gRPC Code Server` twice and then shutsdown. Is this due to some retry twice and then shutdown?\n```(dagster_39) C:\\Users\\someuser\\dev\\pythonprojects\\dagster_poc\\hello_dagster&gt;dagster dev --code-server-log-level DEBUG -p 4001 -f hello-dagster-cs.py\n2023-05-09 21:53:42 +0800 - dagster - INFO - Launching Dagster services...\n2023-05-09 21:53:57 +0800 - dagster - INFO - Started Dagster code server for file hello-dagster-cs.py on port 63598 in process 11680\n2023-05-09 21:54:00 +0800 - dagster - INFO - Started Dagster code server for file hello-dagster-cs.py on port 63604 in process 19640\n2023-05-09 21:54:02 +0800 - dagster - INFO - Shutting down Dagster services...\n2023-05-09 21:54:02 +0800 - dagster - INFO - Dagster services shut down.```", "To be clear, this is happening only in your corporate network environment right? The exact same commands (with the same dagster code) works in a different environment?", "Yes, this happens in a Windows machine (inside the corporate network). I am amidst trying the same steps in a Linux environment to see if the outcome is different - am hoping to see some logs under the $DAGSTER_HOME - not sure why i cannot see any logs (due to Windows?)", "<@U01GTMVMGQH> - The exact setup works in our corporate Linux environment!\n\nSo what I will do is to test the same in a Windows environment outside our corporate network - to isolate if this a generic Windows issue or a something more specific witn the Corporate Widows env. I am assuming that the `gRPC` server is tested in Windows also...\nThanks again for the help.", "&gt; So what I will do is to test the same in a Windows environment outside our corporate network - to isolate if this a generic Windows issue or a something more specific witn the Corporate Widows env.\nOK, great. We do test and support windows but I believe that 100% of our developers (and the majority of our users) use unix, so that\u2019s where the best testing and support is."], "is_dagster_empl": [false, false, false, true, false, false, false, false, false, true, false, false, false, true, false, false, true], "gpt4_replies_target": "The issue you're experiencing seems to be related to your corporate firewall settings. The `trace` log level is not valid and is causing the gRPC process to terminate instantly. The `dagster dev` command launches two processes (the gRPC and the webserver) and the webserver is continuing to launch normally, but then can\u2019t find the gRPC process. It's recommended to test the same setup in a different environment to isolate the issue.", "dagster_empl_first_target": "Hi Ramkumar, can you provide more of the backtrace shown when you use trace?\n\n&gt; However, when I try this - dagster dev --code-server-log-level trace -p 6050 -f hello_dagster.py - the Dasgster at least comes up! But the code does not execute with an error on the GRPC-Server.\nWhat did you mean here by \u201cthe Dagster at least comes up\u201d?", "dagster_empl_last_target": "&gt; So what I will do is to test the same in a Windows environment outside our corporate network - to isolate if this a generic Windows issue or a something more specific witn the Corporate Widows env.\nOK, great. We do test and support windows but I believe that 100% of our developers (and the majority of our users) use unix, so that\u2019s where the best testing and support is."}, {"question": "Does someone know if the dagster-docker <https://docs.dagster.io/_apidocs/libraries/dagster-docker#dagster_docker.DockerRunLauncher|DockerRunLauncher> is the same as Airflow's <https://airflow.apache.org/docs/apache-airflow-providers-docker/stable/_api/airflow/providers/docker/operators/docker/index.html#airflow.providers.docker.operators.docker.DockerOperator|DockerOperator>? Is there a Dagster native way to launch a Docker container with a custom image as an Op? Meaning the code to be executed is packaged in the image and not written in the Op? I have a use-case in which I need to temporarily spin up an API so that data can be pushed to it and then spin it back down and I was thinking to launch a separate Docker container with the API inside it.", "replies": ["From the <https://docs.dagster.io/_modules/dagster_docker/docker_run_launcher#DockerRunLauncher|code> and the example here: <https://docs.dagster.io/_apidocs/libraries/dagster-docker#dagster_docker.docker_executor>, I would say yes. (but I am new to dagster)\n\n```@job(executor_def=docker_executor)\ndef docker_job():\n    pass```", "Thanks :slightly_smiling_face: I will try it out. I am using the K8sRunLauncher but would just like to deploy a specific op in a separate pod (and container)", "Hey Daniel - If you\u2019re running in k8s I think the Ops section here will help: <https://docs.dagster.io/_apidocs/libraries/dagster-k8s#ops|https://docs.dagster.io/_apidocs/libraries/dagster-k8s#ops>", "<@U016C4E5CP8> This is exactly what I want, thank you! :slightly_smiling_face:"], "is_dagster_empl": [false, false, true, false], "gpt4_replies_target": "The Dagster DockerRunLauncher is similar to Airflow's DockerOperator and can be used to launch a Docker container with a custom image as an Op. If you're running in Kubernetes, you can use the Ops section in the Dagster Kubernetes library to deploy a specific op in a separate pod and container.", "dagster_empl_first_target": "Hey Daniel - If you\u2019re running in k8s I think the Ops section here will help: <https://docs.dagster.io/_apidocs/libraries/dagster-k8s#ops|https://docs.dagster.io/_apidocs/libraries/dagster-k8s#ops>", "dagster_empl_last_target": "Hey Daniel - If you\u2019re running in k8s I think the Ops section here will help: <https://docs.dagster.io/_apidocs/libraries/dagster-k8s#ops|https://docs.dagster.io/_apidocs/libraries/dagster-k8s#ops>"}, {"question": "The Dagster code server is shutting down and starting again every 20 - 30 seconds, is this okay or may be a problem?", "replies": ["Anything in the logs that stand out?", "", "It's expected for the daemon to periodically shut down and restart gRPC servers, yes", "I would expect it to be more like every 60 seconds", "Oh, then it's okay. Thanks Daniel and Dusty!"], "is_dagster_empl": [false, false, true, true, false], "gpt4_replies_target": "It's normal for the Dagster daemon to periodically shut down and restart gRPC servers, although it typically happens every 60 seconds. However, you should check the logs for any unusual activities.", "dagster_empl_first_target": "It's expected for the daemon to periodically shut down and restart gRPC servers, yes", "dagster_empl_last_target": "I would expect it to be more like every 60 seconds"}, {"question": "Hi all\n\nI'm trying to mock out a context for an op I'm testing using `build_op_context`\n\nHow do I specify tags in this function? Thanks!", "replies": ["the `tags` come from the `op` being executed, if you want to add some that are not on the definition you can use the `tag` function before invoking\n\n`my_op.tag({'foo': 'bar'})(context=built_context)`", "Perfect, thanks alex!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "To specify tags in the `build_op_context` function, you can use the `tag` function before invoking `my_op.tag({'foo': 'bar'})(context=built_context)`. The `tags` come from the `op` being executed.", "dagster_empl_first_target": "the `tags` come from the `op` being executed, if you want to add some that are not on the definition you can use the `tag` function before invoking\n\n`my_op.tag({'foo': 'bar'})(context=built_context)`", "dagster_empl_last_target": "the `tags` come from the `op` being executed, if you want to add some that are not on the definition you can use the `tag` function before invoking\n\n`my_op.tag({'foo': 'bar'})(context=built_context)`"}, {"question": "Is it possible to create multiple job sensors?", "replies": ["Yes. Different sensors can trigger the same job, and different sensors can also monitor the same job.", "I mean, I have 3 jobs and I want to trigger all of then using the same sensor.", "That's possible, `@sensor` has a `jobs` param that allows you to select multiple jobs that may be triggered from the same sensor", "you'll just need to yield a `RunRequest(job_name=...)` for each job", "Thanks, exactly what I needed."], "is_dagster_empl": [true, false, true, true, false], "gpt4_replies_target": "Yes, it is possible to create multiple job sensors in dagster and even a single sensor can trigger multiple jobs. You just need to yield a `RunRequest(job_name=...)` for each job.", "dagster_empl_first_target": "Yes. Different sensors can trigger the same job, and different sensors can also monitor the same job.", "dagster_empl_last_target": "you'll just need to yield a `RunRequest(job_name=...)` for each job"}, {"question": "Any clue on when this PR will be merged? <https://github.com/dagster-io/dagster/pull/11057>", "replies": ["cc <@U0291UAGGCB>"], "is_dagster_empl": [true], "gpt4_replies_target": "I'm sorry, but as a Dagster expert, I don't have the specific timeline for when the mentioned PR will be merged. You may want to reach out to the user tagged in the reply for more information.", "dagster_empl_first_target": "cc <@U0291UAGGCB>", "dagster_empl_last_target": "cc <@U0291UAGGCB>"}, {"question": "Is there any documentation or examples of the GRPC server being extended to accomodate additional endpoints? MY company has containerized our code server, but we don't want to put another server in the same container.", "replies": ["Hi Anthony - do you mean you want the same running container to be both running the dagster grpc server and some other server at the same time? Or do you want the same image to be able to support other entry points besides just the Dagster gRPC server?", "The latter is I think just a matter of overriding the entrypoint of the image which is easy to do in a system like docker compose (or changing the arguments that you pass to docker run). The former is a bit trickier and I don't think i've seen a request for it before - there are some general docker docs for it here though that aren't dagster specific: <https://docs.docker.com/config/containers/multi-service_container/>", "Thanks for the reply! Our hope is the former, although I think the latter is not a bad alternative.", "<@U016C4E5CP8> For clarification, the GRPC server points at a local repo, but we want to modify code in that repo over the network, after the GRPC server has been started. How can we extend the GRPC server with additional endpoints? For example, we want to modify a config variable in a yaml file remotely. It would be nice to not have to create an additional server on the same machine.", "We don't currently have a built-in way to extend the gRPC server with additional endpoints - is that something you'd be willing to file a feature request for?", "I could do that. Is that something others might want?", "it's not a request i've seen before - I'd be tempted to run two separate gRPC servers instead", "Hm that makes sense. Thanks for the insight. Why does dagster use GRPC for communication anyways?", "they let us run user code in a separate environment from system code - so we can avoid dependency issues and keep different code locations isolated from messing each other up and messing dagit up"], "is_dagster_empl": [true, true, false, false, true, false, true, false, true], "gpt4_replies_target": "Dagster currently does not have a built-in way to extend the gRPC server with additional endpoints. You might consider running two separate gRPC servers instead. Dagster uses gRPC for communication to run user code in a separate environment from system code, avoiding dependency issues and keeping different code locations isolated.", "dagster_empl_first_target": "Hi Anthony - do you mean you want the same running container to be both running the dagster grpc server and some other server at the same time? Or do you want the same image to be able to support other entry points besides just the Dagster gRPC server?", "dagster_empl_last_target": "they let us run user code in a separate environment from system code - so we can avoid dependency issues and keep different code locations isolated from messing each other up and messing dagit up"}, {"question": "Started to learn dagster from this great <https://github.com/petehunt/dagster-poor-mans-data-lake|blog> of <@U031N5S93NG> and <@UCCLPK5G8>\n\nit uses a lot of pip install commands in a script, it seems like the order is important. In this script pip throws a lot of resolution errors, but dagit seem to work fine anyways.\nSince pip dependency resolution is  very simplistic and unreliable I\u2019ve tried to do this tutorial with poetry, but then dagit won\u2019t work due to some gql issues <https://dagster.slack.com/archives/C01U954MEER/p1671637441502839?thread_ts=1671635292.853509&amp;cid=C01U954MEER|mentioned> here before.  I\u2019ve managed to build the project with dagster 1.0.1, but in newer versions some dependency issues arise.\n\nHas somebody managed to have a simple \u201chello-world\u201d-like dagster project with poetry and dagster 1.1.6+ ?", "replies": ["Hi Anton, this doesn't answer every part of your question, but a gql pin fix is going live in the next 24 hours"], "is_dagster_empl": [true], "gpt4_replies_target": "A fix for the gql issue you're experiencing with dagster and poetry is expected to be released in the next 24 hours.", "dagster_empl_first_target": "Hi Anton, this doesn't answer every part of your question, but a gql pin fix is going live in the next 24 hours", "dagster_empl_last_target": "Hi Anton, this doesn't answer every part of your question, but a gql pin fix is going live in the next 24 hours"}, {"question": "I'm working on a POC for Dagster and I keep running into the following error:\n```% helm install dagster dagster/dagster               \nError: INSTALLATION FAILED: values don't meet the specifications of the schema(s) in the following chart(s):\ndagster:\ninvalid character '&lt;' looking for beginning of valuedagster-user-deployments:\ninvalid character '&lt;' looking for beginning of value```\nI've tried using a local copy of `values.yaml` . I've also tried different versions of the helm chart, but I get the same error. My best guess is that it's reaching out and getting a bad http response. Does anybody know what's causing this?", "replies": ["I've been following this: <https://docs.dagster.io/deployment/guides/kubernetes/deploying-with-helm>\n\nThe only part I didn't set up was S3. It doesn't seem like that would give this particular error at `helm install`, but please let me know if I'm wrong.", "(Actually, looks like the layout of that guide has changed since the last time I tried. I'll have to see if any of the content has changed...)", "Hi Mandi - is it possible to share a `values.yaml` file that reproduces the error that you're seeing?", "Yep.", "thanks - and what's the helm command that you're running to use it?", "oh, you had it above - it's the helm install, looks like", "Does the same error happen when you run this command?\n\n```helm upgrade --install dagster dagster/dagster -f values.yaml```\n(in the folder where the values.yaml is)", "It does.", "I'm going through the updated installation doc now...", "That's very strange, what version of helm do you have installed?", "I assume you did the step where you add the helm repo:\n```helm repo add dagster <https://dagster-io.github.io/helm>```\n", "v3.10.3", "dagster helm version: 1.1.7", "A coworker tried it on our production cluster and got the same result. :shrug:", "Have you been able to install any other helm charts in this environment? It seems like it might be having trouble finding the helm schema to check", "that \"invalid character '&lt;' looking for beginning of value\" might be the start of some kind of error message when it tries to fetch the helm schema", "This is the only helm I've tried locally. I know we've installed other charts on our production cluster though.", "If you know a good chart to test with I'm happy to try it.", "Otherwise I'll poke around for one.", "I'm pretty motivated to solve this. :laughing:", "What if you run \"helm template dagster dagster/dagster -f values.yaml\" instead? same error?", "Same error.", "I'm able to install the bitnami/spark helm chart.", "I know you said its happening in prod too, but where is your local k8s cluster running?", "docker-desktop?", "```(dagster_venv) helm_test % helm install myspark bitnami/spark\nNAME: myspark\nLAST DEPLOYED: Fri Jan  6 10:45:14 2023\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: spark\nCHART VERSION: 6.3.13\nAPP VERSION: 3.3.1```", "Yes, Docker desktop on a Mac.", "and what k8s version?", "trying it myself on docker desktop to see if i can reproduce with that values.yaml file", "You'll have to forgive my ignorance--I'm new to Docker and Kubernetes. `kubectl version` gives me this:\n```Client Version: v1.25.2\nKustomize Version: v4.5.7\nServer Version: v1.25.2```", "no problem, that works", "`docker version` gives me this:\n```Client:\n Cloud integration: v1.0.29\n Version:           20.10.21\n API version:       1.41\n Go version:        go1.18.7\n Git commit:        baeda1f\n Built:             Tue Oct 25 18:01:18 2022\n OS/Arch:           darwin/amd64\n Context:           default\n Experimental:      true\n\nServer: Docker Desktop 4.15.0 (93002)\n Engine:\n  Version:          20.10.21\n  API version:      1.41 (minimum version 1.12)\n  Go version:       go1.18.7\n  Git commit:       3056208\n  Built:            Tue Oct 25 18:00:19 2022\n  OS/Arch:          linux/amd64\n  Experimental:     false\n containerd:\n  Version:          1.6.10\n  GitCommit:        770bd0108c32f3fb5c73ae1264f7e503fe7b2661\n runc:\n  Version:          1.1.4\n  GitCommit:        v1.1.4-0-g5fd4c4d\n docker-init:\n  Version:          0.19.0\n  GitCommit:        de40ad0```", "I have to step away for a bit, but running through the current deploy guide in a new directory gives me the same result.", "very odd... this is what I get when I run `helm template dagster/dagster on your values.yaml file`: <https://gist.github.com/gibsondan/3fbd922d3571c7b0695d0787285352f3>", "`helm template` doesn't even need a cluster to be running, so I don't think its related to your kubernetes cluster", "I might try setting it up on my personal computer when I get back. Maybe it's something to do with the vpn or our security setup.", "the other thing you could try is some other chart that uses a schema for validation (I know bitnami/rabbitmq does for example)", "that would rule out some general issue related to helm + schema validation in your environment", "bitnami/rabbitmq seems to install ok:\n``` % helm install myrabbit bitnami/rabbitmq\nNAME: myrabbit\nLAST DEPLOYED: Fri Jan  6 12:40:13 2023\nNAMESPACE: default\nSTATUS: deployed\nREVISION: 1\nTEST SUITE: None\nNOTES:\nCHART NAME: rabbitmq\nCHART VERSION: 11.3.1\nAPP VERSION: 3.11.6** Please be patient while the chart is being deployed **\n\nCredentials:\n    echo \"Username      : user\"\n    echo \"Password      : $(kubectl get secret --namespace default myrabbit-rabbitmq -o jsonpath=\"{.data.rabbitmq-password}\" | base64 -d)\"\n    echo \"ErLang Cookie : $(kubectl get secret --namespace default myrabbit-rabbitmq -o jsonpath=\"{.data.rabbitmq-erlang-cookie}\" | base64 -d)\"\n\nNote that the credentials are saved in persistent volume claims and will not be changed upon upgrade or reinstallation unless the persistent volume claim has been deleted. If this is not the first installation of this chart, the credentials may not be valid.\nThis is applicable when no passwords are set and therefore the random password is autogenerated. In case of using a fixed password, you should specify it when upgrading.\nMore information about the credentials may be found at <https://docs.bitnami.com/general/how-to/troubleshoot-helm-chart-issues/#credential-errors-while-upgrading-chart-releases>.\n\nRabbitMQ can be accessed within the cluster on port 5672 at myrabbit-rabbitmq.default.svc.cluster.local\n\nTo access for outside the cluster, perform the following steps:\n\nTo Access the RabbitMQ AMQP port:\n\n    echo \"URL : <amqp://127.0.0.1:5672/>\"\n    kubectl port-forward --namespace default svc/myrabbit-rabbitmq 5672:5672\n\nTo Access the RabbitMQ Management interface:\n\n    echo \"URL : <http://127.0.0.1:15672/>\"\n    kubectl port-forward --namespace default svc/myrabbit-rabbitmq 15672:15672```", "I'm working on setting up my other computer between meetings. I'll update when I have more info. Thanks for all the help so far!", "Yeah... it works on my private machine.", "We found the blocker and got it working! :tada: Thanks for all the help!", "nice! curious what it ended up being?", "A TLD block. So the process was getting an html error message when it tried to reach out to <http://kubernetesjsonschema.dev|kubernetesjsonschema.dev>", "Ahhh I see"], "is_dagster_empl": [false, false, true, false, true, true, true, false, false, true, true, false, false, false, true, true, false, false, false, false, true, false, false, true, true, false, false, true, true, false, true, false, false, true, true, false, true, true, false, false, false, false, true, false, true], "gpt4_replies_target": "The error was caused by a TLD block which was preventing the process from reaching out to kubernetesjsonschema.dev.", "dagster_empl_first_target": "Hi Mandi - is it possible to share a `values.yaml` file that reproduces the error that you're seeing?", "dagster_empl_last_target": "Ahhh I see"}, {"question": "Hey folks, I have a question re. dagster-daemon.\nKindly redirect me to the relevant channel.\n\nTo share with you a short context, we are running dagster pipelines (using python) on EC2 instance and have scheduled a couple of jobs every 2 hrs. Now the issue is that the jobs are not getting triggered as expected.\nDoes anyone know what could be the root causes?\nFor further clarification, please feel free to connect with me.\n\nP.S. As per the official documentation, \u201cdagster-daemon\u201d is responsible for the scheduler, backfill and sensors and we have \u201cdagster-daemon\u201d running on the EC2 instance.\n\nThanks\ncc <@U04CP7DSMLH>", "replies": ["Hi Mayank - there's a set of troubleshooting steps here for schedules not running: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules#troubleshooting> - can you try these and let us know if they help? If they don't, more information about what exactly the symptoms are that you're seeing and what you were able to rule out by following those steps would be helpful", "Hello <@U016C4E5CP8>, thanks for the reference link.\n*Updates:* I went through all the troubleshooting steps mentioned in the above link and I am still facing the following issues:\n\u2022 Daemons are not running (Please find the below attachment).\n\u2022 Schedule Message: `No new tick times to evaluate for configurable_xxxxx_schedule` \nCould you please assist me?\nThanks", "", "That sounds to me like your dagit and dagster-daemon aren't using the same DAGSTER_HOME - did you set that environment variable? There are some examples of how to run dagit and the daemon here: <https://docs.dagster.io/deployment/guides/service#running-dagster-as-a-service>", "Hey <@U016C4E5CP8>, thanks for that pointer.\nSomehow my `DAGSTER_HOME` variable got reset and I was not aware of it.", "Closing this issue.\n```The solution of the problem was the  DAGSTER_HOME variable was not same because of which Daemons were not running and the reason schedule jobs were not getting triggered. ```\ncc <@U04CP7DSMLH>"], "is_dagster_empl": [true, false, false, true, false, false], "gpt4_replies_target": "The issue of jobs not getting triggered was due to the `DAGSTER_HOME` variable being reset, which caused the daemons to stop running. After resetting the `DAGSTER_HOME` variable, the issue was resolved.", "dagster_empl_first_target": "Hi Mayank - there's a set of troubleshooting steps here for schedules not running: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules#troubleshooting> - can you try these and let us know if they help? If they don't, more information about what exactly the symptoms are that you're seeing and what you were able to rule out by following those steps would be helpful", "dagster_empl_last_target": "That sounds to me like your dagit and dagster-daemon aren't using the same DAGSTER_HOME - did you set that environment variable? There are some examples of how to run dagit and the daemon here: <https://docs.dagster.io/deployment/guides/service#running-dagster-as-a-service>"}, {"question": "Hi, I'm working on an op that processes a dataframe and make several API calls for each row. My initial implementation was to use DynamicOutput to manage the downstream ops. However, the APIs have rate limits and managing a few hundred thousand of parallel ops at one time seems quite expensive. Any other ideas on how I can approach this?", "replies": ["Hi YH. You can limit the number of parallel ops that are running at a time by specifying an op concurrency limit on the default multiprocess executor: <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#default-job-executor>", "Yes thanks <@U028M11QNDD> I found that after reading some other threads. On a side note, I might also redesign the way I chunk the data, so that each parallel op handles more than one row of data. I can then use pyrate to rate limit the subsequent calls. Since I would be using dagster-serverless, I think there would already be a natural limit based on cpu cores too "], "is_dagster_empl": [true, false], "gpt4_replies_target": "You can limit the number of parallel ops by specifying an op concurrency limit on the default multiprocess executor, and consider redesigning your data chunking so each parallel op handles more than one row of data, using pyrate to rate limit the subsequent calls.", "dagster_empl_first_target": "Hi YH. You can limit the number of parallel ops that are running at a time by specifying an op concurrency limit on the default multiprocess executor: <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#default-job-executor>", "dagster_empl_last_target": "Hi YH. You can limit the number of parallel ops that are running at a time by specifying an op concurrency limit on the default multiprocess executor: <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#default-job-executor>"}, {"question": "greetings. we've encountered the same problem as described <https://github.com/dagster-io/dagster/issues/7447|here> with Istio - because services created by cloud-agent use the wrong port name (`http` instead of `grpc` ) cloud-agent times out for grpc healthchecks. do you know if there's any way around to fix it? to specify service port name in dagster-cloud?", "replies": ["Hey, we\u2019re not running dagster-cloud, but we\u2019re running dagster with istio. One of my team mates updated that value in the chart last year (see the version on `master`: <https://github.com/dagster-io/dagster/blob/master/helm/dagster/charts/dagster-user-deployments/templates/service-user.yaml|https://github.com/dagster-io/dagster/blob/master/helm/dagster/charts/dagster-user-deployments/templates/service-user.yaml>). Are you using an extremely old helm chart version?", "thanks for the answer! the issue is that we're using `dagster-cloud-agent` chart and not the `dagster-user-deployments` chart, and I see no way how we can configure it for our case.", "Ah - it might just need the same update then? Feel free to link to this one in a PR", "the thing is I'm not sure where to open a PR - i don't see a clear reference on how/where cloud-agent creates/defines these services.", "Hopefully someone from the dagster team knows!", "Sent out a fix to merge this in to the cloud agent helm chart as well", "thank you for quick resolution! :bow:"], "is_dagster_empl": [false, false, false, false, false, true, false], "gpt4_replies_target": "The issue with the cloud-agent using the wrong port name in Istio can be fixed by updating the value in the chart, similar to what was done in the `dagster-user-deployments` chart. A fix has been sent to merge this into the cloud agent helm chart as well.", "dagster_empl_first_target": "Sent out a fix to merge this in to the cloud agent helm chart as well", "dagster_empl_last_target": "Sent out a fix to merge this in to the cloud agent helm chart as well"}, {"question": "Hi folks, I have a question about Dagster opensource deployment on AWS. I have defined an ECS service with three tasks, one for Dagit, one for dagster-daemon and one for user code. I'm able to run the UI and I see the user code there too, however it seems that Dagit can't see the daemon process (I see \"daemons not running\" on an exclamation mark tooltip in deployment &gt; daemons). When I check log output from each of the tasks, there seem to be no errors etc. So I guess my question is: How does the UI try to communicate with the daemon? I'm running Dagit with a `-w workspace.yaml` argument which allows it to find the user code container. Is there a similar setting somewhere for the daemon? Thanks for any advice!", "replies": ["Hi Jan - the UI communicates with the daemon by both reading from and writing to a shared Dagster database (often in Postgres) - you'll want to make sure that your tasks all have a dagster.yaml that is configured to point to that shared storage engine", "Like in our example here: <https://docs.dagster.io/deployment/guides/aws#deploying-in-ecs>", "Oh thank you Daniel! That makes sense, I didn't want to bother setting up a DB initially so I though it would work if the system fell back on the default SQLite DB but then it isn't shared between containers. Yeah, I'll set up a proper shared persistence layer, thanks again!"], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "The UI communicates with the daemon by both reading from and writing to a shared Dagster database, often in Postgres, so you need to ensure that your tasks all have a dagster.yaml that is configured to point to that shared storage engine.", "dagster_empl_first_target": "Hi Jan - the UI communicates with the daemon by both reading from and writing to a shared Dagster database (often in Postgres) - you'll want to make sure that your tasks all have a dagster.yaml that is configured to point to that shared storage engine", "dagster_empl_last_target": "Like in our example here: <https://docs.dagster.io/deployment/guides/aws#deploying-in-ecs>"}, {"question": "Hello community!\n\nHelp me please. How do I remove colors from dagit and dagster daemon logs?\n\nThese ones:\n```dagster-daemon    | 2023-05-11 12:20:22 +0000 - dagster.daemon.SensorDaemon - INFO - Not checking for any runs since no sensors have been started.\ndagster-daemon    | 2023-05-11 12:20:22 +0000 - dagster.daemon - INFO - Instance is configured with the following daemons: ['BackfillDaemon', 'SchedulerDaemon', 'SensorDaemon']```", "replies": ["Hey Grigorii, this isn\u2019t well supported right now. There\u2019s a discussion on how to improve this experience for users: <https://github.com/dagster-io/dagster/discussions/12495>. Please let us know your feedback there!", "Thank you for the answer!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, removing colors from dagit and dagster daemon logs is not well supported, but there is an ongoing discussion on how to improve this experience.", "dagster_empl_first_target": "Hey Grigorii, this isn\u2019t well supported right now. There\u2019s a discussion on how to improve this experience for users: <https://github.com/dagster-io/dagster/discussions/12495>. Please let us know your feedback there!", "dagster_empl_last_target": "Hey Grigorii, this isn\u2019t well supported right now. There\u2019s a discussion on how to improve this experience for users: <https://github.com/dagster-io/dagster/discussions/12495>. Please let us know your feedback there!"}, {"question": "I have some confusion about the usage of `define_asset_job` and `DynamicPartitionsDefinition`. In my understanding, `define_asset_job` is used to generate a job that materializes assets, while `DynamicPartitionsDefinition` allows for dynamically defining partitions. After reading the documentation, I only found use cases of `DynamicPartitionsDefinition` being updated in a sensor using `SensorResult(dynamic_partitions_requests=[])`. What should I do if I don\u2019t want to use a sensor? For example, I want to define a job that, when executed, will first redefine the partitions in `DynamicPartitionsDefinition` and then materialize the partition asset using these partitions.\nHere is the use case I\u2019ve seen:\n```fruits = DynamicPartitionsDefinition(name=\"fruits\")\n\nmy_job = define_asset_job(\n    \"my_job\", selection=AssetSelection.all(), partitions_def=fruits\n)\n\n@sensor(job=my_job)\ndef my_sensor(context):\n    partitions = fruits.get_partition_keys(dynamic_partitions_store=context.instance)```\nIn my actual requirements, a MongoDB collection has 100 million records. I need to first read its count, then read the data in groups of 500,000, and save each group as a Parquet file as a partitioned asset. Is `DynamicPartitionsDefinition` the best practice here? What suggestions do you have?", "replies": ["Hey, Horatio you should be able to achieve this no problem.\n\nI\u2019m not sure if there\u2019s a better way using the `build_add_request` method, but this little example should achieve what you\u2019re looking for:\n\n```from dagster import Definitions, job, op, asset, DynamicPartitionsDefinition, OpExecutionContext\n\ndynamic_partitions = DynamicPartitionsDefinition(name=\"dynamic_partition\")\n\n\n@asset\ndef get_mongodb_data():\n    return [str(i) for i in range(10)]\n\n\n@op\ndef add_partitions(context: OpExecutionContext, get_mongodb_data):\n    new_partitions = [\n        str(i) for i in get_mongodb_data\n        if not context.instance.has_dynamic_partition(\n            dynamic_partitions.name, str(i)\n        )\n    ]\n    context.instance.add_dynamic_partitions(dynamic_partitions.name, new_partitions)\n\n\n@job\ndef add_mongo_partitions():\n    data = get_mongodb_data()\n    add_partitions(data)\n\n\n@asset(partitions_def=dynamic_partitions)\ndef foo():\n    return 1\n\n\ndefs = Definitions(\n    assets=[foo, get_mongodb_data],\n    jobs=[add_mongo_partitions],\n)```", "You obviously don\u2019t need to cast it to `str` twice but you should hopefully get the idea :sweat_smile:", "Thanks <@U04CEEY7QCU> for the example.\n\nHoratio, the key idea is that you can use an op and `context.instance.add_dynamic_partitions` to do this, as Guy illustrated.", "<@U04CEEY7QCU> <@U01GTMVMGQH> Thank you for your idea, it has been very helpful to me. The `add_dynamic_partitions` and `delete_dynamic_partition` methods are very useful. Perhaps it would be a good idea to mention this API on the <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions> page."], "is_dagster_empl": [false, false, true, false], "gpt4_replies_target": "You can use an op and `context.instance.add_dynamic_partitions` to dynamically define partitions in a job without using a sensor. This allows you to read data in groups and save each group as a partitioned asset.", "dagster_empl_first_target": "Thanks <@U04CEEY7QCU> for the example.\n\nHoratio, the key idea is that you can use an op and `context.instance.add_dynamic_partitions` to do this, as Guy illustrated.", "dagster_empl_last_target": "Thanks <@U04CEEY7QCU> for the example.\n\nHoratio, the key idea is that you can use an op and `context.instance.add_dynamic_partitions` to do this, as Guy illustrated."}, {"question": "is there a way to partition partitions? I read about the multipartitions, but they're not subdivisions, they are a cross product of two independent methods of tagging the data (like date and some other label)", "replies": [":plus1:  - I don\u2019t think there is a native way, I\u2019d also like to see this. In the meantime I use static/dynamic partitions where the partition key is like `parent/child` split by a sensible delimiter", "yep, was thinking of doing the same", "the problem is that I might end up breaking dagit's UI with the amount of subdivisions we want to track :grimacing:", "for instance, we might have hundreds of datasets in a project and each of those might have thousands of entities we want to track individually. It quickly adds up", "Hi Fabio, To be clear, is the primary difference between what you\u2019re asking for and multipartitions that you want the freedom to have a _different_ second axis for each element in the first axis?", "yes, that's exactly it <@U01GTMVMGQH>\nif we went for the cross product, we would have many combinations that aren't valid. So we would like the second axis to depend on the first axis", "but most important of all is the way we visually track these partitions. It is a hard one to design", "OK. Yeah I definitely see the appeal here, but I strongly suspect we\u2019re not going to implement this in a first-class way anytime soon, simply because there\u2019s al ready a ton of work going in other areas of partitions. I have to recommend that for now you follow <@U04UURZ79UJ>\u2019s advice and \u201csimulate\u201d this using a delimiter in the partition key.", "That said, I recommend opening a feature request on GH to inject the idea into the conversation", "When you do, <@U04S2S4BSP9>, link it here and I\u2019ll throw my :+1: on it :grin:", "<https://github.com/dagster-io/dagster/issues/14228>"], "is_dagster_empl": [false, false, false, false, true, false, false, true, true, false, false], "gpt4_replies_target": "Currently, there isn't a native way to partition partitions in Dagster. However, you can simulate this by using a delimiter in the partition key. It's also recommended to open a feature request on GitHub to bring this idea into the conversation.", "dagster_empl_first_target": "Hi Fabio, To be clear, is the primary difference between what you\u2019re asking for and multipartitions that you want the freedom to have a _different_ second axis for each element in the first axis?", "dagster_empl_last_target": "That said, I recommend opening a feature request on GH to inject the idea into the conversation"}, {"question": "hey! the `configure_at_launch` method for resources should partially parametrize the resource configuration? Because if thats the case, I am not being able to do it.\n\nExample in the comments", "replies": ["<https://github.com/dagster-io/dagster/issues/14144>", "Thanks for bringing this up and creating the issue, we can continue any discussion on GH."], "is_dagster_empl": [false, true], "gpt4_replies_target": "Yes, the `configure_at_launch` method for resources should partially parametrize the resource configuration. If you're having trouble, you can refer to the discussion on this GitHub issue: https://github.com/dagster-io/dagster/issues/14144.", "dagster_empl_first_target": "Thanks for bringing this up and creating the issue, we can continue any discussion on GH.", "dagster_empl_last_target": "Thanks for bringing this up and creating the issue, we can continue any discussion on GH."}, {"question": "Hello Dagster people!\nI'm in the process of moving our code locations from development, to a \"staging\" service. The staging environment is largely created using the guide provided in <https://docs.dagster.io/deployment/guides/docker>. It's been pretty smooth sailing so far, however, I have suddenly been facing the issue of the launched docker container exiting before finishing the job (with no feedback in dagit). Note: Another code location works fine with zero issues.\n\nThe error as described by the logs of the exited docker container:\n\n```Usage: dagster api execute_run [OPTIONS] INPUT_JSON\nTry 'dagster api execute_run -h' for help.\n\nError: Got unexpected extra arguments (\"ExecuteRunArgs\", \"instance_ref\": {\"__class__\": \"InstanceRef\", \"compute_logs_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"LocalComputeLogManager\", \"config_yaml\": \"base_dir: /opt/dagster/dagster_home/storage\\n\", \"module_name\": \"dagster.core.storage.local_compute_log_manager\"}, \"custom_instance_class_data\": null, \"event_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"PostgresEventLogStorage\", \"config_yaml\": \"postgres_db:\\n db_name:\\n env: DAGSTER_POSTGRES_DB\\n hostname:\\n env: DAGSTER_POSTGRES_HOST\\n password:\\n env: DAGSTER_POSTGRES_PASSWORD\\n port: 5432\\n username:\\n env: DAGSTER_POSTGRES_USER\\n\", \"module_name\": \"dagster_postgres\"}, \"local_artifact_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"LocalArtifactStorage\", \"config_yaml\": \"base_dir: /ai4xray_2/orchestration/data_storage\\n\", \"module_name\": \"dagster.core.storage.root\"}, \"run_coordinator_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"QueuedRunCoordinator\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster.core.run_coordinator\"}, \"run_launcher_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DockerRunLauncher\", \"config_yaml\": \"container_kwargs:\\n volumes:\\n - /var/run/docker.sock:/var/run/docker.sock\\n - /ai4xray_2/orchestration/data_storage:/ai4xray_2/orchestration/data_storage\\n - /ai4xray_1/data_vault:/ai4xray_1/data_vault\\nenv_vars:\\n- DAGSTER_POSTGRES_HOST\\n- DAGSTER_POSTGRES_USER\\n- DAGSTER_POSTGRES_PASSWORD\\n- DAGSTER_POSTGRES_DB\\n- POSTGRES_DAGSTER_PASSWORD\\n- DATA_ROOT\\n- DAGSTER_MODE\\n- BASE_SCHEMA\\nnetwork: postgres_network\\n\", \"module_name\": \"dagster_docker\"}, \"run_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"PostgresRunStorage\", \"config_yaml\": \"postgres_db:\\n db_name:\\n env: DAGSTER_POSTGRES_DB\\n hostname:\\n env: DAGSTER_POSTGRES_HOST\\n password:\\n env: DAGSTER_POSTGRES_PASSWORD\\n port: 5432\\n username:\\n env: DAGSTER_POSTGRES_USER\\n\", \"module_name\": \"dagster_postgres\"}, \"schedule_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"PostgresScheduleStorage\", \"config_yaml\": \"postgres_db:\\n db_name:\\n env: DAGSTER_POSTGRES_DB\\n hostname:\\n env: DAGSTER_POSTGRES_HOST\\n password:\\n env: DAGSTER_POSTGRES_PASSWORD\\n port: 5432\\n username:\\n env: DAGSTER_POSTGRES_USER\\n\", \"module_name\": \"dagster_postgres\"}, \"scheduler_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DagsterDaemonScheduler\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster.core.scheduler\"}, \"secrets_loader_data\": null, \"settings\": {}, \"storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DagsterPostgresStorage\", \"config_yaml\": \"postgres_db:\\n db_name:\\n env: DAGSTER_POSTGRES_DB\\n hostname:\\n env: DAGSTER_POSTGRES_HOST\\n password:\\n env: DAGSTER_POSTGRES_PASSWORD\\n port: 5432\\n username:\\n env: DAGSTER_POSTGRES_USER\\n\", \"module_name\": \"dagster_postgres\"}}, \"pipeline_origin\": {\"__class__\": \"PipelinePythonOrigin\", \"pipeline_name\": \"take_snapshot\", \"repository_origin\": {\"__class__\": \"RepositoryPythonOrigin\", \"code_pointer\": {\"__class__\": \"ModuleCodePointer\", \"fn_name\": \"definitions\", \"module\": \"annotator.orchestration\", \"working_directory\": \"/opt/pysetup\"}, \"container_context\": {}, \"container_image\": \"annotator_backend_image\", \"entry_point\": [\"dagster\"], \"executable_path\": \"/opt/pysetup/.venv/bin/python\"}}, \"pipeline_run_id\": \"13b11cf8-b134-4e68-a52b-72781de304be\", \"set_exit_code_on_failure\": null})```\nThe command looks a bit messy, but since it's a `Got unexpected extra arguments` I'm guessing it could be a change of API? However I cannot find anything related in the changelog. Can anyone point me in the direction for debugging this?\nThanks! :)", "replies": ["Hi Jakob - does the image that you're using contain an ENTRYPOINT?", "Actually, another idea - have you checked what version of dagster is running in the container that launched the job?", "Hi daniel :slightly_smiling_face: yes, [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"-m\", \"annotator.orchestration\"] in my image", "Try setting that to the CMD instead of the ENTRYPOINT", "Yes, just found theres a version difference between the container, and the daemon", "what are the two versions?", "1.3.3 in the daemon and 1.3.1 in my image", "ok, that shouldn't matter - probably the CMD thing", "dagster updates the CMD but not the ENTRYPOINT, so i think it might be smashing them together?", "Ah! That makes sense! I think that's the solution. Cheers daniel :slightly_smiling_face:"], "is_dagster_empl": [true, true, false, true, false, true, false, true, true, false], "gpt4_replies_target": "The issue seems to be related to the CMD and ENTRYPOINT in the Docker configuration. The user was advised to set the CMD instead of the ENTRYPOINT, as Dagster updates the CMD but not the ENTRYPOINT, which might be causing the unexpected extra arguments error.", "dagster_empl_first_target": "Hi Jakob - does the image that you're using contain an ENTRYPOINT?", "dagster_empl_last_target": "dagster updates the CMD but not the ENTRYPOINT, so i think it might be smashing them together?"}, {"question": "Hello, new here so let me know if I am not posting this in the right location. I am running into a Terraform apply issue with the below error. Does anyone know what the cause of this is?\n\n```helm_release.this: Still modifying... [id=dagster, 4m20s elapsed]\nhelm_release.this: Still modifying... [id=dagster, 4m30s elapsed]\nhelm_release.this: Still modifying... [id=dagster, 4m40s elapsed]\nhelm_release.this: Still modifying... [id=dagster, 4m50s elapsed]\nhelm_release.this: Still modifying... [id=dagster, 5m0s elapsed]\n\nError: timed out waiting for the condition\n\n  with helm_release.this,\n  on <http://helm.tf|helm.tf> line 59, in resource \"helm_release\" \"this\":\n  59: resource \"helm_release\" \"this\" {\n\nError: Process completed with exit code 1.```", "replies": ["We don\u2019t maintain a terraform provider for dagster but it looks like maybe you\u2019re using the hashicorp maintained helm release provider? if so, it looks like you\u2019re hitting a timeout while the helm chart applies and you can override it here:\n\n<https://registry.terraform.io/providers/hashicorp/helm/latest/docs/resources/release#timeout>", "Thanks a ton!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The error you're encountering is likely due to a timeout while the helm chart applies using the Hashicorp maintained helm release provider. You can override this timeout using the instructions provided in the Hashicorp documentation.", "dagster_empl_first_target": "We don\u2019t maintain a terraform provider for dagster but it looks like maybe you\u2019re using the hashicorp maintained helm release provider? if so, it looks like you\u2019re hitting a timeout while the helm chart applies and you can override it here:\n\n<https://registry.terraform.io/providers/hashicorp/helm/latest/docs/resources/release#timeout>", "dagster_empl_last_target": "We don\u2019t maintain a terraform provider for dagster but it looks like maybe you\u2019re using the hashicorp maintained helm release provider? if so, it looks like you\u2019re hitting a timeout while the helm chart applies and you can override it here:\n\n<https://registry.terraform.io/providers/hashicorp/helm/latest/docs/resources/release#timeout>"}, {"question": "Hi all...quick question. I'm trying to run assets in a jupyter notebook. How do I materialize a source asset? Sorry I couldn't find it quickly in the docs. Thanks!!", "replies": ["Hi Yang, source assets are by definition not materializable using a compute function defined in dagster.\n\nYou can always yield `AssetMaterialization` events for an arbitrary asset key from an op though.", "oh ok", "thanks!"], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "Source assets cannot be materialized using a compute function in Dagster, but you can yield `AssetMaterialization` events for an arbitrary asset key from an operation.", "dagster_empl_first_target": "Hi Yang, source assets are by definition not materializable using a compute function defined in dagster.\n\nYou can always yield `AssetMaterialization` events for an arbitrary asset key from an op though.", "dagster_empl_last_target": "Hi Yang, source assets are by definition not materializable using a compute function defined in dagster.\n\nYou can always yield `AssetMaterialization` events for an arbitrary asset key from an op though."}, {"question": "I have metadata on assets in my graph and I'm trying to query that metadata through the graphql endpoint. However, the `metadataEntries` are empty for all of my asssets.\nexample query:\n```query RepositoriesQuery {\n      repositoriesOrError {\n        ... on RepositoryConnection {\n          nodes {\n            name\n            assetNodes{\n                assetKey {\n                  path\n                }\n                metadataEntries {\n                    label\n                    description\n                }\n     ...```\nresponse has `metadataEntires:[]` for all assets despite seeing metadata in dagit.", "replies": ["Hi Drew, Are you able to create a minimal repro case? Like a `Definitions` object with one asset with metadata that is displaying this behavior.", "Is there a simple dagster example repo somewhere? All the examples require environment variables and deployments and whatnot", "But right now, I add metadata to all of my assets with `context.add_output_metadata()` in the `io-manager` and none of them have `metadataEntries`", "Ah-- that\u2019s because `context.add_output_metadata` does not add metadata to the assets definition. That puts metadata on the `AssetMaterialization` event. The metadata on the definition, which is what you\u2019re querying in that GQL query, is set like this (also an example of a simple example of the kind I\u2019m talking about):\n\n```from dagster import asset, Definitions\n\n@asset(metadata={\"foo\": \"bar\"})\ndef foo_asset():\n    return 1\n\ndefs = Definitions(\n    assets=[foo_asset]\n)```\nPut that in a file and run `dagster dev -f path/to/file` and your query should return the correct metadata.", "is there a way to define asset metadata at output time in the io-manager?", "There is not, but I think if you\u2019re trying to do this you might have a fundamantal misconception of Dagster\u2019s model. Dagster `AssetsDefinition` objects are basically wrappers around ops. They are a unit for buliding your data pipelines. They are defined when you load the code location, along with ops, jobs etc. They are not changed by Dagster runs.\n\nWhat is generated by runs are events. An `AssetMaterialization` is a kind of event, generated every time an asset\u2019s materialization function is run. You modify the materialization generated by the current run when you use `add_output_metadata`.", "So, I'm now doing\n```assetMaterializations {\n              timestamp\n              metadataEntries {\n                label\n                description\n              }\n              partition\n            }```\nI'm getting the correct labels but all of the `description` fields are `None`  in the graphql response despite being populated in the  dagit ui?", "", "And from the `graphql` front-end (as well as from a python gql request):\n```  \"path\": [\n                  \"funding_rates_and_info\"\n                ]\n              },\n              \"assetMaterializations\": [\n                {\n                  \"timestamp\": \"1684156958695\",\n                  \"metadataEntries\": [\n                    {\n                      \"label\": \"uri\",\n                      \"description\": null\n                    }\n                  ],\n                  \"partition\": null\n                },\n                {\n                  \"timestamp\": \"1683830321403\",\n                  \"metadataEntries\": [\n                    {\n                      \"label\": \"path\",\n                      \"description\": null\n                    }\n                  ],\n                  \"partition\": null\n                }\n              ],```\n"], "is_dagster_empl": [true, false, false, true, false, true, false, false, false], "gpt4_replies_target": "The issue you're experiencing is due to the fact that `context.add_output_metadata` does not add metadata to the assets definition, but rather to the `AssetMaterialization` event. The metadata on the definition, which is what you\u2019re querying in that GQL query, is set using the `@asset(metadata={\"foo\": \"bar\"})` decorator. If you want to define asset metadata at output time in the io-manager, currently there is no way to do this.", "dagster_empl_first_target": "Hi Drew, Are you able to create a minimal repro case? Like a `Definitions` object with one asset with metadata that is displaying this behavior.", "dagster_empl_last_target": "There is not, but I think if you\u2019re trying to do this you might have a fundamantal misconception of Dagster\u2019s model. Dagster `AssetsDefinition` objects are basically wrappers around ops. They are a unit for buliding your data pipelines. They are defined when you load the code location, along with ops, jobs etc. They are not changed by Dagster runs.\n\nWhat is generated by runs are events. An `AssetMaterialization` is a kind of event, generated every time an asset\u2019s materialization function is run. You modify the materialization generated by the current run when you use `add_output_metadata`."}, {"question": "Hi - I\u2019m trying to set up software-defined assets for my hightouch syncs and running into an issue that\u2019s probably simple. I have the following set-up:\n```hightouch_instance = ht_resource.configured(\n    {\"api_key\": os.getenv(\"HIGHTOUCH_API_KEY\")}\n)\n\nrun_ht_deal_sync_op = hightouch_sync_op.configured(\n    {\"sync_id\": HT_DEAL_DATA}, name=\"platform_deal_sync\"\n)\n\n\n@asset(io_manager_key=\"io_manager\", required_resource_keys={\"hightouch\"})\ndef hightouch_deal_sync():\n    run_ht_deal_sync_op()```\nBut I am getting the following error when running the asset through dagster locally: \u201cCompute function of op \u2018platform_deal_sync\u2019 has context argument, but no context was provided when invoking.\u201d\n\nIf I\u2019m understanding correctly, the hightouch sync op needs the configuration argument to execute, but I thought that the ht_resource.configured() call should set that up already.", "replies": ["Ah - I was able to get this working by creating a context using build_op_context(). However, I am now running into a new issue where the resources I set up in the main file\u2019s Definitions aren\u2019t included when I materialize this asset.", "Hmm, according to the docs, manually building an op context seems to be mostly used for testing, so there\u2019s probably a better way to pipe the resources I defined into the op execution.", "I think you are missing some fundamental Dagster concepts. You should review the docs again, especially the core concepts.", "Can you elaborate more? A lot of my issue here is that the hightouch docs have nothing for asset configuration so I\u2019m guessing with most of the setup for hightouch specifically. I haven\u2019t had an issue with airbyte integration at all.", "I mean the code you provided was not really complete. You want to use the required resource but no where is the resource configured for the asset. ", "Oh - sorry that wasn\u2019t clear, the resource definitions are in another file here:\n\n```defs = Definitions(\n    assets=load_assets_from_package_module(assets),\n    resources={\n        \"io_manager\": FilesystemIOManager(),\n        \"hightouch\": hightouch_instance,\n        \"airbyte\": airbyte_instance,\n    },\n)```\n", "To be a bit more helpful here the main thing you're missing is that you're trying to call an op from within another op/asset. Ops / assets cannot be called from one another, so if you have a function you want to share between ops / assets you need to encapsulate it as a vanilla python function. Dependencies between ops are defined in graphs, whereas assets infer their dependencies to create the execution graph based on the asset functions inputs or explicit asset key references.", "it's likely in your example you'll just want to take the logic from you `hightouch_sync_op` and put it directly in the `hightouch_deal_sync` asset", "scratch that, this is coming from an integration", "Ahh, ok that makes sense, thanks. In this case, the hightouch-dagster library only exposes an op, in the source code it just has the `hightouch_sync_op`\nmethod as an op.", "(this does explain the error I was getting though)", "so if you want to make it into an asset you could use the <https://docs.dagster.io/concepts/assets/graph-backed-assets#graph-backed-assets|graph_asset> decorator to call your op from a graph and create a graph-backed asset", "Awesome, that should work, I\u2019ll try that. I\u2019m a bit confused about the docs <https://docs.dagster.io/concepts/assets/software-defined-assets|here> still - it explicitly says a software-defined asset should include an op. What is the proper way to configure that for my own understanding?", "Happy to just look at an example if that\u2019s easier :slightly_smiling_face:", "yeah that's some very confusing text. a better description is that an asset IS an op under the hood, with additional metadata that Dagster tracks and a bigger emphasis on IO management through IOManagers. the code within the `@asset` definition is just vanilla python code to carry out whatever task / transformation you're modeling, the same way an op is defined. so a simple asset would be like\n\n```@asset\ndef unique_companies(industry_df: spark.sql.DataFrame):\n    return [r.company for r in industry_df.select(\"company\").distinct().collect()]```", "you could do the same exact thing with an op, but dagster treats them slightly differently in that it doesn't have first-class mechanisms for tracking state associated with an execution for ops. assets are a way to essentially execute logic that also executes in a graph, but for which you want to track some external state", "Ahhhh ok that makes complete sense. So in this case because the hightouch integration only exposes an op method, I can create an op graph from that and create my asset from there.", "exactly, graph-backed assets were exposed as an additional way to create assets for situations where the logic for creating the state being produced by the asset is dynamic in nature or better modeled as a number of steps. in your case, because the framework exposes the functionality you want as an op, graph-backed assets make an easy way to transform that into an asset", "nice, makes sense now, thanks for all the help!", "<@U056T5AFEUT>, did you ever figure out how to splice the Hightouch integration stuff together?\n\n<@U05DLJAKVBL> and I struggling with the same issue. I think we may be misunderstanding how to work with GraphAssets and/or `AssetsDefinition.from_op`", "<https://docs.dagster.io/_apidocs/assets#dagster.AssetsDefinition.from_op>", "Hi Dan, sort of, I basically implemented a fake asset that represents the hightouch sync output, using a graph asset definition. The core issue here is that the hightouch library only provides an API to create an op from a sync. I think using `from_op`  should technically work, but my approach was to use the `@graph` decorator. The code is pretty hacky, really just a way to consolidate my syncs into assets so that the rest of the asset model can be clean.", "```@graph_asset(\n    ins={\n        \"platform_tables\": AssetIn(\n            key=AssetKey([\"platform_tables\"]),\n        )\n    },\n)\ndef platform_export_live(platform_tables):\n    first_sync = run_ht_first_sync_op(start_after=platform_tables)\n    return run_ht_second_sync_op(start_after=first_sync)```", "that\u2019s pretty much what I did (names anonymized). Basically just giving it an input asset (not that it can use anything from it) and then manually setting the ops to run after each other.", "if you\u2019re able to create assets directly from ops, and still have the proper dependencies, that\u2019s likely a cleaner solution.", "&gt; really just a way to consolidate my syncs into assets so that the rest of the asset model can be clean.\nAwesome - that's really all we need, too :slightly_smiling_face:", "Thank you <@U056T5AFEUT>!", "No problem!"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "The issue you're facing is due to trying to call an op from within another op/asset, which is not allowed in Dagster. Instead, you can encapsulate the shared function as a vanilla python function. If you want to convert an op into an asset, you can use the graph_asset decorator to call your op from a graph and create a graph-backed asset.", "dagster_empl_first_target": "Ah - I was able to get this working by creating a context using build_op_context(). However, I am now running into a new issue where the resources I set up in the main file\u2019s Definitions aren\u2019t included when I materialize this asset.", "dagster_empl_last_target": "No problem!"}, {"question": "Hi All!\nIs it possible to add more functionality to an Op RetryPolicy where if an Op execution exceeds a pre-set amount of time the Op is re-executed?", "replies": ["Hi Sanidhya, That\u2019s not currently possible but if you would find this useful, I encourage you to open a Github feature request."], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, it's not possible to add more functionality to an Op RetryPolicy where if an Op execution exceeds a pre-set amount of time the Op is re-executed.", "dagster_empl_first_target": "Hi Sanidhya, That\u2019s not currently possible but if you would find this useful, I encourage you to open a Github feature request.", "dagster_empl_last_target": "Hi Sanidhya, That\u2019s not currently possible but if you would find this useful, I encourage you to open a Github feature request."}, {"question": "Hello,\nCan I access the `RetryPolicy` applied to an Op from `OpExecutionContext` ? My goal is to check whether the current number of retries is still less than the maximum retries configured for an Op, so that I can raise a certain exception, otherwise I would just want to continue the pipeline passing on information about the error.", "replies": ["I\u00b4ve found out that I can access the `RetryPolicy` through `OpExecutionContext.op.retry_policy`"], "is_dagster_empl": [false], "gpt4_replies_target": "No, you cannot directly access the `RetryPolicy` applied to an Op from `OpExecutionContext`. However, you can access the `RetryPolicy` through `OpExecutionContext.op.retry_policy`.", "dagster_empl_first_target": "I\u00b4ve found out that I can access the `RetryPolicy` through `OpExecutionContext.op.retry_policy`", "dagster_empl_last_target": "I\u00b4ve found out that I can access the `RetryPolicy` through `OpExecutionContext.op.retry_policy`"}, {"question": "Hello,\nI'm trying to understand <https://docs.dagster.io/guides/dagster/asset-versioning-and-caching#asset-versioning-and-caching|asset versioning and caching>. I'm rematerializing an upstream asset with a static code version and output content.\n\nDespite upstream code version and data version remaining unchanged after each materialization, the downstream asset is always rematerialized through reconciliation sensor. How can I prevent downstream assets materialization when upstream code and data version did not change ?", "replies": ["Hi Lucas, the asset reconciliation sensor currently does not take account of \u201cStaleness\u201d as represented in dagit. The behavior of the asset reconciliation sensor is described here: <https://docs.dagster.io/_apidocs/schedules-sensors#dagster.build_asset_reconciliation_sensor>\n\nWe recognize that this is very confusing, and we are going to be landing an integration of freshness policies and the version-based staleness system very soon which will address this issue.", "Hello <@U01GTMVMGQH>,\nThank you for the update. I'm also trying to move from Schedule to Freshness/Reconciliation sensor for our time-partitioned assets. I was trying to translate a Schedule with N minutes offset into a Freshness with \"N * * * *\" cron in order to only trigger materialization at Nth minute past the hour.\n\nIt does not seem to work (yet?). Can I expect the integration you mentionned to solve this issue ?"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The asset reconciliation sensor currently does not account for \"Staleness\" as represented in dagit, but an integration of freshness policies and the version-based staleness system is expected to be implemented soon to address this issue.", "dagster_empl_first_target": "Hi Lucas, the asset reconciliation sensor currently does not take account of \u201cStaleness\u201d as represented in dagit. The behavior of the asset reconciliation sensor is described here: <https://docs.dagster.io/_apidocs/schedules-sensors#dagster.build_asset_reconciliation_sensor>\n\nWe recognize that this is very confusing, and we are going to be landing an integration of freshness policies and the version-based staleness system very soon which will address this issue.", "dagster_empl_last_target": "Hi Lucas, the asset reconciliation sensor currently does not take account of \u201cStaleness\u201d as represented in dagit. The behavior of the asset reconciliation sensor is described here: <https://docs.dagster.io/_apidocs/schedules-sensors#dagster.build_asset_reconciliation_sensor>\n\nWe recognize that this is very confusing, and we are going to be landing an integration of freshness policies and the version-based staleness system very soon which will address this issue."}, {"question": "Hi, I am not correctly able to grasp the concept of the class `EnvVar` as in what purpose does the class solve, how would `EnvVar` serve better when python inbuilt `os.getenv()` exists ?", "replies": ["I was assuming its because with EnvVar, making a Resource, Dagster can check before running the operation if the dependencies are satisfied, instead of throwing a runtime error.", "Oh okay that makes sense <@U051401N6M6>"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The `EnvVar` class in Dagster allows for checking if dependencies are satisfied before running an operation, providing an advantage over Python's inbuilt `os.getenv()`, which would only throw a runtime error.", "dagster_empl_first_target": "I was assuming its because with EnvVar, making a Resource, Dagster can check before running the operation if the dependencies are satisfied, instead of throwing a runtime error.", "dagster_empl_last_target": "Oh okay that makes sense <@U051401N6M6>"}, {"question": "Hello - I've got an asset that is a fan-in from _many_ upstream assets. Those upstream assets have `output_required=False` set on them, since they only output new data if the data is changed since last materialization. I'd like the downstream asset to be materialized if _any_ of the upstream assets output new data, but currently the job will abort materializing the downstream asset if even one upstream _doesn't_ output a new value.", "replies": ["Is there a way to change the logic so that the downstream asset is always rebuilt?", "I don't work for dagster support -- but look at a multi-asset-sensor. There is an example in the docs that shows a sensor kicking off a job if ALL of the upstream assets have output and also an example kicking off a job if ANY of the upstream assets have output.", "Thank you. I'll check it out!"], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "You can change the logic so that the downstream asset is always rebuilt by using a multi-asset-sensor, as shown in the Dagster documentation, which can kick off a job if ANY of the upstream assets have output.", "dagster_empl_first_target": "Is there a way to change the logic so that the downstream asset is always rebuilt?", "dagster_empl_last_target": "Thank you. I'll check it out!"}, {"question": "Hi Team,  I\u2019m trying to use MonthlyPartitionsDefinition with multi_asset and it doesn\u2019t go well with mypy, here is an sample\n```from dagster import AssetOut, MonthlyPartitionsDefinition, multi_asset\n\n@multi_asset(\n    partitions_def=MonthlyPartitionsDefinition(start_date='2022-01-01'),\n    outs={'test': AssetOut()},\n)\ndef test_multi_asset():\n    pass```", "replies": ["Mypy error output is\n```test_asset.py:5: error: Argument \"partitions_def\" to \"multi_asset\" has incompatible type \"MonthlyPartitionsDefinition\"; expected \"Optional[PartitionsDefinition[object]]\"  [arg-type]```", "I think we should update the type for  `partitions_def` to `Optional[PartitionsDefinition[Any]]`", "Hi Sean, thanks for bringing this to our attention-- I think we will solve this by making the TypeVar for `PartitionsDefinition` covariant. Look for this in the next release."], "is_dagster_empl": [false, false, true], "gpt4_replies_target": "The issue you're experiencing with MonthlyPartitionsDefinition and multi_asset is due to the incompatible type \"MonthlyPartitionsDefinition\"; the expected type is \"Optional[PartitionsDefinition[object]]\". The team plans to solve this by making the TypeVar for `PartitionsDefinition` covariant in the next release.", "dagster_empl_first_target": "Hi Sean, thanks for bringing this to our attention-- I think we will solve this by making the TypeVar for `PartitionsDefinition` covariant. Look for this in the next release.", "dagster_empl_last_target": "Hi Sean, thanks for bringing this to our attention-- I think we will solve this by making the TypeVar for `PartitionsDefinition` covariant. Look for this in the next release."}, {"question": "Hi,\n\nI need to process a large amount of data files in S3. These files although scattered in different paths, they still can be grouped in various different ways.\nIn past I have achieved this with Celery, but it was like I was going in blind with no observability. but since Dagster will give me good observability on smaller chunks too, i am looking for recommendations on approach\n\nForemost I am looking to collect them every hour and then combine*[bulk via jsonl]* them on a per day basis and then process each day\u2019s bulked files.\n\nI need to be able to check on each hour\u2019s execution and finally validate counts on the bulk file.\n\nWould assets + partitions be more suitable for such work or would it be better with Ops ?\n\nI had asked this question <https://dagster.slack.com/archives/C01U954MEER/p1677761140234819|here> but it got buried under other messages", "replies": ["I am trying to do something similar like this with a DynamicPartition. I think you currently cannot have custom parition dependencies, but as long as you have 1 on 1 dependencies it should work.", "by 1-1 dependencies, you mean 1-1 dependencies between upstream and downstream assets ?", "yes", "although you mentioned hourly / day partition, I think that's already implemented in dagster itself", "Yeah, I see that it has Hourly and Daily partitions, but does it appear as fan-in as in  24 hourly Upstream assets to 1 daily downstream asset ? I am still building the strategy to parse and process my data files", "<https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#partition-dependencies>", "seems to be the case", "Oh cool thank you, let me check that out"], "is_dagster_empl": [false, false, false, false, false, false, false, false], "gpt4_replies_target": "You can achieve your goal of processing large amounts of data files in S3 using Dagster's DynamicPartition feature, which allows for 1-1 dependencies between upstream and downstream assets. Dagster also has built-in hourly and daily partitions, which can be used to collect and process your data files on an hourly basis and then combine them on a daily basis. You can check on each hour's execution and validate counts on the bulk file using Dagster's observability features.", "dagster_empl_first_target": "I am trying to do something similar like this with a DynamicPartition. I think you currently cannot have custom parition dependencies, but as long as you have 1 on 1 dependencies it should work.", "dagster_empl_last_target": "Oh cool thank you, let me check that out"}, {"question": "What is the most elegant way to give `op` a custom name? The most simple I've found so far is something like this:\n`my_op.configured(None, \"my_custom_name\")()` but maybe there are more elegant ways?", "replies": ["For just changing the name you can do: `my_op.alias(\"my_custom_name\")()`", "Great! Thanks Alex"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The most elegant way to give `op` a custom name is by using the `alias` method like this: `my_op.alias(\"my_custom_name\")()`.", "dagster_empl_first_target": "For just changing the name you can do: `my_op.alias(\"my_custom_name\")()`", "dagster_empl_last_target": "Great! Thanks Alex"}, {"question": "Don't know if this is more of a python or Dagster question, but.... is there an easy way with Dagster, when using more than one code location, to make a custom function available to all code location and their modules/sub-modules? Or do I have to write and install a package?", "replies": ["If your code locations live in the same monorepo, you can install a package from local path. Otherwise, you can install the common package from `git+https` or `git+ssh`.\n\nI advise to publish it a private PyPI properly tho, this way different code locations don't need to be in sync with the common package version all the time. Installing from `git` would allow this too but it's a little harder to configure for docker builds etc", "They don't live in the same monorepo, unfortunately... they're all in separate repos and deployed in docker containers.", "^ added comments about `git+...` installtion", "Thank you, Daniel!", "I was hoping Dagster had some type of official extension system that I didn't know about. :slightly_smiling_face:", "For what it\u2019s worth we are doing the `git` way from Daniel\u2019s solutions with some code that is used in different services across our systems. It\u2019s doable but currently requires manually updating Dockerfiles with the latest version of the package, so it\u2019s not as nice as pinning some version range in a requirements.txt. But it\u2019s usable still.", "you can use `git+` with requirements.txt as well as with Poetry", "shouldn't be a problem with that", "Yes, that\u2019s right. I think we had some other reasons (unrelated to this question) why we went for adding it in the Dockerfile, so just ignore the dockerfile part.", "But if you want to control what version package you install from the repo via `git+`  (eg. from `release/...` branches) you\u2019ll end up manually updating your requirements.txt too.", "yes"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "In Dagster, to make a custom function available to all code locations and their modules/sub-modules, you can install a package from a local path if your code locations live in the same monorepo, or install the common package from `git+https` or `git+ssh` if they are in separate repos. However, it's recommended to publish it a private PyPI for easier version control.", "dagster_empl_first_target": "If your code locations live in the same monorepo, you can install a package from local path. Otherwise, you can install the common package from `git+https` or `git+ssh`.\n\nI advise to publish it a private PyPI properly tho, this way different code locations don't need to be in sync with the common package version all the time. Installing from `git` would allow this too but it's a little harder to configure for docker builds etc", "dagster_empl_last_target": "yes"}, {"question": "how to add custom markers like \"api\"?", "replies": ["you can add a \"kind\" tag to the op definition, like this:\n```@op(\n    tags={\"kind\": \"api\"},\n)\ndef do_processing():\n   ...```\n", "Zach is right for ops! and for assets, you can use the `compute_kind` argument on the asset decorator, like <https://github.com/dagster-io/hooli-data-eng-pipelines/blob/efb6a3e4c964f15e7fc86aa39092726a35c30d91/hooli_data_eng/assets/raw_data/__init__.py#L6|here>\n```@asset(\n    compute_kind=\"api\",\n    required_resource_keys={\"data_api\"}\n)\ndef users(context) -&gt; pd.DataFrame:\n    \"\"\"A table containing all users data\"\"\"\n    api = context.resources.data_api\n    resp = api.get_users()\n    users = pd.read_json(resp.json())\n    return users```", "thanks guys!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "To add custom markers like \"api\", you can add a \"kind\" tag to the op definition or use the `compute_kind` argument on the asset decorator.", "dagster_empl_first_target": "Zach is right for ops! and for assets, you can use the `compute_kind` argument on the asset decorator, like <https://github.com/dagster-io/hooli-data-eng-pipelines/blob/efb6a3e4c964f15e7fc86aa39092726a35c30d91/hooli_data_eng/assets/raw_data/__init__.py#L6|here>\n```@asset(\n    compute_kind=\"api\",\n    required_resource_keys={\"data_api\"}\n)\ndef users(context) -&gt; pd.DataFrame:\n    \"\"\"A table containing all users data\"\"\"\n    api = context.resources.data_api\n    resp = api.get_users()\n    users = pd.read_json(resp.json())\n    return users```", "dagster_empl_last_target": "Zach is right for ops! and for assets, you can use the `compute_kind` argument on the asset decorator, like <https://github.com/dagster-io/hooli-data-eng-pipelines/blob/efb6a3e4c964f15e7fc86aa39092726a35c30d91/hooli_data_eng/assets/raw_data/__init__.py#L6|here>\n```@asset(\n    compute_kind=\"api\",\n    required_resource_keys={\"data_api\"}\n)\ndef users(context) -&gt; pd.DataFrame:\n    \"\"\"A table containing all users data\"\"\"\n    api = context.resources.data_api\n    resp = api.get_users()\n    users = pd.read_json(resp.json())\n    return users```"}, {"question": "I am testing out using an asset reconciliation sensor for selected assets, and encountering this issue: Assets A and B are managed by the sensor. Asset A is upstream of Asset B. Asset B is downstream of Asset A and (non-observable) Source Asset C. When Asset A is materialized, I expect Asset B to be materialized shortly thereafter, but this is not happening. Any ideas what is going on?", "replies": ["pseudo-code:\n```asset_reconciliation_sensor(A,B)\nA &gt;&gt; B\nSource Asset C &gt;&gt; B```", "cc <@U01J51Y6B9D>", "hi <@U03V3QRND0U> ! this is a bug -- luckily, it looks like there's an easy fix, so this can make its way into next week's release"], "is_dagster_empl": [false, true, true], "gpt4_replies_target": "The issue you're experiencing with the asset reconciliation sensor for Assets A and B is identified as a bug, but there's an easy fix that should be included in the next week's release.", "dagster_empl_first_target": "cc <@U01J51Y6B9D>", "dagster_empl_last_target": "hi <@U03V3QRND0U> ! this is a bug -- luckily, it looks like there's an easy fix, so this can make its way into next week's release"}, {"question": "I ran into an error when creating a `graph_asset` from dynamic output ops:\n\n```@op(out=DynamicOut())\ndef test1() -&gt; DynamicOut:\n    for i in range(10):\n        yield DynamicOutput(i, mapping_key=str(i))\n\n@op\ndef test2(i: int) -&gt; None:\n    print(i)\n\n@graph_asset\ndef test_graph() -&gt; None:\n    test1().map(test2)```\nGives the following error:\n```dagster._check.CheckError: Invariant failed. Description: The set of output names keys specified in the keys_by_output_name argument must equal the set of asset keys outputted by test_graph. \nkeys_by_input_name keys: {'result'} \nexpected keys: set()```\nI think it's somethin about the wiring of the ops but I can't wrap my head around it.\n\nAny ideas :slightly_smiling_face: ?", "replies": ["I think you still need to return the data you want to represent as your \"asset\" from your graph_asset. something like\n```@op(out=DynamicOut())\ndef test1() -&gt; DynamicOut:\n    for i in range(10):\n        yield DynamicOutput(i, mapping_key=str(i))\n\n@op\ndef test2(i: int) -&gt; None:\n    print(i)\n\n@graph_asset\ndef test_graph() -&gt; None:\n    return test1().map(test2).collect()```", "I am new to dagster and trying to learn dynamic task generation. I tried this code of yours, however i am getting the below error. I have a very similar use case where I have an OP which returns list of values, which are used to generate dynamic OPs result of which is then sent to an ASSET. Any help on this would be awesome :slightly_smiling_face:\n```dagster._core.errors.DagsterInvalidDefinitionError: @graph 'test_graph' returned problematic value of type &lt;class 'dagster._core.definitions.composition.DynamicFanIn'&gt;. Expected return value from invoked solid or dict mapping output name to return values from invoked solids```", "yeah so dynamic ops need to have their outputs collected / collapsed before saving into an asset. you might need another op to do that collection. something like this might work\n```@op(out=DynamicOut())\ndef test1() -&gt; DynamicOut:\n    for i in range(10):\n        yield DynamicOutput(i, mapping_key=str(i))\n\n@op\ndef test2(i: int) -&gt; int:\n    print(i)\n    return i + 1\n\n@op\ndef test3(i: List[int]):\n    return i\n\n@graph_asset\ndef test_graph() -&gt; None:\n    return test3(test1().map(test2).collect())```", "This is exactly what I needed. Thanks a ton. Really appreciate you for the learning :)", "I got NoneType when .collect()", "are you returning a value from the op you're calling .collect() on?", "yes", "could you provide an example of what you're trying to do and the stack trace that you're getting? it's hard to know how to help or what might be going wrong without knowing what you're doing", "I really don't know what I did, but now it's working, hahaha, thanks, if I get into another trouble I'll ask :slightly_smiling_face:"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "The error occurs because dynamic ops need to have their outputs collected or collapsed before saving into an asset. You might need another op to do that collection. Make sure you are returning a value from the op you're calling .collect() on.", "dagster_empl_first_target": "I think you still need to return the data you want to represent as your \"asset\" from your graph_asset. something like\n```@op(out=DynamicOut())\ndef test1() -&gt; DynamicOut:\n    for i in range(10):\n        yield DynamicOutput(i, mapping_key=str(i))\n\n@op\ndef test2(i: int) -&gt; None:\n    print(i)\n\n@graph_asset\ndef test_graph() -&gt; None:\n    return test1().map(test2).collect()```", "dagster_empl_last_target": "I really don't know what I did, but now it's working, hahaha, thanks, if I get into another trouble I'll ask :slightly_smiling_face:"}, {"question": "I am currently building a pipeline that 1. pulls data from a google sheet -&gt; 2. gets gpt to write a summary of the data -&gt; 3. sends a message on slack. I have heard that I can use dagster to run this on a schedule as well as change one variable each time it runs, I have written the code and it works already but am not sure how to convert it into a dagster project, has anyone created something similar which I could take a look at, or have advice for how I can approach this project.", "replies": ["<https://docs.dagster.io/guides/dagster/intro-to-ops-jobs/single-op-job>\n\nyou could do it all in one op just to get started to make things simple, just calling your existing code from the op. then once you've got that working you could split it into 3 ops to model the three steps in your pipeline", "here\u2019s a blog post talking about this transition: <https://dagster.io/blog/dagster-script-to-assets>", "thanks!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "You can start by converting your code into a single op in Dagster and then gradually split it into three ops to represent each step of your pipeline, you can refer to the Dagster documentation and blog post for a detailed guide on this transition.", "dagster_empl_first_target": "here\u2019s a blog post talking about this transition: <https://dagster.io/blog/dagster-script-to-assets>", "dagster_empl_last_target": "here\u2019s a blog post talking about this transition: <https://dagster.io/blog/dagster-script-to-assets>"}, {"question": "I'm noticing with the graphql API kicking off a job has  `respositoryLocation` and `repositoryName` as required -- I'm also seeing in the docs that definitions effectively replace repositories -- what goes in the GQL fields now?", "replies": ["nvm, figured this out via a little reverse engineering, took a look at the dagster console outgoing requests -- just filled in with my code location name and `__repository__`"], "is_dagster_empl": [false], "gpt4_replies_target": "You can fill in the GQL fields with your code location name and `__repository__`.", "dagster_empl_first_target": "nvm, figured this out via a little reverse engineering, took a look at the dagster console outgoing requests -- just filled in with my code location name and `__repository__`", "dagster_empl_last_target": "nvm, figured this out via a little reverse engineering, took a look at the dagster console outgoing requests -- just filled in with my code location name and `__repository__`"}, {"question": "Hi, how do I best manage freshness policies. When I have a bigquery source  table that is updated outside of the control of dagster. I can get events from Google pubsub to signal the table has been updated. Would I use asset versions for the source table? Or how can I signal that the data in the source table has been updated.", "replies": ["Downstream assets mostly live in dbt.", "You might think about putting a sensor on your source table and then add a reconciliation for the downstream tables.", "Wouldn\u2019t it make sense to materialize via api instead of polling the table via api?", "I had forgotten you had a pubsub message. I haven't done it, but a cloud function responding to the pubsub message to trigger the dagster API seems pretty viable as well.", "I have a similar scenario I\u2019m trying to solution for. I have a log sink going to pubsub for when a new source table lands in a monitored dataset in BigQuery.\n\nDoes Dagster require being triggered via API from a Cloud Function? It isn\u2019t possible to use a Dagster sensor directly tied to the pubsub topic?", "Even better, <@U04LXFQQEC9>!", "On second thought\u2026\n1. I don\u2019t see any Dagster documentation about interacting with PubSub. \n2. Do sensors incur compute billing as always being on?\nSo perhaps it\u2019s better to have a Cloud Function initiate a Dagster materialization via API", "Actually I just checked if it is possible to send a post request directly from pub sub. And it seems it is not possible to define a post body. So a cloud function that triggers the api makes more sense. <https://cloud.google.com/pubsub/docs/push>", "A sensor that responds to a pubsub message could look like this (not tested, consider this pseudocode):\n```import os\nfrom dagster import sensor, RunRequest\nfrom google.cloud import pubsub_v1\n\n\n@sensor(job=trigger_job)\ndef my_directory_sensor():\n\n    # project_id = \"your-project-id\" (probably in config)\n    # subscription_id = \"your-subscription-id\"\n\n    subscriber = pubsub_v1.SubscriberClient()\n    subscription_path = subscriber.subscription_path(project_id, subscription_id)\n\n    NUM_MESSAGES = 1\n\n    streaming_pull_future = subscriber.subscribe(subscription_path, callback=callback)\n    with subscriber:\n        # The subscriber pulls a specific number of messages. The actual\n        # number of messages pulled may be smaller than max_messages.\n        response = subscriber.pull(\n            request={\"subscription\": subscription_path, \"max_messages\": NUM_MESSAGES},\n            retry=retry.Retry(deadline=300),\n        )\n\n        if len(response.received_messages) == 0:\n            return\n\n        ack_ids = []\n        for received_message in response.received_messages:\n            print(f\"Received: {received_message.message.data}.\")\n            ack_ids.append(received_message.ack_id)\n\n        # Acknowledges the received messages so they will not be sent again.\n        subscriber.acknowledge(\n            request={\"subscription\": subscription_path, \"ack_ids\": ack_ids}\n        )\n        filename = response.received_messages[0].message.data \n        \n        if os.path.isfile(filename):\n            yield RunRequest(\n                run_key=filename,\n                run_config={\n                    \"ops\": {\"process_file\": {\"config\": {\"filename\": filename}}}\n                },\n            )```\nThis uses a pubsub pull rather than push. By default, dagster sensor will attempt to pull a message from the subscription. If one exists (and the message contains the filename), the sensor will fire off the job using the filename pulled from the pubsub message. See <https://cloud.google.com/pubsub/docs/pull#unary_pull_code_samples> and <https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors#defining-a-sensor>.", "<@U031ST9MT6F> just tried your implementation and it seems to work as expected. Did not have to change much. Thank you!"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "You can manage freshness policies by using a sensor on your source table and adding a reconciliation for the downstream tables. You can also use a cloud function responding to the pubsub message to trigger the Dagster API when the table has been updated.", "dagster_empl_first_target": "Downstream assets mostly live in dbt.", "dagster_empl_last_target": "<@U031ST9MT6F> just tried your implementation and it seems to work as expected. Did not have to change much. Thank you!"}, {"question": "Feel like a noob here, but what are the differences between python_logs and compute_logs in dagster.yaml? (dagster 1.2.1, dagster-k8s 0.18.1)", "replies": ["Python logs are messages sent using python's build-in logging module (<https://docs.python.org/3/howto/logging.html>), Compute logs are just anything printed to stdout/stderr during op execution", "and those keys in dagster.yaml let you configure the capturing/storage of those two types of logs", "Thank you Anthony! It seems these two capture the same information right?", "Usually no - most python loggers are not usually writing to the terminal, and a function/program you call from your op could be just `print`ing stuff you want to capture. Unless I misunderstand what you mean?", "No I think you are right, though there is some overlap. When I look at the structured logs of a run, some of the logs also go to  stderr. But not all", "Suddenly it becomes very clear!", "thank you!", "For some reason mentally I connected the structured event logs in dagit with the button next to it: raw compute logs (with stdout, stderr). But these things are different", "no worries! there's definitely a lot of concepts to untangle at first - still trying to get my own head around everything. And yeah, many messages will appear in multiple logs :slightly_smiling_face: I think the structured event logs are actually a third thing, not the same as python logs OR compute logs: <https://docs.dagster.io/concepts/ops-jobs-graphs/op-events>"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "Python logs in dagster.yaml are messages sent using Python's built-in logging module, while compute logs are anything printed to stdout/stderr during op execution. These two keys in dagster.yaml allow you to configure the capturing/storage of these two types of logs. They capture different information, although there may be some overlap.", "dagster_empl_first_target": "Python logs are messages sent using python's build-in logging module (<https://docs.python.org/3/howto/logging.html>), Compute logs are just anything printed to stdout/stderr during op execution", "dagster_empl_last_target": "no worries! there's definitely a lot of concepts to untangle at first - still trying to get my own head around everything. And yeah, many messages will appear in multiple logs :slightly_smiling_face: I think the structured event logs are actually a third thing, not the same as python logs OR compute logs: <https://docs.dagster.io/concepts/ops-jobs-graphs/op-events>"}, {"question": "Hi dagster support, would someone be able to help with the following?\nI'm using the `dagster-pandera` integration. When the validation fails, it fails the op and the result isn't saved anywhere, so that I can inspect it. Ideally it wouldn't fail the op, but just show as a warning. Any ideas how I can achieve that? I can split out the validation as a separate op, but I then need lots of additional ops to validate each result", "replies": ["Hi Alex, This is definitely a weakness in the current system. Unfortunately, we don\u2019t have a great solution for now-- putting validation in a separate op (or job if it\u2019s an asset) is the best we can do.\n\nHowever, you should know that first-class Asset Expectations are on our road map, they will allow you to do data quality checks etc independent of the type-check (that dagster-pandera uses) or materialization.", "ok thanks for the update"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, the best solution is to put validation in a separate operation or job, but first-class Asset Expectations, which will allow independent data quality checks, are planned for future updates.", "dagster_empl_first_target": "Hi Alex, This is definitely a weakness in the current system. Unfortunately, we don\u2019t have a great solution for now-- putting validation in a separate op (or job if it\u2019s an asset) is the best we can do.\n\nHowever, you should know that first-class Asset Expectations are on our road map, they will allow you to do data quality checks etc independent of the type-check (that dagster-pandera uses) or materialization.", "dagster_empl_last_target": "Hi Alex, This is definitely a weakness in the current system. Unfortunately, we don\u2019t have a great solution for now-- putting validation in a separate op (or job if it\u2019s an asset) is the best we can do.\n\nHowever, you should know that first-class Asset Expectations are on our road map, they will allow you to do data quality checks etc independent of the type-check (that dagster-pandera uses) or materialization."}, {"question": "Hi, I have the exact same question about partitions as this poster: <https://stackoverflow.com/questions/75703813/reduce-a-partitioned-asset-to-a-single-data-frame> . Can we get an answer to this question (either here or on the site). Thanks! :slightly_smiling_face:", "replies": ["If you use your own IO manager you can then define whatever behaviour you want in `load_input()` for loading a partitioned upstream into a non-partitioned downstream. It has the benefit of flexibility, but you might have to reimplement stuff that is otherwise available in the built-in IO managers (the partitions as a dict thing is actually a feature of the built-in IO manager, not a default thing \u2014 you can create whatever loading logic you need in a custom IO manager).", "thanks Andras", "Yes, what Andras said is exactly right."], "is_dagster_empl": [false, false, true], "gpt4_replies_target": "You can define your own behavior in `load_input()` for loading a partitioned upstream into a non-partitioned downstream by using your own IO manager, although you might have to reimplement features available in the built-in IO managers.", "dagster_empl_first_target": "Yes, what Andras said is exactly right.", "dagster_empl_last_target": "Yes, what Andras said is exactly right."}, {"question": "Hi! :wave: I'm currently unable to deploy to my Dagster Cloud as I keep hitting `Error: No Dagster Cloud agent is actively heartbeating. Make sure that you have a Dagster Cloud agent running.` in the GitHub branch deploy action. The instance used to work a few months ago (I only upgraded today) and I've also updated the GitHub actions to the latest format from the serverless quickstart template.\n\n<https://dagster.slack.com/archives/C01U954MEER/p1682533378906179|I believe this might be a similar issue to this one> as I also had a trial last year which I let lapse. I added a payment method today to start using it again, so perhaps my agent was also spun down?", "replies": ["If it helps, I also see this on the agents page in the Dagster Cloud UI \u2013 it's been stuck on this for a while", "Hi robin - sorry for the trouble here, we\u2019ll get this sorted out for you shortly (should be no more than 30 minutes or so)", "You should be back up and running now - sorry about that!", "Just re-run the GH actions and looks like things deployed this time \u2013 thanks!"], "is_dagster_empl": [false, true, true, false], "gpt4_replies_target": "The user was unable to deploy to Dagster Cloud due to an error message indicating no active Dagster Cloud agent, but after reporting the issue, it was resolved within 30 minutes and the user was able to successfully deploy using GitHub actions.", "dagster_empl_first_target": "Hi robin - sorry for the trouble here, we\u2019ll get this sorted out for you shortly (should be no more than 30 minutes or so)", "dagster_empl_last_target": "You should be back up and running now - sorry about that!"}, {"question": "Hey folks, is it possible for an asset to take on optional asset dependencies? For example, I've got an asset that aggregates over a set of other assets (if those other assets have been materialized). I want this asset to run after all of the others, but not force the execution of the other assets if the user hasn't included them in the materialize call.", "replies": ["for example:\n```@asset\ndef asset1():\n    return 1\n\n@asset\ndef asset2():\n    return 2\n\n@asset\ndef my_asset(asset1: Optional, asset2: Optional):\n    sum = 0\n    if asset1:\n        sum+=asset1\n    if asset2:\n        sum+=asset2\n    return sum```", "Maybe you could achieve this with <https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#multi-asset-sensors|MultiAssetSensors>?", "Thanks for the pointer, will give it a shot!"], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "Yes, it is possible for an asset to take on optional asset dependencies. You can achieve this by defining your assets with optional dependencies and using MultiAssetSensors.", "dagster_empl_first_target": "for example:\n```@asset\ndef asset1():\n    return 1\n\n@asset\ndef asset2():\n    return 2\n\n@asset\ndef my_asset(asset1: Optional, asset2: Optional):\n    sum = 0\n    if asset1:\n        sum+=asset1\n    if asset2:\n        sum+=asset2\n    return sum```", "dagster_empl_last_target": "Thanks for the pointer, will give it a shot!"}, {"question": "So this is a super silly thing. But we run Dagit with multiple remote user code locations. We've noticed that recently (not sure exactly when) Dagit stopped expanding repositories in the sidebar. Screenshot in :thread:.", "replies": ["you can see the carets are pointing all which way", "If we select a single location from the filters, it does show the included assets.", "I have debug console logging enabled, but nothing shows up in the dev console when I click on the locations to expand/shrink assets.", "the image running Dagit here is\n```<http://docker.io/dagster/dagster-celery-k8s:1.3.1|docker.io/dagster/dagster-celery-k8s:1.3.1>```", "I\u2019ve just put up a fix for this, believe it or not. :slightly_smiling_face: <https://github.com/dagster-io/dagster/pull/14217>", "The issue is related to the space characters in the code location names", "I do believe it! Impeccable timing :slightly_smiling_face:", "oh wow you are totally right, I never would have picked that out", ":facepalm:", "Thanks for the report!", "and thanks for the patch!!"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "The issue with Dagit not expanding repositories in the sidebar when running with multiple remote user code locations is related to the space characters in the code location names. A fix for this issue has been implemented and can be found at https://github.com/dagster-io/dagster/pull/14217.", "dagster_empl_first_target": "you can see the carets are pointing all which way", "dagster_empl_last_target": "and thanks for the patch!!"}, {"question": "Hello Dagster :wave:\n\nIs it possible to dynamically update the `Definitions` of a user code without restarting it?\nFor example adding/updating the cron schedule of a job.", "replies": ["Hi Antoine, Generally no. `DynamicPartitionsDefinition` can be updated but I don\u2019t think any other definitions can be updated without reloading the code location.", "Ok, thanks a lot :pray:\nIs it possible to delete a partition key for a `DynamicPartition` ?\nLike in the \"release\" example used in the doc, deleting a release.", "yes, there is a `build_delete_request` method that functions jut like the `build_add_request` in that example", "Very nice, thanks again!"], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "While it's generally not possible to dynamically update the `Definitions` of user code without restarting it, `DynamicPartitionsDefinition` can be updated. Additionally, you can delete a partition key for a `DynamicPartition` using the `build_delete_request` method.", "dagster_empl_first_target": "Hi Antoine, Generally no. `DynamicPartitionsDefinition` can be updated but I don\u2019t think any other definitions can be updated without reloading the code location.", "dagster_empl_last_target": "yes, there is a `build_delete_request` method that functions jut like the `build_add_request` in that example"}, {"question": "Hello :wave: , I'm having an issue with a slow-running sensor that times out after 60 seconds.\nIt iterates over ~70 tables to check if a run is needed and yields a RunRequest for the required tables.\nI'd like to avoid creating 70 sensors to prevent clutter in Dagit and to simplify monitoring.\n\nI attempted using a cursor to filter tables, but it still results in the same DEADLINE_EXCEEDED error.\nI'd appreciate any suggestions on how to resolve this issue. Thanks in advance!", "replies": ["I'm looking at a similar situation and I think \"observable source assets' might be a better alternative to sensors here:\n<https://docs.dagster.io/concepts/assets/asset-observations#observable-source-assets>\nSo there would be new source assets that represent the upstream tables, those are \"observed\" on whatever schedule you want, then the downstream tables could use auto-materialize. \"DataVersion\" here could be a timestamp, hash of timestamp+row count, or whatever you're checking in the cursor.", "Hi Felix - do you have a sense from running it locally how long its taking after adding the cursor? is it just over the 60 second limit, or taking much longer?", "Hey <@U016C4E5CP8>, I initially thought using a cursor in a sensor would allow Dagster to resume after a timeout. However, it seems that when a sensor fails (even due to timeouts), the yielded RunRequests and cursor positions are ignored. I assume this is intended behavior?\n\nTo resolve this, I defined a BATCH_SIZE constant and adjusted the sensor logic to evaluate a limited number of tables at a time, updating the cursor accordingly. Now it works, but the sensor interval no longer reflects the table refresh interval (e.g., if the batch_size represents half of the tables, I need to halve the sensor interval).\n\nFor similar use cases, it would be helpful if:\n1. Sensors could run for 15 minutes (or at least 5 minutes) instead of 1 minute.\n2. Dagster could provide enhanced support for batched sensor evaluation by allowing users to customize the interval between batches vs the normal interval (minimum_interval_seconds). Ideally, the interval between batches could be set to 1 minute or less, while the normal interval would apply once all batches are complete. This could be achieved by enabling users to modify the next interval during a sensor run, either by skipping ticks for the next hour (assuming the normal interval is set to 1 minute) or by forcing a new tick in 60 seconds (assuming the normal interval is set to 1 hour). This flexibility would better accommodate use cases like ours.\nI'd love to hear your thoughts on these ideas. In any case, thank you for your help and your work on Dagster!", "yeah we're thinking about ways to improve this in the next few months to have fewer restrictions here", "I also think the observable source asset idea that Chris suggested is a good idea if that's an option for your use case", "And was also thinking of increasing the default to something like 5 minutes instead of 1 minute on a shorter timeframe (next few weeks)", "Hey <@U02R136SLDS>, thanks for the suggestion! Looks like the best approach for asset-based orchestration. However, for ops/jobs orchestration uses cases, like mine, I don't think it would work", "Is there a GH issue that we can subscribe to for this problem? I'm having a similar issue and my usual workaround using `DAGSTER_GRPC_TIMEOUT_SECONDS` doesn't seem to help here.", "I've also tried the new `num_submit_workers` setting in 1.3.6, but it resulted in this error:", "I would expect setting DAGSTER_GRPC_TIMEOUT_SECONDS to affect the 60 second timeout if you set it on the daemon. If you can paste the text of the error that you're seeing after you've triple checked that you've set that env var, we could take a look. We're working on a better solution than setting that env var though.", "if you can include the text and full stack trace of the error rather than a screenshot, those are easier for us to work with", "<https://github.com/dagster-io/dagster/pull/14516> is a fix for the constraint issue you ran into - thanks for reporting that, but i don't think it has any bearing on the timeouts, and the num_submit_workers setting won't affect whether or not this particular grpc call times out", "Yeah, sorry about the screenshot, I'll make sure to extract the stacktrace text next time.\nAs for the `DAGSTER_GRPC_TIMEOUT_SECONDS` I've checked my k8s deployment and the environment variable is definitely set:\n```env:\n  - name: DAGSTER_HOME\n    value: \"/dagster-home\"\n  - name: POSTGRES_HOSTNAME\n    value: \"postgres\"\n  - name: DAGSTER_GRPC_TIMEOUT_SECONDS\n    value: \"300\"```\n And this is what I saw in the logs when the sensor started:\n```2023-05-29T14:34:20.376278480Z INFO:dagster.daemon.SensorDaemon:Checking for new runs for sensor: rtpd_publications_sensor```\nand when it failed:\n```2023-05-29T14:37:05.672024247Z ERROR:dagster.daemon.SensorDaemon:Sensor daemon caught an error for sensor rtpd_publications_sensor```\nFull trace:\n```Traceback (most recent call last):\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_daemon/sensor.py\", line 520, in _process_tick_generator\n    yield from _evaluate_sensor(\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_daemon/sensor.py\", line 583, in _evaluate_sensor\n    sensor_runtime_data = code_location.get_external_sensor_execution_data(\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_core/host_representation/code_location.py\", line 845, in get_external_sensor_execution_data\n    return sync_get_external_sensor_execution_data_grpc(\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_api/snapshot_sensor.py\", line 63, in sync_get_external_sensor_execution_data_grpc\n    api_client.external_sensor_execution(\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_grpc/client.py\", line 388, in external_sensor_execution\n    chunks = list(\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_grpc/client.py\", line 184, in _streaming_query\n    self._raise_grpc_exception(\n  File \"/app/.venv/lib/python3.10/site-packages/dagster/_grpc/client.py\", line 140, in _raise_grpc_exception\n    raise DagsterUserCodeUnreachableError(\ndagster._core.errors.DagsterUserCodeUnreachableError: Could not reach user code server. gRPC Error code: UNAVAILABLE```\nAs you can see, the time between the sensor starting and it failing is more than 60s, but it's also not 300s. I tried bumping that to 600s and I got a similar result (though I don't have the exact timeout on hand at the moment). I'm happy to test this using \"Test Sensor\" if that should also respect the environment variable setting.", "That doesn\u2019t look like a timeout to me - that indicates that it can\u2019t connect to the server, maybe it\u2019s crashing or OOMing in the middle of the request?", "Well, the sensor daemon and the code location are in the same process, so I'm not sure what could have crashed without both going down", "But I agree that this doesn\u2019t look like a timeout now that you mention it\u2026 That should have been `DEADLINE_EXCEEDED` Perhaps some other resource is being exhausted here, but there\u2019s nothing in the logs to go on", "We generally recommend running the code server in a separate pod when running at scale in production", "Sure, but I have a tiny setup and running things in the same container is more cost effective for me at this time.", "Fair enough - I think it would be a lot easier to understand why the server became unreachable if it were more isolated though. Is trying with that an option to see if that helps get to the bottom of what\u2019s going on?", "Yeah, I can definitely do that for debugging purposes", "Will using \u201cTest Sensor\u201d allow me to reproduce this?", "I mean, should \u201cTest Sensor\u201d behave the same way as a normal sensor run?", "It\u2019s worth a try - probably depends on the nature of the problem since it\u2019s not exactly the same setup", "right, I believe the sensor would run from the dagit process in that case?", "I believe that\u2019s right, yeah", "I bet I know what is happening here - the daemon periodically reloads code servers if they aren\u2019t being run separately, the tick is now long enough that that reload will happen in the middle of the tick and the old server shuts down", "ah, yeah that makes sense; what\u2019s the frequency on the code server reload?", "I think its every 60 seconds but the servers stay around for a bit longer", "running the code server separately is probably the easiest workaround with what's in master currently", "Ok thanks and in terms of tracking a fix for this, should I create a GH issue?", "that would be great, yeah", "<https://github.com/dagster-io/dagster/issues/14523>"], "is_dagster_empl": [false, true, false, true, true, true, false, false, false, true, true, true, false, true, false, false, true, false, true, false, false, false, true, false, true, true, false, true, true, false, true, false], "gpt4_replies_target": "The user can resolve the issue of a slow-running sensor by defining a BATCH_SIZE constant and adjusting the sensor logic to evaluate a limited number of tables at a time, updating the cursor accordingly. However, this would require adjusting the sensor interval to reflect the table refresh interval. Alternatively, using \"observable source assets\" could be a better alternative for this situation.", "dagster_empl_first_target": "Hi Felix - do you have a sense from running it locally how long its taking after adding the cursor? is it just over the 60 second limit, or taking much longer?", "dagster_empl_last_target": "that would be great, yeah"}, {"question": "Hi All,\n\nI have dagster installed through helm on an EKS cluster. When ever I run a job I get the error shown. Any idea why this happens? Appreciate any help", "replies": ["Hi Jon - you may need to upgrade the dagster version that\u2019s being used by your job code to match the version that\u2019s being used by the helm chart ", "Thanks for your response. I went ahead and updated everything to match. The Dagster app version `1.3.3`. And then for the job code I also updated the dependencies. I am still getting the same error.\n\nrequirements.txt:\n```dagster==1.3.3\ndagster-graphql==1.3.3\ndagit==1.3.3\ndagster-postgres==0.19.3\ndagster-dbt==0.19.3\ndagster-pandas==0.19.3\ndagster-docker==0.19.3\ndagster-airbyte==0.19.3\ndagster-shell==0.19.3\ndbt-core==1.5.0\ndbt-postgres==1.5.0\ndbt-databricks==1.5.1```", "Would it be possible to run 'pip freeze' in your job image?", "```Received unexpected config entries \"['dequeue_num_workers', 'dequeue_use_threads']\" at the root. Expected: \"['dequeue_interval_seconds', 'max_concurrent_runs', 'tag_concurrency_limits'].\"```\nThe config entries that it mentions there that are missing are definitely present in 1.3.3. - so I just want to be absolutely certain that your job code is running that version", "To make sure I understand this is what you mean? I added that in the dockerfile", "I would do something like `docker run &lt;image name&gt; /bin/bash` after building the image", "then in the shell, run 'pip freeze'", "oh oh gotcha", "trying", "Here is the output", "Fixed now. I appreciate your help. Solution: I went back and made sure all dependencies were present"], "is_dagster_empl": [true, false, true, true, false, true, true, false, false, false, false], "gpt4_replies_target": "The error was due to a version mismatch between the dagster used by the job code and the helm chart. The user fixed the issue by ensuring all dependencies were present and matched the Dagster app version `1.3.3`.", "dagster_empl_first_target": "Hi Jon - you may need to upgrade the dagster version that\u2019s being used by your job code to match the version that\u2019s being used by the helm chart ", "dagster_empl_last_target": "then in the shell, run 'pip freeze'"}, {"question": "Hi,\nI am using `dagster project scaffold` to start a new dagster project which uses `repository`. I see in the <https://docs.dagster.io/guides/dagster/software-defined-assets#connecting-assets-to-external-services|docs> that `Definitions` is used to load assets.\nWhat is the connections between these two and which one to use?", "replies": ["Hi Akshay. We recently replaced `repository` with `Definitions` for a couple of reasons:\n\u2022 repositories cause confusion as they collide with GitHub repositories\n\u2022 multiple repositories can exist at the same code location which causes more organizational burden\nI'd recommend using `Definitions` instead of `repository`, you can follow the code example here: <https://docs.dagster.io/concepts/code-locations#defining-code-locations>", "Thank you, will have a look."], "is_dagster_empl": [true, false], "gpt4_replies_target": "Dagster has replaced `repository` with `Definitions` to avoid confusion with GitHub repositories and to reduce organizational burden, hence it is recommended to use `Definitions` for loading assets in your new project.", "dagster_empl_first_target": "Hi Akshay. We recently replaced `repository` with `Definitions` for a couple of reasons:\n\u2022 repositories cause confusion as they collide with GitHub repositories\n\u2022 multiple repositories can exist at the same code location which causes more organizational burden\nI'd recommend using `Definitions` instead of `repository`, you can follow the code example here: <https://docs.dagster.io/concepts/code-locations#defining-code-locations>", "dagster_empl_last_target": "Hi Akshay. We recently replaced `repository` with `Definitions` for a couple of reasons:\n\u2022 repositories cause confusion as they collide with GitHub repositories\n\u2022 multiple repositories can exist at the same code location which causes more organizational burden\nI'd recommend using `Definitions` instead of `repository`, you can follow the code example here: <https://docs.dagster.io/concepts/code-locations#defining-code-locations>"}, {"question": "Hi,\n\nI am trying to test an ops that access the run_config\n\nMy ops looks like:\n```@op(out={\n        'start_datetime': Out(dagster_type=pd.Timestamp),\n    })\ndef calculate_build_times(context: OpExecutionContext):\n    job_frequency = context.op_config['job_frequency']\n\tstart_datetime = pd.Timestamp(context.run_config.get('job_start_datetime'), tz='UTC') \\\n\t\tif context.run_config.get('job_start_datetime') else pd.Timestamp.utcnow().floor('D')\n\t\t\n\treturn start_datetime```\nMy test is:\n```def test_calculate_build_times():\n    with build_op_context(\n            op_config={\n                'job_frequency': 'DAILY',\n                'job_start_datetime': '2022-12-01',\n            }) as context:\n        start_datetime = calculate_build_times(context)\n\n    assert start_datetime == pd.Timestamp('2022-12-01', tz='UTC')```\nThe test fails because `context.run_config.get('job_start_datetime')` returns `None`\n\nHow can I set the `run_config` in the context?", "replies": ["Hey Thierry--you're setting config correctly in the `build_op_context` call. In your op, you'll need to replace every `context.run_config` with `context.op_config`.", "<@U028M11QNDD> Maybe I wasn't clear enough in exposing the problem, the config `job_start_datetime` can be overridden in the Launchpad UI when I want to run the job manually, hence the use of `run_config`.\nI'm trying to test different scenario with for my ops based on the configuration passed in the Launchpad.\nThe way I by passed the problem is to have a function `get_run_config` that access the run_config, then I can mock that function to return the type of config I want but I was wondering whether there was a more elegant way to address my problem\nDo you have any other idea?", "Hi Thierry. Run config gets mapped down to the specific op, so specifying op config within `build_op_context` and accessing config within your op via `context.op_config` is the recommended pattern.\n\nAny run config that is specified via Dagit's launchpad will be applied to the `context.op_config` object.", "Thanks, I tested it and it didn't work so I must have done something wrong. I will test again tomorrow and let you know.", "You might need to define a `config_schema` field on your op", "Thanks, I didn't have that for sure"], "is_dagster_empl": [true, false, true, false, true, false], "gpt4_replies_target": "In your op, replace every `context.run_config` with `context.op_config`. Any run config specified via Dagit's launchpad will be applied to the `context.op_config` object. Also, you might need to define a `config_schema` field on your op.", "dagster_empl_first_target": "Hey Thierry--you're setting config correctly in the `build_op_context` call. In your op, you'll need to replace every `context.run_config` with `context.op_config`.", "dagster_empl_last_target": "You might need to define a `config_schema` field on your op"}, {"question": "Hi all,\n\nWe're testing out Dagster and are using it with a grpc server to serve a repository. We have a folder with different files containing asset definitions. We load these dynamically into a list of repository objects:\n```@repository()\ndef test_repo():\n    repository_objects = get_repository_objects()\n    return repository_objects```\nSo when the gRPC server is started these can all be fetched by dagit. We'd like to now copy new assets into the asset file directory and have the newly added files synced to dagit. It only seems to pick up the new assets when we restart the gRPC server. Is there any way to have the gRPC server pick up the changes to the repo without restarting? I could not find anything about this in the docs.", "replies": ["Hey Oscar - if your Python code hasn't changed, one possibility is to return a RepositoryData object from the repository instead - there's an example here <https://docs.dagster.io/_apidocs/repositories#dagster.repository>\n\nIf your python code has changed and needs to be re-imported ,the only way to pick up those changes currently is to restart the gRPC server", "Thanks Daniel. We use this pattern since we're moving away from Airflow where we have a side-cart syncing dags. All our assets are generated from config. Do you know what the common CICD pattern is for Dagster? A git repo with the code that triggers a restart of the gRPC server on change?", "Yeah, some of the details depend on how you're deploying Dagster (whether you're using Docker images or not) but I think that would be a common setup", "<@U016C4E5CP8> What do you mean with 'python code changes' while using RepositoryData. Don't we finally still need to use the @repository decorator? It seems this decorator caches the jobs etc so I don't see how this would allow us to add and modify jobs without having to restart the gRPC server.", "the decorator doesn't cache the jobs - it returns an instance of that RepositoryData object, which allows you to control whether it caches", "<@U016C4E5CP8>do we need to return something different?\n\nNot entirely sure what a pipeline definition is.. We're just returning our defined assets and schedules as a list via the `get_all_pipelines` method. We get an error like:", "you can use get_all_jobs instead of get_all_pipelines but you'll hit the same error -\n\nIt looks like we're going to need to replicate this logic from the repository decorator and return that resolved job object instead: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/definitions/repository_definition.py#L847-L859>", "If you can share what you have now we could take a look - this may be the first time somebody has tried to use RepositoryData and define_asset_job together", "but it looks like it also needs a list of AssetsDefinitions and SourceAssets and then those get passed into a resolve function that produces a job definition", "<@U016C4E5CP8> Here we created a minimal example that reproduces the error.\n```import os\n\nfrom dagster import (\n    op, build_schedule_from_partitioned_job, Output,\n    DefaultScheduleStatus, AssetsDefinition, graph, DailyPartitionsDefinition, define_asset_job,\n    AssetSelection, Nothing, RepositoryData, repository,\n)\n\nfrom dagster_assets.ops import extract\n\n\n@op\ndef etl_tnt_dp_skeleton_skeletontable_end(test):\n    return Output(None, metadata={\"row_count\": 1})\n\n@graph\ndef etl_tnt_dp_skeleton_skeletontable() -&gt; Nothing:\n    r_extract = extract()\n    r_end = etl_tnt_dp_skeleton_skeletontable_end(r_extract)\n    return r_end\n\npartitions_def = DailyPartitionsDefinition(start_date=\"2022-04-01\")\n\netl_tnt_dp_skeleton_skeletontable_asset = AssetsDefinition.from_graph(\n    etl_tnt_dp_skeleton_skeletontable,\n    partitions_def=partitions_def\n)\n\netl_tnt_dp_skeleton_skeletontable_job = define_asset_job(\n    \"etl_tnt_dp_skeleton_skeletontable_job\",\n    selection=AssetSelection.assets(etl_tnt_dp_skeleton_skeletontable_asset),\n    partitions_def=partitions_def)\n\netl_tnt_dp_skeleton_skeletontable_schedule = build_schedule_from_partitioned_job(\n    etl_tnt_dp_skeleton_skeletontable_job,\n    default_status=DefaultScheduleStatus.RUNNING)\n\n\nclass ComplexRepositoryData(RepositoryData):\n    def __init__(self):\n        pass\n\n    def get_all_pipelines(self):\n        return self.get_repository_objects()\n\n    def get_repository_objects(self):\n        return [etl_tnt_dp_skeleton_skeletontable_job, etl_tnt_dp_skeleton_skeletontable_schedule]\n\n@repository\ndef tnt_dp():\n    return ComplexRepositoryData()```\n", "We are not yet that comfortable with all the nomenclature of Dagster so it might be that we are doing something obvious wrong :slightly_smiling_face:", "Try this:\n\n```class ComplexRepositoryData(RepositoryData):\n    def __init__(self):\n        pass\n\n    def get_all_pipelines(self):\n        return [\n            etl_tnt_dp_skeleton_skeletontable_job.resolve(\n                assets=[etl_tnt_dp_skeleton_skeletontable_asset],\n                source_assets=[],\n            )\n        ]\n\n    def get_all_schedules(self):\n        return [etl_tnt_dp_skeleton_skeletontable_schedule]```", "bit odd that you have to specify the asset twice", "You asked earlier what \"'python code changes'\" means - what that means is that if you had to make any changes to that .py file, you would need to restart the server for those changes to be applied. However, if those functions pulled from a file or something and the contents of that file changed (but not the python code), then you would not need to restart the server.", "Great, that clarifies the 'python code changes', thanks. This example seems to work! Thanks a lot for the quick and helpful feedback. Will try to get this to work tomorrow since it's end of day here. Will get back to you", "We got it to work sooner then expected :slightly_smiling_face: Does Dagit have a polling interval to pick up the changes coming from the repository server or do you need to do a manual reload?", "You need to do a manual reload currently", "either from the UI or over graphql", "If you do restart the server, dagit will automatically pick up the changes", "I guess you mean this w.r.t graphql? <https://docs.dagster.io/concepts/dagit/graphql-client#reloading-all-repositories-in-a-repository-location>", "yeah", "Thanks a lot!"], "is_dagster_empl": [true, false, true, false, true, false, true, true, true, false, false, true, true, true, false, false, true, true, true, false, true, false], "gpt4_replies_target": "To have the gRPC server pick up changes to the repo without restarting, you can return a RepositoryData object from the repository. However, if your Python code has changed and needs to be re-imported, the only way to pick up those changes currently is to restart the gRPC server. Dagit does not have a polling interval to pick up changes from the repository server, so a manual reload is required.", "dagster_empl_first_target": "Hey Oscar - if your Python code hasn't changed, one possibility is to return a RepositoryData object from the repository instead - there's an example here <https://docs.dagster.io/_apidocs/repositories#dagster.repository>\n\nIf your python code has changed and needs to be re-imported ,the only way to pick up those changes currently is to restart the gRPC server", "dagster_empl_last_target": "yeah"}, {"question": "Hi, I have a question on the DockerRunLauncher. This may have been asked before but Slack doesn't show messages older than 90 days...Specifically, I use a host for Dagit and another for the user code server. If I use DockerRunLauncher to execute jobs, are the containers created on the Dagit host or the user code host? It wasn't very clear in the documentation <https://docs.dagster.io/deployment/guides/docker> I assume it is the user code server but just want to confirm. Thank you.", "replies": ["Hi Jing - are you running the dagster daemon in a container as well?", "<https://docs.dagster.io/deployment/guides/docker#multi-container-docker-deployment> lays out when you would have one - if you have a run queue set up on your daemon, every run will be launched using the daemon container on the same machine where that container is running.\n\nIf you don't have a run queue, then runs kicked off manually from dagit will be launched from the dagit container, but other runs (schedules and sensors) will be launched from the daemon container", "Thanks Daniel! Sounds like I need 3 host, one for dagit, one for the daemon with a queue and one for the user code vm. In this case, all the job containers are created on the daemon host?", "I would expect them to also work on a single host too (the daemon in particular doesn\u2019t need to run as a server)", "It\u2019ll either be on the daemon machine or the dagit machine - my comment just above lays out the different cases"], "is_dagster_empl": [true, true, false, true, true], "gpt4_replies_target": "The DockerRunLauncher will execute jobs and create containers either on the Dagit host or the user code host, depending on whether you have a run queue set up on your daemon. If you have a run queue, every run will be launched using the daemon container on the same machine where that container is running. If you don't have a run queue, then runs kicked off manually from Dagit will be launched from the Dagit container, but other runs (schedules and sensors) will be launched from the daemon container.", "dagster_empl_first_target": "Hi Jing - are you running the dagster daemon in a container as well?", "dagster_empl_last_target": "It\u2019ll either be on the daemon machine or the dagit machine - my comment just above lays out the different cases"}, {"question": "Hi team, happy 2023! We ran into a weird `ChildProcessCrashException` with `exit code -9`  for one job consistently. I am wondering if anyone has seen this before?", "replies": ["```Multiprocess executor: child process for step run_forecast_xp_cng_conversion_rate_nc unexpectedly exited with code -9\ndagster._core.executor.child_process_executor.ChildProcessCrashException\n\nStack Trace:\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/multiprocess.py\", line 210, in execute\n    event_or_none = next(step_iter)\n,  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/multiprocess.py\", line 324, in execute_step_out_of_process\n    for ret in execute_child_process_command(multiproc_ctx, command):\n,  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/child_process_executor.py\", line 163, in execute_child_process_command\n    raise ChildProcessCrashException(exit_code=process.exitcode)```", "```dagster._core.executor.child_process_executor.ChildProcessCrashException\n\nStack Trace:\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/multiprocess.py\", line 210, in execute\n    event_or_none = next(step_iter)\n,  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/multiprocess.py\", line 324, in execute_step_out_of_process\n    for ret in execute_child_process_command(multiproc_ctx, command):\n,  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/child_process_executor.py\", line 163, in execute_child_process_command\n    raise ChildProcessCrashException(exit_code=process.exitcode)\n\ndagster._check.CheckError: Invariant failed. Description: Attempted to mark step run_forecast_xp_cng_conversion_rate_nc as complete that was not known to be in flight\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/execution/api.py\", line 1033, in pipeline_execution_iterator\n    for event in pipeline_context.executor.execute(pipeline_context, execution_plan):\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/executor/multiprocess.py\", line 240, in execute\n    active_execution.handle_event(step_failure_event)\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/execution/plan/active.py\", line 400, in handle_event\n    self.mark_failed(step_key)\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/execution/plan/active.py\", line 342, in mark_failed\n    self._mark_complete(step_key)\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/execution/plan/active.py\", line 387, in _mark_complete\n    check.invariant(\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_check/__init__.py\", line 1470, in invariant\n    raise CheckError(f\"Invariant failed. Description: {desc}\")```", "exit code `-9` means the process was killed via the `SIGKILL` signal, likely by the `oom_killer` which happens when the process tries to use too much memory.", "Thanks Alex!"], "is_dagster_empl": [false, false, true, false], "gpt4_replies_target": "The `ChildProcessCrashException` with `exit code -9` you're encountering is likely due to the process being killed via the `SIGKILL` signal, often caused by the `oom_killer` when the process tries to use too much memory.", "dagster_empl_first_target": "exit code `-9` means the process was killed via the `SIGKILL` signal, likely by the `oom_killer` which happens when the process tries to use too much memory.", "dagster_empl_last_target": "exit code `-9` means the process was killed via the `SIGKILL` signal, likely by the `oom_killer` which happens when the process tries to use too much memory."}, {"question": "Can we have multiple code locations in workspace.yaml? Documentation says we can but when I try to provide a python_package and python_file elements, dagit is not able keep the code server alive. The code server for package entry tries to start but is terminated subsequently. the python file entry never starts. When I try each option individually it works.", "replies": ["Has anyone tried this before? Have code locations from both package and file?", "Hi Daya. I've been able to load Dagit successfully with both a package and a python file. Is there any error message that shows up when the dagster attempts to load the package?", "Thanks for your reply <@U028M11QNDD>. I don't see any error. It just says that the code server is being shutdown. I never get to a point where it says serving dagit on...", "So I tried to move out of workspace.yaml and moved away from package initialization as well and created a file which created a definitions object in global scope. Created 2 files like this in the base working directory. When I call dagit with multiple - f tags it doesn't work. Each one individually works. Both dagit and Dagster are on 1.1.7 and python version is 3.7.5", "Odd... I'm able to successfully run the 2 files in Dagit as you mentioned above using 1.1.7. I'm surfacing your question to the team, hopefully can get back to you soon. What was the log output when calling dagit with mutiple -f tags?", "Thanks. The logs have nothing useful.\n\nStarting code server for file1\nShutting down code server for file1\n\nNo entry for the second file at all.", "Tried changing the log level to trace. Still nothing", "Hi Daya - what version of grpcio do you have installed? (If you run `pip freeze | grep grpcio`)", "1.48.0", "Try installing 1.47.2 instead", "1.48  was yanked for causing mysterious hangs", "Oh ok.. Trying now. Thanks a lot Daniel. Will get back to you once done", "This is nice. It's working now! Thanks a lot Daniel. Issues like this are so difficult to debug."], "is_dagster_empl": [false, true, false, false, true, false, false, true, false, true, true, false, false], "gpt4_replies_target": "The issue with multiple code locations in workspace.yaml was resolved by downgrading the version of grpcio from 1.48.0 to 1.47.2, as the former was causing mysterious hangs.", "dagster_empl_first_target": "Hi Daya. I've been able to load Dagit successfully with both a package and a python file. Is there any error message that shows up when the dagster attempts to load the package?", "dagster_empl_last_target": "1.48  was yanked for causing mysterious hangs"}, {"question": "Hi!\nHow can I record materialization metadata (<https://docs.dagster.io/concepts/assets/software-defined-assets#recording-materialization-metadata>)\nwhile using assets without return values (<https://docs.dagster.io/tutorial/assets/non-argument-deps>) ?", "replies": ["You can return `Output(None, metadata={...})` from the asset function", "oh, that makes a lot of sense, thanks so much!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "You can record materialization metadata while using assets without return values by returning `Output(None, metadata={...})` from the asset function.", "dagster_empl_first_target": "You can return `Output(None, metadata={...})` from the asset function", "dagster_empl_last_target": "oh, that makes a lot of sense, thanks so much!"}, {"question": "Got an `ExperimentalWarning` when adding an executor to my code location's `Definitions` -- is there a more recommended way of defining an executor for assets?", "replies": ["we're actually getting rid of that warning in this week's release", "nice!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The warning you're seeing when adding an executor to your code location's `Definitions` is being removed in the upcoming week's release.", "dagster_empl_first_target": "we're actually getting rid of that warning in this week's release", "dagster_empl_last_target": "we're actually getting rid of that warning in this week's release"}, {"question": "Is there a way to get resources by name from an `OpExecutionContext` ? i.e. instead of `context.resources.foo` , can I use something like `context.resources.get(\"foo\")` ?", "replies": ["nevermind! it's a namedtuple, so you can do this via `getattr`", "glad you figured it out - that's what I was going to suggest"], "is_dagster_empl": [false, true], "gpt4_replies_target": "Yes, you can get resources by name from an `OpExecutionContext` using `getattr` as it's a namedtuple.", "dagster_empl_first_target": "glad you figured it out - that's what I was going to suggest", "dagster_empl_last_target": "glad you figured it out - that's what I was going to suggest"}, {"question": "Hi, did not see any relevant course about dagster, which one are recommended ?\nAny article / docs to see?", "replies": ["Hi!\n\nThe <https://docs.dagster.io/tutorial/introduction|docs and tutorial> are your best resources. Is there anything in particular you're looking for", "gcs integration", "Notably, you might want to look at what you can do out of the box with our <https://docs.dagster.io/_apidocs/libraries/dagster-gcp?_gl=1*9s9b46*_ga*NTI1MDk1OTMwLjE2ODEzOTk5MjI.*_ga_84VRQZG7TV*MTY4MzQxMDI0OS44LjAuMTY4MzQxMDI0OS4wLjAuMA..|GCP integration library>, including working with GCS", "I would like to do end to end pipeline since extraction to bq + Dbt ", "Join our event on Wednesday.  We demo exactly that... <https://dagster.io/events/analytics-event-may-2023|https://dagster.io/events/analytics-event-may-2023>", "Nice , this event will be recorded ??", "Yes, we will record the event - register to get the recording link.  The event will also have a live Q&amp;A but if you want to drop any high level questions you have here, I will make sure we answer them in the session."], "is_dagster_empl": [true, false, true, false, false, false, false], "gpt4_replies_target": "The best resources to learn about Dagster are the official documentation and tutorials, particularly the GCP integration library for working with GCS. There's also an upcoming event that will demo an end-to-end pipeline from extraction to BigQuery and DBT, which will be recorded for later viewing.", "dagster_empl_first_target": "Hi!\n\nThe <https://docs.dagster.io/tutorial/introduction|docs and tutorial> are your best resources. Is there anything in particular you're looking for", "dagster_empl_last_target": "Notably, you might want to look at what you can do out of the box with our <https://docs.dagster.io/_apidocs/libraries/dagster-gcp?_gl=1*9s9b46*_ga*NTI1MDk1OTMwLjE2ODEzOTk5MjI.*_ga_84VRQZG7TV*MTY4MzQxMDI0OS44LjAuMTY4MzQxMDI0OS4wLjAuMA..|GCP integration library>, including working with GCS"}, {"question": "Is is possible to retry a run initiated by a sensor?  I tried including the below in mine but the runs don't appear to be retrying on failure.\n```yield RunRequest(\n    run_config={\"ops\": {...}},\n    tags={\n        \"dagster/max_retries\": \"3\",\n        \"dagster/retry_strategy\": \"ALL_STEPS\"}\n)```", "replies": ["Nevermind, this appears to be because I'm using SQLite for event log storage."], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, it is possible to retry a run initiated by a sensor, but the user's issue appears to be due to using SQLite for event log storage.", "dagster_empl_first_target": "Nevermind, this appears to be because I'm using SQLite for event log storage.", "dagster_empl_last_target": "Nevermind, this appears to be because I'm using SQLite for event log storage."}, {"question": "Struggling with a weird issue creating a sensor that monitors GSheets. I'm getting a 404 error on the sensor when it tries to get the last modified date, but only on the sensor run. I have a test that runs successfully locally AND on the code location k8s pod, and I know it's using the correct service account credentials on the sensor from logging. Am I missing something about the context that the sensor runs in? Does it not run actually on my code location pod?", "replies": ["ohhhhh man I got it", "source asset metadata doesn't return raw values, it returns a class of the value type"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue you're experiencing might be due to the fact that the source asset metadata doesn't return raw values, but rather it returns a class of the value type.", "dagster_empl_first_target": "ohhhhh man I got it", "dagster_empl_last_target": "source asset metadata doesn't return raw values, it returns a class of the value type"}, {"question": "upgrading to dagster 1.3.10 and now some tests that use `build_op_context` are failing with the following error:\n```alembic.util.exc.CommandError: Path doesn't exist: '/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/alembic'.  Please use the 'init' command to create a new scripts folder.```\nfeels like alembic shouldn't need to run anything when we're using `build_op_context`, but maybe I'm missing something? Including the whole stack trace and code in :thread:", "replies": ["```/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/execution/context/invocation.py:712: in build_op_context\n    return UnboundOpExecutionContext(\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/execution/context/invocation.py:94: in __init__\n    self._instance = self._instance_cm.__enter__()\n/Users/zach/.pyenv/versions/3.9.5/lib/python3.9/contextlib.py:117: in __enter__\n    return next(self.gen)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/execution/api.py:279: in ephemeral_instance_if_missing\n    with DagsterInstance.ephemeral() as ephemeral_instance:\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/instance/__init__.py:470: in ephemeral\n    run_storage=InMemoryRunStorage(preload=preload),\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/runs/in_memory.py:35: in __init__\n    stamp_alembic_rev(alembic_config, self._held_conn)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/sql.py:68: in stamp_alembic_rev\n    stamp(alembic_config, rev)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/alembic/command.py:644: in stamp\n    script = ScriptDirectory.from_config(config)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/alembic/script/base.py:221: in from_config\n    return ScriptDirectory(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &lt;alembic.script.base.ScriptDirectory object at 0x2a5e09490&gt;, dir = PosixPath('/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/alembic'), file_template = '%(rev)s_%(slug)s', truncate_slug_length = None, version_locations = None, sourceless = False, output_encoding = 'utf-8'\ntimezone = None, hook_config = {'black.entrypoint': 'black', 'black.options': '--line-length 100 --target-version py36 --target-version py37 --target...s/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/runs/sqlite/alembic', ...}, recursive_version_locations = False\nmessaging_opts = immutabledict({'quiet': False})\n\n    def __init__(\n        self,\n        dir: str,  # noqa\n        file_template: str = _default_file_template,\n        truncate_slug_length: Optional[int] = 40,\n        version_locations: Optional[List[str]] = None,\n        sourceless: bool = False,\n        output_encoding: str = \"utf-8\",\n        timezone: Optional[str] = None,\n        hook_config: Optional[Mapping[str, str]] = None,\n        recursive_version_locations: bool = False,\n        messaging_opts: MessagingOptions = cast(\n            \"MessagingOptions\", util.EMPTY_DICT\n        ),\n    ) -&gt; None:\n        self.dir = dir\n        self.file_template = file_template\n        self.version_locations = version_locations\n        self.truncate_slug_length = truncate_slug_length or 40\n        self.sourceless = sourceless\n        self.output_encoding = output_encoding\n        self.revision_map = revision.RevisionMap(self._load_revisions)\n        self.timezone = timezone\n        self.hook_config = hook_config\n        self.recursive_version_locations = recursive_version_locations\n        self.messaging_opts = messaging_opts\n    \n        if not os.access(dir, os.F_OK):\n&gt;           raise util.CommandError(\n                \"Path doesn't exist: %r.  Please use \"\n                \"the 'init' command to create a new \"\n                \"scripts folder.\" % os.path.abspath(dir)\n            )\nE           alembic.util.exc.CommandError: Path doesn't exist: '/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/alembic'.  Please use the 'init' command to create a new scripts folder.\n\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/alembic/script/base.py:102: CommandError```", "Being called like this:\n```op_context = build_op_context(\n            resources={\"s3\": mock_s3, \"file_manager\": mock_file_manager},\n            op_config=test_run_config,\n        )```", "hmm I am using `pyfakefs`, maybe that's causing an issue?", "Yeah definitely the result of `pyfakefs`. Will need to use the `tmp_path` fixture instead."], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "The error is likely due to the use of `pyfakefs`. You should consider using the `tmp_path` fixture instead.", "dagster_empl_first_target": "```/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/execution/context/invocation.py:712: in build_op_context\n    return UnboundOpExecutionContext(\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/execution/context/invocation.py:94: in __init__\n    self._instance = self._instance_cm.__enter__()\n/Users/zach/.pyenv/versions/3.9.5/lib/python3.9/contextlib.py:117: in __enter__\n    return next(self.gen)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/execution/api.py:279: in ephemeral_instance_if_missing\n    with DagsterInstance.ephemeral() as ephemeral_instance:\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/instance/__init__.py:470: in ephemeral\n    run_storage=InMemoryRunStorage(preload=preload),\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/runs/in_memory.py:35: in __init__\n    stamp_alembic_rev(alembic_config, self._held_conn)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/sql.py:68: in stamp_alembic_rev\n    stamp(alembic_config, rev)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/alembic/command.py:644: in stamp\n    script = ScriptDirectory.from_config(config)\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/alembic/script/base.py:221: in from_config\n    return ScriptDirectory(\n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _\n\nself = &lt;alembic.script.base.ScriptDirectory object at 0x2a5e09490&gt;, dir = PosixPath('/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/alembic'), file_template = '%(rev)s_%(slug)s', truncate_slug_length = None, version_locations = None, sourceless = False, output_encoding = 'utf-8'\ntimezone = None, hook_config = {'black.entrypoint': 'black', 'black.options': '--line-length 100 --target-version py36 --target-version py37 --target...s/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/runs/sqlite/alembic', ...}, recursive_version_locations = False\nmessaging_opts = immutabledict({'quiet': False})\n\n    def __init__(\n        self,\n        dir: str,  # noqa\n        file_template: str = _default_file_template,\n        truncate_slug_length: Optional[int] = 40,\n        version_locations: Optional[List[str]] = None,\n        sourceless: bool = False,\n        output_encoding: str = \"utf-8\",\n        timezone: Optional[str] = None,\n        hook_config: Optional[Mapping[str, str]] = None,\n        recursive_version_locations: bool = False,\n        messaging_opts: MessagingOptions = cast(\n            \"MessagingOptions\", util.EMPTY_DICT\n        ),\n    ) -&gt; None:\n        self.dir = dir\n        self.file_template = file_template\n        self.version_locations = version_locations\n        self.truncate_slug_length = truncate_slug_length or 40\n        self.sourceless = sourceless\n        self.output_encoding = output_encoding\n        self.revision_map = revision.RevisionMap(self._load_revisions)\n        self.timezone = timezone\n        self.hook_config = hook_config\n        self.recursive_version_locations = recursive_version_locations\n        self.messaging_opts = messaging_opts\n    \n        if not os.access(dir, os.F_OK):\n&gt;           raise util.CommandError(\n                \"Path doesn't exist: %r.  Please use \"\n                \"the 'init' command to create a new \"\n                \"scripts folder.\" % os.path.abspath(dir)\n            )\nE           alembic.util.exc.CommandError: Path doesn't exist: '/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/dagster/_core/storage/alembic'.  Please use the 'init' command to create a new scripts folder.\n\n/Users/zach/Documents/empirico/projects/etxlib/.venvs/dagster3/lib/python3.9/site-packages/alembic/script/base.py:102: CommandError```", "dagster_empl_last_target": "Yeah definitely the result of `pyfakefs`. Will need to use the `tmp_path` fixture instead."}, {"question": "I have a question about how I can set up some declarative scheduling that archives the following:\n\nLets say I have a date partitioned software defined asset 'A'. The asset has a dependency on an upstream software defined asset and also the previous dates partition of itself.\n\nWe decide we want to backfill the upstream assets from date 0 to date 5. The partitions for asset 'A' from date 0-5 are now stale. However, I dont want them to all start running at the same time since if date 3 starts before date 2 finishes it will need to run again once date 2 finishes. How could I make it so that date 3 does not start until date 2 is no longer stale.", "replies": ["After doing a bit more digging it seems that if a dependency is not reconciled the job will not start so it should just work the way I want it to out of the box."], "is_dagster_empl": [false], "gpt4_replies_target": "If a dependency is not reconciled, the job will not start, so it should work as you want it to by default, without all partitions starting at the same time.", "dagster_empl_first_target": "After doing a bit more digging it seems that if a dependency is not reconciled the job will not start so it should just work the way I want it to out of the box.", "dagster_empl_last_target": "After doing a bit more digging it seems that if a dependency is not reconciled the job will not start so it should just work the way I want it to out of the box."}, {"question": "*Resolved:* See thread\n\nHi all!\n\nI'm rather new to dagster and am trying to get an asset to use a custom BigQuery I/O manager. I got the I/O manager to work with my creds by forking the <https://github.com/dagster-io/quickstart-gcp|quickstart repo for GCP>.\n\nMy company uses a repository instead of the definitions that the quickstart repo uses.\n\nI've got the simple asset below to run and to write BQ successfully but only when putting a definition in the asset script.\n\n```from dagster import asset\nimport pandas as pd\n\n@asset(\n    group_name=\"new_group\",\n    required_resource_keys={\"bq_io_manager\"}\n)\ndef totango_accounts() -&gt; pd.DataFrame:\n    data = {\n        'name': ['name', 'name_2'],\n        'number': ['+123', '+456']\n    }\n\n    return pd.DataFrame(data)```\nAnd repository.py looks like:\n```    resource_defs = {\n        \"io_manager\": s3_pickle_io_manager.configured(\n            {\"s3_bucket\": \"&lt;bucket&gt;\", \"s3_prefix\": s3_prefix}\n        ),\n        \"s3\": s3_resource,\n        \"bq_io_manager\": bigquery_pandas_io_manager.configured(\n            {\n                \"credentials\": {\"env\": \"BIGQUERY_JSON_PATH\"},\n                \"project_id\": {\"env\": \"BIGQUERY_PROJECT_ID\"},\n                \"dataset_id\": '&lt;dataset_name&gt;'\n            }\n        )\n    }\n    \n    repository_assets = load_assets_from_modules(\n        modules=[\n            some_s3_assets,\n            dagster_quickstart_assets,\n            my_new_assets  #assets from the script above\n        ]) \\\n        + dbt_assets\n\n\n    return [\n        *with_resources(\n            definitions=repository_assets, resource_defs=resource_defs\n        ),\n        define_asset_job(name=\"all_assets_job\"),\n    ]```\nEven though I pass the I/O manager that I copied from the quickstart repo to the relevant asset it somehow uses the `s3` resource instead. It successfully runs but I'm at a loss for why it uses a completely different resource than the one I explicitly give the asset.\n\nDoes anyone have an idea what might be occurring?", "replies": [":upside_down_face:\n\nForgot to use `io_manager_key` instead of `required_resource_keys`. Obviously user error but is it expected behavior that if an invalid resource key is used that it would fall back to another resource that handles the same types?", "Glad you were able to figure this out.\n\nI'm not 100% following what you mean by \"but is it expected behavior that if an invalid required resource key that it would fall back to another resource that handles the same types?\"", "Hi Sandy, ya that was not stated clear.\n\nWhen I used `required_resource_keys` instead of `io_manager_key` it saved the data using a S3 IO manager even though I never made any mention of S3 in the asset. I had the impression that maybe it falls back on a I/O manager that handles the type that was being outputted (pandas dataframe)? But that seemed like an odd and unhelpful behavior instead of throwing an error.", "Ah I see - all assets have \"io_manager\" as their default IO manager, so if you don't specify an IO manager, this default is still used. Something that we could consider doing is issuing an error or warning if someone provides an IO manager to \"required_resource_keys\". However, there are some situations where that can still be a legitimate thing to do, so it's tough.", "Makes sense! Appreciate the answer, I think us renaming the IO manager named `io_manager` currently would be enough to get errors when we \"should\" in the future. Thanks!"], "is_dagster_empl": [false, true, false, true, false], "gpt4_replies_target": "The user was able to resolve their issue by using `io_manager_key` instead of `required_resource_keys`. The confusion arose because all assets have \"io_manager\" as their default IO manager, so if an IO manager is not specified, this default is still used.", "dagster_empl_first_target": "Glad you were able to figure this out.\n\nI'm not 100% following what you mean by \"but is it expected behavior that if an invalid required resource key that it would fall back to another resource that handles the same types?\"", "dagster_empl_last_target": "Ah I see - all assets have \"io_manager\" as their default IO manager, so if you don't specify an IO manager, this default is still used. Something that we could consider doing is issuing an error or warning if someone provides an IO manager to \"required_resource_keys\". However, there are some situations where that can still be a legitimate thing to do, so it's tough."}, {"question": "hi everyone, i need url websockets dagster thanks", "replies": ["hi <@U03TS6QV0TB> - the best way to submit a feature request is to file an issue on Github. the more detail that you're able to include about what functionality you're looking for, the better"], "is_dagster_empl": [true], "gpt4_replies_target": "To request a URL websockets feature for Dagster, you should file an issue on Github and provide as much detail as possible about the functionality you need.", "dagster_empl_first_target": "hi <@U03TS6QV0TB> - the best way to submit a feature request is to file an issue on Github. the more detail that you're able to include about what functionality you're looking for, the better", "dagster_empl_last_target": "hi <@U03TS6QV0TB> - the best way to submit a feature request is to file an issue on Github. the more detail that you're able to include about what functionality you're looking for, the better"}, {"question": "Hey there I am seeing this error when trying to load the 'Runs' page for a job:\n```Operation name: PipelineRunsRootQuery\n\nMessage: Cannot return null for non-nullable field Run.mode.\n\nPath: [\"pipelineRunsOrError\",\"results\",0,\"mode\"]\n\nLocations: [{\"line\":29,\"column\":3}]\n\nStack Trace:\n  File \"/usr/local/lib/python3.7/site-packages/graphql/execution/execute.py\", line 541, in execute_field\n    return_type, field_nodes, info, path, result\n  File \"/usr/local/lib/python3.7/site-packages/graphql/execution/execute.py\", line 621, in complete_value\n    \"Cannot return null for non-nullable field\"```", "replies": ["hey pablo what version are you on? We fixed something related to this last week.", "1.2.6", "let me upgrade to 1.3.2", "did the upgrade solve your problem?", "We are still seeing it trying to check if we upgraded correctly", "Yep upgrade had failed error is now gone thank you!", "I just started seeing this in 1.3.3 :(", "<@U03LAUL6PUH> are you sure you\u2019re running 1.3.3 of all of dagster/dagster-graphql/dagit? This error was fixed I believe in 1.3.1-&gt;1.3.2.", "Hey <@U01GTMVMGQH> unfortunately, yeah.", "Ahhh! Wait - my library is at 1.3.3 but in the dagster config it says 1.3.1. Crap. Let me double check I updated the helm file...", "Yeah, my fault. Forgot to rev the Chart.yaml again. So easy to forget that!"], "is_dagster_empl": [true, false, false, true, false, false, false, true, false, false, false], "gpt4_replies_target": "The error \"Cannot return null for non-nullable field Run.mode\" was a known issue that was fixed in version 1.3.2 of Dagster. Make sure all components of Dagster (dagster, dagster-graphql, dagit) are upgraded to the correct version to resolve this issue.", "dagster_empl_first_target": "hey pablo what version are you on? We fixed something related to this last week.", "dagster_empl_last_target": "<@U03LAUL6PUH> are you sure you\u2019re running 1.3.3 of all of dagster/dagster-graphql/dagit? This error was fixed I believe in 1.3.1-&gt;1.3.2."}, {"question": "Hi all!\nDoes anyone knows if theres a way to make Dagster \"ignore\" failed ops?\nmeaning that the op will end up in failure status (as well of the job of course) but the other ops won't  be affected by that?\nour use case is ETL job that running few parallel ops, and if one op is failing we don't want to fail the other ops, even if the job takes a long time.\nthanks!", "replies": ["Hi May, I believe this is already the case except for ops downstream of the failing op. You can confirm with this (alpha, delta execute successfully but beta fails):\n\n```import time\n\nfrom dagster import Definitions, job, op\n\n\n@op\ndef alpha():\n    time.sleep(5)\n    return 1\n\n\n@op\ndef beta():\n    raise Exception()\n\n\n@op\ndef delta(x):\n    return x + 1\n\n\n@job\ndef foo_job():\n    delta(alpha())\n    beta()\n\n\ndefs = Definitions(jobs=[foo_job])```", "Hey Sean thank you for your answer!\nI understood that in some cases the run manger is might not uploading new ops (even if they are parallel) in case of a failure in one of the ops in the job. You know anything about it?\nspecificity in out case we have hard coded limit for the number of parallel ops- meaning that if we sometimes encounter a case that two ops were spoused to be parallel but actually running in some time gap, and in this case if the run manger indeed not uploading new ops in case of failure- this will be a problem for us", "Is this something you\u2019ve actually observed or are just worried about? I\u2019m not 100% up to date on this but I believe any limitations on parallelism shouldn\u2019t matter. For instance, in the above example snippet `delta` is launched well after `beta` already failed (because `alpha` sleeps for 5 seconds)", "personally I have not experienced that issue- but our jobs currently not that long and I also think 5 sec won't be enough (but Im not that familiar with dagster architecture). I don't know if it's matter, but we are using <https://docs.dagster.io/_apidocs/libraries/dagster-k8s#dagster_k8s.k8s_job_executor|k8s_job_executor> as an executor to our ops.", "The 5 seconds above is arbitrary-- if there are still ops in the queue (that are not being skipped due to upstream failure) they should run.\n\nBut to confirm, it sounds to me like you haven\u2019t actually encountered a problem yet? If that\u2019s the case, I think you\u2019re good to go, Dagster will only cancel ops downstream of failed ops.", "yes- we haven't actually had this issue but got warned by it. I will give it try as it and if something will go wrong- I will write you guys again, Thank you for all your help! :dagster-bot-responded-by-community:"], "is_dagster_empl": [true, false, true, false, true, false], "gpt4_replies_target": "In Dagster, if an op fails, it won't affect other ops unless they are downstream of the failing op. However, if there are still ops in the queue that are not being skipped due to upstream failure, they should run.", "dagster_empl_first_target": "Hi May, I believe this is already the case except for ops downstream of the failing op. You can confirm with this (alpha, delta execute successfully but beta fails):\n\n```import time\n\nfrom dagster import Definitions, job, op\n\n\n@op\ndef alpha():\n    time.sleep(5)\n    return 1\n\n\n@op\ndef beta():\n    raise Exception()\n\n\n@op\ndef delta(x):\n    return x + 1\n\n\n@job\ndef foo_job():\n    delta(alpha())\n    beta()\n\n\ndefs = Definitions(jobs=[foo_job])```", "dagster_empl_last_target": "The 5 seconds above is arbitrary-- if there are still ops in the queue (that are not being skipped due to upstream failure) they should run.\n\nBut to confirm, it sounds to me like you haven\u2019t actually encountered a problem yet? If that\u2019s the case, I think you\u2019re good to go, Dagster will only cancel ops downstream of failed ops."}, {"question": "is there a way to rename a code location in order to have it shorter in Dagit? I have the following workspace:\n\n```load_from:\n  - python_module: a.very.long.module.name1\n  - python_module: a.very.long.module.name2```\nand in the left men\u00f9 in Dagit the name is too long to be displayed completely, can I have it renamed like `name1`  and `name2` ?", "replies": ["I think if you include `location_name` as a key alongside `python_module` that should work", "it works, thanks!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "In Dagit, you can rename a code location by including `location_name` as a key alongside `python_module` in your workspace.", "dagster_empl_first_target": "I think if you include `location_name` as a key alongside `python_module` that should work", "dagster_empl_last_target": "I think if you include `location_name` as a key alongside `python_module` that should work"}, {"question": "Hi! I need help.\n\nI am trying to use `define_asset_job` with both source &amp; regular assets. I have an asset group with both types of assets in it.  f I use `AssetSelection.groups` I get the error:\n\n&gt; Asset selection specified both regular assets and source assets. This is not currently supported. Selections must be all regular assets or all source assets.\nI'm even more confused because I have a different job that points to another group mixing both types of assets. That one  loads successfully. :confused:", "replies": ["I think I may be misunderstanding what source assets are. I have a dbt project and I assumed that all assets loaded with `load_assets_from_dbt_project` would be source assets.\n\nWhen I look at the job that is working, the dbt assets are coming up as regular assets, but not on the job that is not working.", "so, the dbt assets that are explicitly used as inputs on dagster software-defined assets get categorized as \"SourceAsset\", causing this issue.", "how can I work around it?", "Hi Remi, thanks for reaching out. We recently started allowing `define_asset_job` to specify source assets, but it is a (temporary) limitation that we can\u2019t allow both observable source and regular assets to be included in the same job, so selections including both will error. This is not meant to happen unless you explicitly specify a key for both a source asset and regular asset.\n\nLooks like the issue with `groups` was just an oversight, it should resolve only to regular assets. We are releasing tmrw and I can patch this for that release.", "thanks for the quick reply <@U01GTMVMGQH>\nActually, I just solved out my issue! I was mistakenly using `SourceAsset` to reference dbt models instead of `AssetIn`.", "ah great-- well you found a bug in any case", "yeah I see that :slightly_smiling_face:", "I was doing:\n```\ndbt_asset = SourceAsset(key=[\"warehouse\", DBT_SCHEMA, \"dbt_asset\"])\n\n@asset\ndef es_index_profiles_v3b(context,\n                          dbt_asset):\n    ...```\nbut I meant:\n```dbt_asset = AssetIn(key=[\"warehouse\", DBT_SCHEMA, \"dbt_asset\"])\n\n@asset\ndef es_index_profiles_v3b(context,\n                          dbt_asset):\n    ...```", "I'm still unclear what `SourceAsset` really is", "A `SourceAsset` represents a data asset that you want to use as an input but that Dagster doesn\u2019t know how to compute. e.g. maybe the result of a request from some web api", "I see. So it's similar to dbt sources right? I can declare it to document that a dagster depends on an external asset.", "I\u2019m not very familiar with dbt so I can\u2019t speak to its similarity, but yes, you can \u201cdeclare it to document that dagster depends on an external asset\u201d.", "thank you. I appreciate the help <@U01GTMVMGQH>!"], "is_dagster_empl": [false, false, false, true, false, true, false, false, false, true, false, true, false], "gpt4_replies_target": "The issue arises from using `SourceAsset` and regular assets in the same job, which is currently not supported in Dagster. The user was mistakenly using `SourceAsset` to reference dbt models instead of `AssetIn`. A `SourceAsset` represents a data asset that Dagster doesn\u2019t know how to compute, like the result of a request from a web API.", "dagster_empl_first_target": "Hi Remi, thanks for reaching out. We recently started allowing `define_asset_job` to specify source assets, but it is a (temporary) limitation that we can\u2019t allow both observable source and regular assets to be included in the same job, so selections including both will error. This is not meant to happen unless you explicitly specify a key for both a source asset and regular asset.\n\nLooks like the issue with `groups` was just an oversight, it should resolve only to regular assets. We are releasing tmrw and I can patch this for that release.", "dagster_empl_last_target": "I\u2019m not very familiar with dbt so I can\u2019t speak to its similarity, but yes, you can \u201cdeclare it to document that dagster depends on an external asset\u201d."}, {"question": "Hi all! I'm starting to use environmental variables from a .env file with my run configurations, and am running into some challenges.\n\nUsing the instructions on run configuration <https://docs.dagster.io/guides/dagster/using-environment-variables-and-secrets#from-dagster-configuration|at this link>, I've written the following in my job file:\n\n```defs = Definitions(\n    jobs=[\n        pq_first_load_graph.to_job(\n            resource_defs={'s3': s3_res},\n            config={\n                'ops': {\n                    'get_city_data': {\n                        'config': {\n                            'cities': {'env': 'CITIES'}\n                        }\n                    }\n                }\n            }\n        )\n    ]\n)```\nWhen I run \"dagster dev -f &lt;job_file&gt;\", I get the following error:\n\n```dagster._core.errors.DagsterInvalidConfigError: Invalid default_value for Field. Error 1: Invalid scalar at path root:get_city_data:config:cities. Value \"{'env': 'CITIES'}\" of type \"&lt;class 'dict'&gt;\" is not valid for expected type \"String\".```\nAny ideas on what I'm doing wrong? Also, out of curiosity, once I solve this issue, will I have to manually enter the cities value in the job launchpad? I had previously been using os.getenv to load the environmental variables and couldn't figure out a way to get around entering them manually in the launchpad.", "replies": ["Hi Phil, You need `StringSource` set in the config schema for your `get_city_data` op, see here: <https://docs.dagster.io/concepts/configuration/config-schema#passing-secrets-as-configuration>\n\nI recognize that that snippet of docs you linked is misleading because it does not include this, I will fix it.", "&gt; Also, out of curiosity, once I solve this issue, will I have to manually enter the cities value in the job launchpad? I had previously been using os.getenv to load the environmental variables and couldn\u2019t figure out a way to get around entering them manually in the launchpad. \nYou shouldn\u2019t need to do this if you have a `.env` file with the necessary values in the folder where dagit is launched.", "Ok perfect. I've got it working now -- thanks very much <@U01GTMVMGQH>!"], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "To resolve the error, you need to set `StringSource` in the config schema for your `get_city_data` operation. Once this issue is solved, you won't have to manually enter the cities value in the job launchpad if you have a `.env` file with the necessary values in the folder where dagit is launched.", "dagster_empl_first_target": "Hi Phil, You need `StringSource` set in the config schema for your `get_city_data` op, see here: <https://docs.dagster.io/concepts/configuration/config-schema#passing-secrets-as-configuration>\n\nI recognize that that snippet of docs you linked is misleading because it does not include this, I will fix it.", "dagster_empl_last_target": "&gt; Also, out of curiosity, once I solve this issue, will I have to manually enter the cities value in the job launchpad? I had previously been using os.getenv to load the environmental variables and couldn\u2019t figure out a way to get around entering them manually in the launchpad. \nYou shouldn\u2019t need to do this if you have a `.env` file with the necessary values in the folder where dagit is launched."}, {"question": "I'm looking for advice on how best to structure my project. Notionally, the questoin is how to do \"fan-out\" operations using (preferably dynamically) partitioned assets, but I might be overlooking another approach.\n\nFor context, I'm new to dagster.\n\nThe situation: I have sets of data files. Each set has multiple image data files, and a single calibration data file. I want to be able to build a pipeline to apply the calibration to each uncalibrated image file to create corresponding calibrated image files. There are per-image tasks downstream as well, but I don't think those are relevant to this question. Both the calibration data and each image file need (somewhat time consuming) preprocessing before the calibration is applied to the images.\n\nThe way I'd think to structure this would be a preprocessed_calibration asset partitioned by set, and a calibrated_image asset partitioned by image file id, with a mapping between the set name and the image file ids (one set goes to many image file ids). As I understand it, this way each set's calibration data would be processed exactly once, and then applied to all of the image assets in the set.\n\nNow, the practical problem.  We want to be able to add data sets over time. Moreover, we want to be able to use this code in multiple locations (different laptops and a cloud deployment), each of which will have their own datasets. The first thought is to use `DynamicPartitions` to generate the partitions at runtime, based upon the files available on the system its running on. The problem is, it seems that there is no way to map dynamic partitions to each other (see feature request <https://github.com/dagster-io/dagster/issues/13139>). Thus, with dynamic partitions, I'm seemingly locked out of \"fan-out\" type operations, so I can't use the mapping strategy to only preprocess the calibration once per set.\n\nThe second option is to use static partitions, and do the mapping on those. My main concern is that the static partitions it seems have to be instantiated before passing to the `@asset` decorator. So, we'd have to edit the code upstream of the asset, every time we want to run on new files. Possible, but this makes the code less portable. I'd prefer to separate the asset logic from the data it runs on. It would be better if we could define the static partitions downstream of the asset definitions, so they could be reusable against different data locations. For example, if we defined them in a per-deployment script that calls `load_ssets_from_package_module` . Even better would be if the static partitions could be loaded from a config file or environmental variables, but I'm guessing at that point we are back to dynamic partitions.\n\nA third option is to define partitions and assets only at the set level, but that isn't really the partitioning scheme that is natural for downstream things we want to do, and since most of the operations are highly parallelizable by file, we'd want to find a new parallelization solution nested inside of dagster (maybe dask).\n\nAny advice on how to handle this situation?", "replies": ["Hi Caleb, Thanks for this very thorough problem description-- will be useful for us as we continue developing dynamic partitions.\n\nI think probably the best solution for you for now (pending custom mapping between dynamic partitions) is to use static partitions. You could create a function that generates the `StaticPartitionsDefinitions` by reading a file, so you shouldn't have to edit any code. You will of course need to reload the code location to register an update to the data file defining the partitions.", "Thanks. Yeah, I wasn't sure how to explain the problem in a more pithy way without potentially omitting something, since I really wasn't sure if I was even thinking about the problem correctly in the dagster framework. Thanks for reading the whole thing!\n\nThis is actually the solution I came to late yesterday. Still vetting it. Thanks for confirming that it is a reasonable way to go about it, since having a \"generator function\" for these things loaded at run time is not something I noted in the docs, and I wasn't sure if there might be some pitfalls. Simply clicking on the \"reload\" button in dagit seems to detect new static partitions fine, so this seems like a pretty good solution.\n\nIf you ever add dynamic partition mappings, I should be able to swap those in with minimal impact to the rest of the code.", "Great, glad that this solution is working for you. I will add something to the docs explaining this alternative approach to dynamic partitions.", "interesting problem! gives some food for though", "This seems to be working. It's a little awkward, since I need to setup the file locations before the partitions are created and thus also before the software defined assets are declared. Essentially, I need to define it by something like a global variable before importing my assets (if I want the assets to be in an importable python module.) I'm currently doing this via an environmental variable.\n\nI'm a little worried this is inflexible as the project scales up and when writing tests. I'd prefer to somehow use the dagster context/config system to define this path.\n\nIf this winds up in the documentation <@U01GTMVMGQH>, I'll be interested in taking a look to see if you have a more elegant way to set this sort of \"dynamic-defined\" static partitions up."], "is_dagster_empl": [true, false, true, false, false], "gpt4_replies_target": "The best solution for the described situation is to use static partitions, with a function that generates the `StaticPartitionsDefinitions` by reading a file, thus avoiding code editing. However, the code location needs to be reloaded to register an update to the data file defining the partitions.", "dagster_empl_first_target": "Hi Caleb, Thanks for this very thorough problem description-- will be useful for us as we continue developing dynamic partitions.\n\nI think probably the best solution for you for now (pending custom mapping between dynamic partitions) is to use static partitions. You could create a function that generates the `StaticPartitionsDefinitions` by reading a file, so you shouldn't have to edit any code. You will of course need to reload the code location to register an update to the data file defining the partitions.", "dagster_empl_last_target": "Great, glad that this solution is working for you. I will add something to the docs explaining this alternative approach to dynamic partitions."}, {"question": "Using Docker compose and trying to increase <https://docs.dagster.io/deployment/dagster-instance#grpc-servers|gRPC code server timeout> to something above 60. In dagster.yaml available to Dagster daemon and Dagit containers, I've included the following:\n```code_servers:\n  local_startup_timeout: 180```\nI even tried it in another dagster.yaml in the user code image, but nothing works.\n\nNo matter what I try, I still see the error `dagster._core.errors.DagsterUserCodeUnreachableError: User code server request timed out due to taking longer than 60 seconds to complete.`\n\nAny tips?", "replies": ["Hey Nicolas - wheee exactly are you seeing that error, does it come with a stack trace? That timeout covers starting up the servers / loading your code, not requests to the servers in general", "Yes thanks! From the Dagster daemon container:\n```docker_daemon_local               | 2023-03-28 17:10:00 +0000 - dagster.daemon.SchedulerDaemon - WARNING - Could not load location user_code_beam_dw_dagster to check for schedules due to the following error: dagster._core.errors.DagsterUserCodeUnreachableError: User code server request timed out due to taking longer than 60 seconds to complete.\ndocker_daemon_local               |\ndocker_daemon_local               | Stack Trace:\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_core/workspace/context.py\", line 599, in _load_location\ndocker_daemon_local               |     location = self._create_location_from_origin(origin)\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_core/workspace/context.py\", line 519, in _create_location_from_origin\ndocker_daemon_local               |     return origin.create_location()\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_core/host_representation/origin.py\", line 332, in create_location\ndocker_daemon_local               |     return GrpcServerRepositoryLocation(self)\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_core/host_representation/repository_location.py\", line 637, in __init__\ndocker_daemon_local               |     self._external_repositories_data = sync_get_streaming_external_repositories_data_grpc(\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_api/snapshot_repository.py\", line 25, in sync_get_streaming_external_repositories_data_grpc\ndocker_daemon_local               |     external_repository_chunks = list(\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_grpc/client.py\", line 351, in streaming_external_repository\ndocker_daemon_local               |     for res in self._streaming_query(\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_grpc/client.py\", line 186, in _streaming_query\ndocker_daemon_local               |     self._raise_grpc_exception(\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_grpc/client.py\", line 137, in _raise_grpc_exception\ndocker_daemon_local               |     raise DagsterUserCodeUnreachableError(\ndocker_daemon_local               |\ndocker_daemon_local               | The above exception was caused by the following exception:\ndocker_daemon_local               | grpc._channel._MultiThreadedRendezvous: &lt;_MultiThreadedRendezvous of RPC that terminated with:\ndocker_daemon_local               | \tstatus = StatusCode.DEADLINE_EXCEEDED\ndocker_daemon_local               | \tdetails = \"Deadline Exceeded\"\ndocker_daemon_local               | \tdebug_error_string = \"{\"created\":\"@1680023292.241437667\",\"description\":\"Deadline Exceeded\",\"file\":\"src/core/ext/filters/deadline/deadline_filter.cc\",\"file_line\":81,\"grpc_status\":4}\"\ndocker_daemon_local               | &gt;\ndocker_daemon_local               |\ndocker_daemon_local               | Stack Trace:\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_grpc/client.py\", line 182, in _streaming_query\ndocker_daemon_local               |     yield from self._get_streaming_response(\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/dagster/_grpc/client.py\", line 171, in _get_streaming_response\ndocker_daemon_local               |     yield from getattr(stub, method)(request, metadata=self._metadata, timeout=timeout)\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/grpc/_channel.py\", line 426, in __next__\ndocker_daemon_local               |     return self._next()\ndocker_daemon_local               |   File \"/usr/local/lib/python3.9/site-packages/grpc/_channel.py\", line 826, in _next\ndocker_daemon_local               |     raise self\ndocker_daemon_local               |```", "The code location has a huge dbt project in it", "That\u2019s an odd one - typically if the code is taking a long time to load the timeout would happen earlier. What version of dagster is this?", "1.1.21", "How huge a project are we talking?", "~1800 models... there's also an upstream Fivetran instance with ~30 connectors", "Could you possibly share your workspace.yaml file or pyproject.toml file?", "```load_from:\n  # Each entry here corresponds to a service in the docker-compose file that exposes user code.\n  - grpc_server:\n      host: docker_user_code_beam_dw_dagster\n      port: 4004\n      location_name: \"user_code_beam_dw_dagster\"```", "Ahh ok, so that's another reason that local_startup_timeout isn't going to help here - the servers aren't actually considered 'local' when you're running in docker. Let's se...", "Is the user code container running?", "Ahhhhh... okay", "Yes", "It has a line like \"Started Dagster code server on port 4004 in process 7283\" in its container output?", "```docker_user_code_beam_dw_dagster  | 2023-03-28 17:26:03 +0000 - dagster - INFO - Started Dagster code server for package dagster_integrations on port 4004 in process 1```", "Got it - so after that happens, the daemon is still spewing that error? I could imagine it struggling while the user code container is starting up - the intended behavior would be that the error stops within a minute or so after the user code container finishes loading", "And it's a little unpredictable... after a while it someteims _does_ load... but then it disappears from Dagit UI", "I'd check the Code Locations tab in Dagit when that happens, there might be an error there", "(potentially this same error)", "I get the error in Dagit code locations tab and Docker logs", "ok, once we've gone through all that - there is a DAGSTER_GRPC_TIMEOUT_SECONDS env var that you could try setting to something like 180 in dagit and the daemon container. We're usually hesistant to recommend that first since in many cases there's some underlying issue (the default is 60 seconds which is quite long). It's possible that your dbt project is large enough that it's actually taking more than 60 seconds just to stream all the data over though - i'd be curious if things get more stable after this though or if the sheer size of the data causes other problems", "I'll give this a shot rn", "FWIW I've set `display_raw_sql=False` in `load_assets_from_dbt_manifets` to try to speed things up", "Ah yes - maybe try that first", "Sorry... I already have :upside_down_face:", "waiting &gt;60 seconds each time it needs to reload doesn't sound very enjoyable even once its no longer timing out", "Like a couple days ago", "ah unfortunate", "&gt; doesn't sound very enjoyable even once its no longer timing out\nYou are correct", "If you're comfortable sending us your metadata I could give you a script that prints out the serialized object that it's trying to fetch - we could see if there are other improvements like display_raw_sql that we need to make for these large asset graphs", "Thanks! That's really generous... I'll hafta check with my team\n\nQuestion: \"metadata\" == manifest.json?", "It's basically 'what's displayed in dagit for your assets'", "for your asset graph, rather", "Env var `DAGSTER_GRPC_TIMEOUT_SECONDS` seems to've worked better", "Sorry... just trying to clarify, our dbt target/manifest.json would be helpful? Is that right?", "(Like what specifically do you mean by \"metadata\"?)", "Oh I see - us seeing the object that it's fetching from this particular API call (derived from your code and your manifest.json) that's taking so long to generate is what would be helpful", "Ah okay... how would I retrieve that?", "I was going to give you a quick script that basically runs the API call that's taking forever and prints the serialized object to stdout", "That'd be great thx!", "And again... thanks so much for your help on this", "no problem - what's the name of your repository (if you're not using Definitions)", "I'm using Definitions", "gotcha", "Ok, here's a quick script that should work when run from your daemon or dagit container:\n\n```from dagster._core.host_representation.origin import (\n    GrpcServerRepositoryLocationOrigin,\n)\n\nhost = \"docker_user_code_beam_dw_dagster\"\nport = 4004\n\norigin = GrpcServerRepositoryLocationOrigin(host=host, port=port)\n\nlocation = origin.create_location()\n\nfor repo_name in location.get_repositories():\n    print(f\"{repo_name}:\")\n    print(str(location.get_repositories()[repo_name].external_repository_data))```", "(with that timeout increased so the call actually finishes)", "I'll give it a shot, check with my team, and if we're all good I'll circle back to this thread if that's the best way", "For sure - entirely optional and you're of course welcome to DM or email it to <mailto:daniel@elementl.com|daniel@elementl.com> instead of posting here", "Oh great... Thanks!", "<@U016C4E5CP8> Just giving you a heads-up that I sent a follow-up email a few seconds ago from nicolas.may@[<http://my_employer_domain.com|my_employer_domain]>... Thx!", "Thanks nicolas - how's it performing after that timeout increase, are you seeing any other issues?", "Totally missed your reply... Sorry\n\nThe timeout increase has definitely improved things... and we scaled up the VM running the Docker compose deployment in production... so that's helped too... We still get InactiveRpcErrors on a pretty regular cadence, and I'm not sure how to improve/fix that", "Can you post a stack trace for one of the latest InactiveRpcErrors?", "Will do in a few min... thx!", "```/usr/local/lib/python3.9/site-packages/dagster/_core/workspace/context.py:602: UserWarning: Error loading repository location user_code_beam_dw_dagster:dagster._core.errors.DagsterUserCodeUnreachableError: Could not reach user code server. gRPC Error code: UNAVAILABLE\n\nStack Trace:\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/workspace/context.py\", line 599, in _load_location\n    location = self._create_location_from_origin(origin)\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/workspace/context.py\", line 519, in _create_location_from_origin\n    return origin.create_location()\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/host_representation/origin.py\", line 332, in create_location\n    return GrpcServerRepositoryLocation(self)\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/host_representation/repository_location.py\", line 603, in __init__\n    list_repositories_response = sync_list_repositories_grpc(self.client)```", "Got it - and that's in the dagit container it looks like?", "I'd associate that particular error with the user code container being down / entirely unreachable", "I think it's coming from Dagster daemon container... Fluent Bit is a sidecar container sending Datadog logs", "ok, if its the daemon, one nice thing is that it'll periodically refresh - so if the code location is temporarily down, the daemon should be able to load it again and try again in about a minute", "And ya... that error definitely happens when things are first spinning up... but it also happens intermittently", "and it will pause executing things untii its available again", "would you know from your logs if the user code container went down?", "Yes! That's great... love this feature", "&gt; would you know from your logs if the user code container went down?\nHmm... I don't think I've set that up", "I'll add this... we've got 2 user code repos... the other one that we've been running in prod for a few months isn't this noisy", "could it be running out of memory or something possibly?", "It's far less complex... has a bunch of op graphs but it's not a big hairy dependency ball like this problem repo w/ Fivetran + dbt + Census", "Maybe it's a memory problem... we've just upgraded the VM... GCP CE e2-standard-4... 4 vCPUs and 16 GB mem", "I thought that'd hack it...\n\u2022 Dagster daemon container\n\u2022 Dagit container\n\u2022 1 simple user code container\n\u2022 1 big gnarly user code container\n\u2022 Fluent Bit sidecar", "Morning <@U016C4E5CP8>... I just wanted to follow up on this... So the reason this user code container was going down every few minutes and then coming back up was because (1) I'd set a `--heartbeat-timeout 1200` (20 min) on the dagster api grpc command in the docker-compose.yaml to try to fix the gRPC timeout problem, but hadn't removed the heartbeat flags, and (2) the docker compose service restart policy is `unless-stopped`...\n\nOnly by looking at the service's logs and noticing that there were repeated cycles of `Started Dagster code server for package ...` and `Shutting down Dagster code server for package ...` exactly 20 minutes apart did it occur to me that this heartbeat timeout flag was causing the problem\n\nOnce got rid of the heartbeat timeout, this user code repo container (after it takes a while to spin up) works without a hitch...\n\nThanks for helping me out with this!"], "is_dagster_empl": [true, false, false, true, false, true, false, true, false, true, true, false, false, true, false, true, false, true, true, false, true, false, false, true, false, true, false, true, false, true, false, true, true, false, false, false, true, false, true, false, false, true, false, true, true, true, false, true, false, false, true, false, true, false, false, true, true, false, true, false, true, true, false, false, false, true, false, false, false, false], "gpt4_replies_target": "The user was experiencing a timeout error with their gRPC code server in Dagster. After several troubleshooting steps, they discovered that the issue was caused by a heartbeat timeout flag in their docker-compose.yaml file. Once this flag was removed, the user code repository container worked without any issues.", "dagster_empl_first_target": "Hey Nicolas - wheee exactly are you seeing that error, does it come with a stack trace? That timeout covers starting up the servers / loading your code, not requests to the servers in general", "dagster_empl_last_target": "could it be running out of memory or something possibly?"}, {"question": "Hi team, my Jupyter notebooks are in google colab. Can I use dagster notebooks integration to bring them into dagster as assets?", "replies": ["Hey <@U02P92K2FRQ> that isn\u2019t supported right now. However, if you can execute a google colab notebook via papermill, you may be able to write the asset yourself. If you\u2019re interested in that, this repo, that integrates another notebook service (Noteable) with dagster may be a helpful resoruce <https://github.com/noteable-io/papermill-origami/tree/main/papermill_origami/noteable_dagstermill>"], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, Dagster does not support integration with Google Colab notebooks directly, but you might be able to create the asset yourself if you can execute a Google Colab notebook via Papermill.", "dagster_empl_first_target": "Hey <@U02P92K2FRQ> that isn\u2019t supported right now. However, if you can execute a google colab notebook via papermill, you may be able to write the asset yourself. If you\u2019re interested in that, this repo, that integrates another notebook service (Noteable) with dagster may be a helpful resoruce <https://github.com/noteable-io/papermill-origami/tree/main/papermill_origami/noteable_dagstermill>", "dagster_empl_last_target": "Hey <@U02P92K2FRQ> that isn\u2019t supported right now. However, if you can execute a google colab notebook via papermill, you may be able to write the asset yourself. If you\u2019re interested in that, this repo, that integrates another notebook service (Noteable) with dagster may be a helpful resoruce <https://github.com/noteable-io/papermill-origami/tree/main/papermill_origami/noteable_dagstermill>"}, {"question": "<https://dagster.io/> seems to be down?", "replies": ["Hi Spencer\u2026 we just had a minor downtime hiccup as we were migrating some account status\u2026 Is it still down?", "And I think its back", "Sorry for the disruption!", "np!"], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "The Dagster website had a minor downtime due to some account status migration, but it should be back up now. Apologies for any inconvenience caused.", "dagster_empl_first_target": "Hi Spencer\u2026 we just had a minor downtime hiccup as we were migrating some account status\u2026 Is it still down?", "dagster_empl_last_target": "Sorry for the disruption!"}, {"question": "In a sensor, what method should I be using to get all of the events for an asset that have not yet been consumed?", "replies": ["I believe `materialization_records_for_key` in the multi-asset sensor should do that: <https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#triggering-runs-upon-materializations>\n\nThe call is relative to the cursor, which tracks which materializations have been consumed", "Thank you"], "is_dagster_empl": [true, false], "gpt4_replies_target": "You should use the `materialization_records_for_key` method in the multi-asset sensor to get all of the events for an asset that have not yet been consumed, as it is relative to the cursor which tracks consumed materializations.", "dagster_empl_first_target": "I believe `materialization_records_for_key` in the multi-asset sensor should do that: <https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#triggering-runs-upon-materializations>\n\nThe call is relative to the cursor, which tracks which materializations have been consumed", "dagster_empl_last_target": "I believe `materialization_records_for_key` in the multi-asset sensor should do that: <https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#triggering-runs-upon-materializations>\n\nThe call is relative to the cursor, which tracks which materializations have been consumed"}, {"question": "i want to define a graph/job with some ops that can run simultaneously (independent) and when they finish - a final op will start. I have something like this in my graph:\n\n`fanouts=[]`\n`sublists=['list','with','locations','with','ever_changing','list_length']`\n`for i in range(len(sublists)):`\n   `fanouts.append(getCritOp(locs=sublists[i])())`\n`#finally, this op should execute after all the others are successful`\n`combineCritdata(list=fanouts)()`\n\n`Note the extra () on getCritOp and combineCritdata - these are op factories`\nCurrently in this state , 3 ops will start, and the first two will finish, then the combineCritdata will start prematurely  (i have max_concurrent = 3 configured) . I need it to require the other 3  (or however many it will be, i dont know) to finish first.", "replies": ["in order to define dependencies, you'll want to pass in the fanouts to the op invocation, rather than the op factory itself. So this would look something like `combineCritdata()(start_after=fanouts)`, assuming combineCritdata looks something like\n```def combineCritdata():\n    @op(name=\"combineCritdata\", ins={\"start_after\": In(Nothing)})\n    def my_op():\n        ...\n    return my_op```", "This worked thank you so much!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "To define dependencies in Dagster, you should pass in the fanouts to the op invocation, not the op factory itself. For example, `combineCritdata()(start_after=fanouts)`, assuming combineCritdata is defined with an input dependency.", "dagster_empl_first_target": "in order to define dependencies, you'll want to pass in the fanouts to the op invocation, rather than the op factory itself. So this would look something like `combineCritdata()(start_after=fanouts)`, assuming combineCritdata looks something like\n```def combineCritdata():\n    @op(name=\"combineCritdata\", ins={\"start_after\": In(Nothing)})\n    def my_op():\n        ...\n    return my_op```", "dagster_empl_last_target": "in order to define dependencies, you'll want to pass in the fanouts to the op invocation, rather than the op factory itself. So this would look something like `combineCritdata()(start_after=fanouts)`, assuming combineCritdata looks something like\n```def combineCritdata():\n    @op(name=\"combineCritdata\", ins={\"start_after\": In(Nothing)})\n    def my_op():\n        ...\n    return my_op```"}, {"question": "Does `@dagster.asset_sensor` support partitioned assets, or should I be using `@dagster.multi_asset_sensor` for those?", "replies": ["You should be able to use an asset sensor with a partitioned asset-- the sensor will trigger for all materializations regardless of partition. You\u2019ll just need to apply any logic dependent on the materialization partition inside the sensor eval function.", "What is the right way to find out which upstream partition was materialized?", "I found \u201cevent.dagster_event.event_specific_data.materialization.partition\u201d but that seems way too deep in internals", "`event.dagster_event.partition` should work"], "is_dagster_empl": [true, false, false, true], "gpt4_replies_target": "Yes, `@dagster.asset_sensor` does support partitioned assets and will trigger for all materializations regardless of partition. For finding out which upstream partition was materialized, you can use `event.dagster_event.partition`.", "dagster_empl_first_target": "You should be able to use an asset sensor with a partitioned asset-- the sensor will trigger for all materializations regardless of partition. You\u2019ll just need to apply any logic dependent on the materialization partition inside the sensor eval function.", "dagster_empl_last_target": "`event.dagster_event.partition` should work"}, {"question": "Hey all, silly question but how do I gracefully stop an instance of dagster dev started on a remote connection, from another remote connection? I find many running processes but I don't know which one to quit to make it exit gracefully and clean up the temp directory..", "replies": ["Hi Emir, I would expect sending a SIGINT / CTRL-C to the parent dagster dev process to also gracefully interrupt any subprocesses that it spins up - not sure if that fully answers your question", "That probably works, thanks, I can find the parent and kill"], "is_dagster_empl": [true, false], "gpt4_replies_target": "You can gracefully stop an instance of dagster dev started on a remote connection by sending a SIGINT / CTRL-C to the parent dagster dev process, which should also interrupt any subprocesses that it spins up.", "dagster_empl_first_target": "Hi Emir, I would expect sending a SIGINT / CTRL-C to the parent dagster dev process to also gracefully interrupt any subprocesses that it spins up - not sure if that fully answers your question", "dagster_empl_last_target": "Hi Emir, I would expect sending a SIGINT / CTRL-C to the parent dagster dev process to also gracefully interrupt any subprocesses that it spins up - not sure if that fully answers your question"}, {"question": "What happens if multiple partitions of an asset are materialized between ticks of an AssetSensor? Will it fire multiple times?", "replies": ["<https://github.com/dagster-io/dagster/issues/5699> suggests the answer is it will only run for the last partition to complete within the tick interval?", "Yes, it will only run for the last partition to complete in the interval."], "is_dagster_empl": [false, true], "gpt4_replies_target": "If multiple partitions of an asset are materialized between ticks of an AssetSensor, it will only run for the last partition to complete within the tick interval.", "dagster_empl_first_target": "Yes, it will only run for the last partition to complete in the interval.", "dagster_empl_last_target": "Yes, it will only run for the last partition to complete in the interval."}, {"question": "Is there any trick for converting from a Definitions to a RepositoryDefinition? `build_sensor_context` requires a RepositoryDefinition so it\u2019s hard for me to figure out how to write tests using my Definitions", "replies": ["Hmm that\u2019s an oversight that we\u2019ve left a public API requiring a `RepositoryDefinition`. Would you mind filing an issue?\n\nBut for an immediate solution to your problem, you can call `&lt;definitions_object&gt;.get_repository_def()`. This is a private method but it will work for you for now.", "Sure i made one last night: <https://github.com/dagster-io/dagster/issues/13207|https://github.com/dagster-io/dagster/issues/13207>"], "is_dagster_empl": [true, false], "gpt4_replies_target": "You can use the private method `<definitions_object>.get_repository_def()` to convert from Definitions to a RepositoryDefinition for testing purposes, although this is not the intended use. An issue has been filed to address this oversight in the public API.", "dagster_empl_first_target": "Hmm that\u2019s an oversight that we\u2019ve left a public API requiring a `RepositoryDefinition`. Would you mind filing an issue?\n\nBut for an immediate solution to your problem, you can call `&lt;definitions_object&gt;.get_repository_def()`. This is a private method but it will work for you for now.", "dagster_empl_last_target": "Hmm that\u2019s an oversight that we\u2019ve left a public API requiring a `RepositoryDefinition`. Would you mind filing an issue?\n\nBut for an immediate solution to your problem, you can call `&lt;definitions_object&gt;.get_repository_def()`. This is a private method but it will work for you for now."}, {"question": "I have an asset, say `A` with daily partitioning with `start_date` of `start_a`. I also have a `B` asset that consumes `A` but which is offset by one day. As such, the `start_date` for `B` is `start_a + 1` since it can only run once `A` has at least one day available. This works all fine, except that I find myself having to treat these 2 assets separately in some places because their `PartitionDefinitions` are not exactly the same. Specifically, I cannot use `build_asset_job` and specify both `A` and `B` . This happens quite a bit throughout because this \"offset\" pattern is common in the data I'm dealing with (i.e.: I also use `end_offset` with positive and negative values). So I end up having to create many jobs that really shouldn't have to. Also, I haven't found a great way of always triggering the downstream job since asset materialization sensors only looks at the latest partition AFAIK. Is there a better way to deal with lots of variability in `PartitionDefinitions` like I have?", "replies": ["There's currently a restriction that each run needs to target the same partition across all assets - it's not impossible, but breaking this constraint would require some deep surgery\n\nIf you're using the asset reconciliation sensor, it should handle this automatically as long as you're using TimeWindowPartitionMapping with an offset to express the partition dependency\n\nIf you're using a custom multi-asset sensor, it should also respect the partition mapping\n\n<https://docs.dagster.io/_apidocs/partitions#dagster.TimeWindowPartitionMapping>", "also, you can now select assets with different partitionings and click the materialize button. dagster will launch a backfill that gets split into sub-runs that target different partitions", "On that last point, shouldn\u2019t that make it possible to define some kind of \u201cmulti-run\u201d job? One that would allow targeting related assets, but that have different partition definition (say ones that only differ by start/end offset)? It\u2019s great that I can do this in the UI, but it would be a lot more convenient to be able to make a formal job out of this.", "I filed an issue to track this: <https://github.com/dagster-io/dagster/issues/13257>"], "is_dagster_empl": [true, true, false, true], "gpt4_replies_target": "Currently, each run needs to target the same partition across all assets. However, if you're using the asset reconciliation sensor with TimeWindowPartitionMapping with an offset, it should handle this automatically. You can also select assets with different partitionings and click the materialize button to launch a backfill that targets different partitions. An issue has been filed to track the possibility of defining a \u201cmulti-run\u201d job for related assets with different partition definitions.", "dagster_empl_first_target": "There's currently a restriction that each run needs to target the same partition across all assets - it's not impossible, but breaking this constraint would require some deep surgery\n\nIf you're using the asset reconciliation sensor, it should handle this automatically as long as you're using TimeWindowPartitionMapping with an offset to express the partition dependency\n\nIf you're using a custom multi-asset sensor, it should also respect the partition mapping\n\n<https://docs.dagster.io/_apidocs/partitions#dagster.TimeWindowPartitionMapping>", "dagster_empl_last_target": "I filed an issue to track this: <https://github.com/dagster-io/dagster/issues/13257>"}, {"question": "how do asset reconciliation sensors work when a particular partition is failing. is it going to keep try the failing asset partition or launch a run and move on", "replies": ["hi <@U04SEPU9G04>! it will launch the run and move on", "thanks, my partition names are GUIDs, so I was losing my mind trying to frantically copy/paste and search"], "is_dagster_empl": [true, false], "gpt4_replies_target": "If a particular partition is failing, the asset reconciliation sensor in Dagster will launch a run and move on, it won't keep trying the failing asset partition.", "dagster_empl_first_target": "hi <@U04SEPU9G04>! it will launch the run and move on", "dagster_empl_last_target": "hi <@U04SEPU9G04>! it will launch the run and move on"}, {"question": "Hi, I wanted to deploy Dagster on local with the DAGSTER_HOME environment variable set in .env file following the documentation: <https://docs.dagster.io/guides/dagster/using-environment-variables-and-secrets#declaring-environment-variables>, however, after I ran the Dagster service using \"dagster dev\" command, the log said that DAGSTER_HOME was not set. How should I resolve this thread ?", "replies": ["Hi Ramii - the .env file will help with enviroment variables used in your Dagster code, but not with a Dagster system environment variable like DAGSTER_HOME. Would something like this work to set the environment variable ?\n\n```DAGSTER_HOME=/your/directory/goes/here dagster dev```", "<@U04NU0RNX88>, you might also want to look at using a system like [direnv](<https://direnv.net/>) that will give you the benefits of a .env(rc) file and also automatically export those variables when you enter the directory. Note, it is just a convenience. In practice, it just automates what <@U016C4E5CP8> noted above about exporting the DAGSTER_HOME variable."], "is_dagster_empl": [true, false], "gpt4_replies_target": "To set the DAGSTER_HOME environment variable, you can use the command `DAGSTER_HOME=/your/directory/goes/here dagster dev`. Alternatively, you can use a system like [direnv](<https://direnv.net/>) that automatically exports those variables when you enter the directory.", "dagster_empl_first_target": "Hi Ramii - the .env file will help with enviroment variables used in your Dagster code, but not with a Dagster system environment variable like DAGSTER_HOME. Would something like this work to set the environment variable ?\n\n```DAGSTER_HOME=/your/directory/goes/here dagster dev```", "dagster_empl_last_target": "Hi Ramii - the .env file will help with enviroment variables used in your Dagster code, but not with a Dagster system environment variable like DAGSTER_HOME. Would something like this work to set the environment variable ?\n\n```DAGSTER_HOME=/your/directory/goes/here dagster dev```"}, {"question": "Hi Folks. Curious if anyone has come up with a convenient way to load up an interactive shell for one or more of the user deployment environments. We've got a self-hosted deploy using the Helm method in a GKE cluster. I'm sure I could come up with a way to spin up a pod using our same user deployment image and get an interactive shell in it, but I wanted to make sure there wasn't something more bespoke set up for it.\n\nMain use case is loading an interactive environment in k8s to debug and do on the fly analysis of dagster assets.", "replies": ["Something like this?\n```kubectl run -i --tty my_interactive --image=YOURUSER/CODEIMAGE -- sh```\n<https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#run>", "Brilliant, thank you", "Hey <@U04GRPANY3C>, I see you used GKE so I wanted to ask a question.. did you have to enable access to Dagster to the folks in your org? We are using Google at our company so it should be SSO as it's GKE but am not sure how..", "We are doing a few different things. For production, we use Cloudflare's Zero Trust network. This lets us access our production dagster instance at `<http://dagster.org-domain.io|dagster.org-domain.io>` but only if you have Cloudflare WARP on and you also login using our Google Workspace account. This doesn't really use any GKE specific auth, as it were.\n\nFor development environments, we have it set up so we can each deploy to our own namespaces in a dev cluster. Then we access those dagit instances using `kubectl` port-forwarding. When I have more time I will probably be able to set up subdomains for each of our environments also using Cloudflare.\n\nBoth production and dev environments are managed with `<http://garden.io|garden.io>`.", "Thanks <@U04GRPANY3C>. When you access via `kubectl port-forwarding` , did you do it via cloud shell? I am running that command via cloud shell and as soon as I leave the shell, the dagit UI stops working..", "did you face this issue? I see the dagit pod and other pods running but not sure how to access the UI if I don't run the above command", "No, I just use it in my local terminal. You have to keep it running in the background and unfortunately it's rather brittle.", "I typically run the kubectl port-forward in a `while true` bash loop, as it does die regularly.", "But cloud shell is not a good place to run it given that cloud shell also times out.", "exactly..! we can do it from the UI I guess.."], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "To load an interactive shell for one or more of the user deployment environments in a GKE cluster, you can spin up a pod using the same user deployment image and get an interactive shell in it using the command `kubectl run -i --tty my_interactive --image=YOURUSER/CODEIMAGE -- sh`. For accessing Dagster, you can use Cloudflare's Zero Trust network for production and `kubectl` port-forwarding for development environments.", "dagster_empl_first_target": "Something like this?\n```kubectl run -i --tty my_interactive --image=YOURUSER/CODEIMAGE -- sh```\n<https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands#run>", "dagster_empl_last_target": "exactly..! we can do it from the UI I guess.."}, {"question": "Trying to import my own repo and receiving this error:\n```dagster._core.errors.DagsterInvariantViolationError: No repositories, jobs, pipelines, graphs, asset groups, or asset definitions found in \"data\".\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_grpc/server.py\", line 256, in __init__\n    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_grpc/server.py\", line 114, in __init__\n    loadable_targets = get_loadable_targets(\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_grpc/utils.py\", line 57, in get_loadable_targets\n    else loadable_targets_from_python_package(package_name, working_directory)\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/workspace/autodiscovery.py\", line 52, in loadable_targets_from_python_package\n    return loadable_targets_from_loaded_module(module)\n  File \"/usr/local/lib/python3.8/site-packages/dagster/_core/workspace/autodiscovery.py\", line 133, in loadable_targets_from_loaded_module\n    raise DagsterInvariantViolationError(```\nbut I do have assets in this repo and this message looks cutoff... any ideas on how to debug?", "replies": ["Here are the logs from the serverless deployment", "Hi Kevin - do you see the same error if you checkout your github repo and run \"dagster dev\" locally? (That can be a bit faster than waiting for serverless to deploy your code)", "One common cause of this - the __init__ file of your package (`data` in this case) needs to actually import your assets. It doesn't automatically scan them just because they exist somewhere in the package.\n\nFor example if you look in the __init__ file in the quickstart package, it actually imports the assets there: <https://github.com/dagster-io/quickstart-etl/blob/main/quickstart_etl/__init__.py#L8>", "ah it was the init.py file", "thanks!"], "is_dagster_empl": [false, true, true, false, false], "gpt4_replies_target": "The error might be due to the __init__ file of your package not importing your assets. Ensure that your assets are imported in the __init__ file, as it doesn't automatically scan them just because they exist somewhere in the package.", "dagster_empl_first_target": "Hi Kevin - do you see the same error if you checkout your github repo and run \"dagster dev\" locally? (That can be a bit faster than waiting for serverless to deploy your code)", "dagster_empl_last_target": "One common cause of this - the __init__ file of your package (`data` in this case) needs to actually import your assets. It doesn't automatically scan them just because they exist somewhere in the package.\n\nFor example if you look in the __init__ file in the quickstart package, it actually imports the assets there: <https://github.com/dagster-io/quickstart-etl/blob/main/quickstart_etl/__init__.py#L8>"}, {"question": "Hi Everyone, Im facing the below error while trying to depoy dagster to openshift using azure pipeline. My deployment worked well for a long time but i get this error now. kindly assist :)\n\nImportError: cannot import name '_BindParamClause' from 'sqlalchemy.sql.expression' (/usr/local/lib/python3.7/site-packages/sqlalchemy/sql/expression.py)", "replies": ["If you update to the latest release, that\u2019s resolved. Alternatively, add an explicit dependency to sqlalchemy&lt;2.0.0", "They (sqlalchemy) released a major version upgrade and older dagster releases did not have an upper bound on the dependency, so pip is likely installing the new (incompatible) version since nothing is telling it not to", "Thanks Adam for the response, could you also let me know if I can add it like below in my docker file?\n\n RUN pip install \\\n    dbt-core \\\n    dbt-sqlserver \\\n    dagster==${DAGSTER_VERSION} \\\n    dagster-azure==${DAGSTER_VERSION} \\\n    dagster-postgres==${DAGSTER_VERSION} \\\n    dagster-k8s==${DAGSTER_VERSION} \\\n    dagster-aws==${DAGSTER_VERSION} \\\n    dagster-celery[flower,redis,kubernetes]==${DAGSTER_VERSION} \\\n    dagster-celery-k8s==${DAGSTER_VERSION} \\\n    dagster-gcp==${DAGSTER_VERSION} \\\n    dagster-graphql==${DAGSTER_VERSION} \\\n    dagit==${DAGSTER_VERSION} \\\n    sqlalchemy&lt;2.0.0", "yes the above worked with sqlalchemy/&lt;2.0.0. thankyou"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "The error you're encountering is due to a major version upgrade of sqlalchemy which is incompatible with older dagster releases. You can resolve this by either updating to the latest dagster release or adding an explicit dependency to sqlalchemy<2.0.0 in your Docker file.", "dagster_empl_first_target": "If you update to the latest release, that\u2019s resolved. Alternatively, add an explicit dependency to sqlalchemy&lt;2.0.0", "dagster_empl_last_target": "yes the above worked with sqlalchemy/&lt;2.0.0. thankyou"}, {"question": "Hi, how are you?\n\nIm new to dasgter, I am currently trying to make a concept test and after reading the docs/community there is a doubt that i cant resolve.\n\nMy final goal is to create a Dasgter proyect that reads information from a SQL server, calculates per employee diferents kpi/grafics (similar to a Power BI Dashboard) and then send them thorough email to each employee.\n\nHow is the best practice to \"iterate\" trough diferent parameters and use them? (in assets or opps)\n\nI was thinking in a solution something looking similar to the photo attached\n\nThanks in advance!\n\nPD: Sorry for the ignorance :slightly_smiling_face:", "replies": ["Hi Alexis! An asset is typically an object in persistent storage (i.e. a file or a table). I would say emails would not be an asset as they are not persisted. And since the SQL server and excel file are not being updated, I would recommend representing these operations as `op`s instead that read from these sources.\n\nIf I were to implement this myself, I'd do something like this:", "We do have a doc describing when to use assets: <https://docs.dagster.io/guides/dagster/enriching-with-software-defined-assets#when-should-i-use-software-defined-assets>\n\nAnd in the image I added above, you could use <https://docs.dagster.io/concepts/ops-jobs-graphs/dynamic-graphs#using-dynamic-outputs|dynamic outputs> to create a \"loop\". A dynamic output allows you to yield an arbitrary number of outputs from an op, and then map over these outputs in a downstream op, running any per-output operations as needed.", "I will check out wat you send me, thanks!!"], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "To iterate through different parameters in Dagster, you can use dynamic outputs to create a \"loop\". This allows you to yield an arbitrary number of outputs from an operation (op), and then map over these outputs in a downstream op, running any per-output operations as needed. Emails would not be considered an asset as they are not persisted, and operations that read from sources like a SQL server or an excel file are best represented as `op`s.", "dagster_empl_first_target": "Hi Alexis! An asset is typically an object in persistent storage (i.e. a file or a table). I would say emails would not be an asset as they are not persisted. And since the SQL server and excel file are not being updated, I would recommend representing these operations as `op`s instead that read from these sources.\n\nIf I were to implement this myself, I'd do something like this:", "dagster_empl_last_target": "We do have a doc describing when to use assets: <https://docs.dagster.io/guides/dagster/enriching-with-software-defined-assets#when-should-i-use-software-defined-assets>\n\nAnd in the image I added above, you could use <https://docs.dagster.io/concepts/ops-jobs-graphs/dynamic-graphs#using-dynamic-outputs|dynamic outputs> to create a \"loop\". A dynamic output allows you to yield an arbitrary number of outputs from an op, and then map over these outputs in a downstream op, running any per-output operations as needed."}, {"question": "since today, I am experiencing the Repository navbar on the left in Dagit UI seems to be shortening Repository names randomly and also flickering UI elements. This only happens on Firefox for me. The issue is also replicated when tried on another machine and display. Wondering if anyone else has also been facing the same?\n<https://github.com/dagster-io/dagster/issues/11489>", "replies": ["Hi Sanidhya, would you be able to provide a screenshot of this?", "Hi Claire,\nLooks like <@U01AJKLN5A9> already put in a fix for it and it will be available from the next release. The issue is closed now."], "is_dagster_empl": [true, false], "gpt4_replies_target": "The issue with the Repository navbar in Dagit UI on Firefox has been addressed and a fix will be available in the next release.", "dagster_empl_first_target": "Hi Sanidhya, would you be able to provide a screenshot of this?", "dagster_empl_last_target": "Hi Sanidhya, would you be able to provide a screenshot of this?"}, {"question": "is the only way to pre-set the multiprocess executor config on a job without having to go through `.configured()` and provide a whole new config schema if I don't want to pre-set all the other parameters? seems like there has to be", "replies": ["Ooo seems like maybe I can use `.configured()` on the `multiprocess_executor` and pass that to the job definition, that's much nicer"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, you can use `.configured()` on the `multiprocess_executor` and pass that to the job definition as a simpler way to pre-set the multiprocess executor config on a job.", "dagster_empl_first_target": "Ooo seems like maybe I can use `.configured()` on the `multiprocess_executor` and pass that to the job definition, that's much nicer", "dagster_empl_last_target": "Ooo seems like maybe I can use `.configured()` on the `multiprocess_executor` and pass that to the job definition, that's much nicer"}, {"question": "I'm trying to build two similar Graph Assets and would like to reuse one or more of the same Op definitions in both but would like to set different Config on the Op at design time. Is this possible? I can't seem to pass a Config instance to the Op inside of the Graph Asset definition.", "replies": ["maybe an <https://docs.dagster.io/concepts/ops-jobs-graphs/ops#op-factory|op factory> might help?", "Thanks <@U0300665S8G> I'll give that a shot! I've used op factories before, but the docs seem to recommend this mostly if you need to change the arguments themselves. I had interpreted that as changing the list of arguments and not the values passed to them (Config objects don't seem \"passable\" though, I guess -- I may end up handling this using a standard function argument with different defaults instead of by type hinting different Config objects in the factory)."], "is_dagster_empl": [false, false], "gpt4_replies_target": "Yes, it is possible to set different Config on the same Op in different Graph Assets. You can achieve this by using an op factory, which allows you to change the arguments and their values.", "dagster_empl_first_target": "maybe an <https://docs.dagster.io/concepts/ops-jobs-graphs/ops#op-factory|op factory> might help?", "dagster_empl_last_target": "Thanks <@U0300665S8G> I'll give that a shot! I've used op factories before, but the docs seem to recommend this mostly if you need to change the arguments themselves. I had interpreted that as changing the list of arguments and not the values passed to them (Config objects don't seem \"passable\" though, I guess -- I may end up handling this using a standard function argument with different defaults instead of by type hinting different Config objects in the factory)."}, {"question": "We have about 100 time series we want to use, along with some new ones that might be created by analysts. How should we think about this, create 100 individual assets and all new ones needs to be defined in code as well sounds like the correct approach. But is there another way or methodology i am missing?", "replies": ["If how they are being extracted and outputted are just the same, you can create an asset factory. See this pattern here: <https://docs.dagster.io/concepts/ops-jobs-graphs/ops#op-factory> (just the same as op factory)", "And yes, they do need to be defined in the code. What you can do is have some config file, read that config file that specifies your asset locations, and create an asset through an asset factory", "if they are the similar business logic but vary based on time, i\u2019d also recommend trying out partitions, especially if it\u2019s time based partitions: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#partitioned-assets>", "Great will take a deeper look, been looking at partioning as well so both seems like great options to evaluate."], "is_dagster_empl": [false, false, true, false], "gpt4_replies_target": "You can create an asset factory if the extraction and output methods are the same for all time series. They need to be defined in the code, but you can use a config file to specify asset locations. If the business logic is similar but varies based on time, consider using partitions, especially time-based ones.", "dagster_empl_first_target": "if they are the similar business logic but vary based on time, i\u2019d also recommend trying out partitions, especially if it\u2019s time based partitions: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#partitioned-assets>", "dagster_empl_last_target": "if they are the similar business logic but vary based on time, i\u2019d also recommend trying out partitions, especially if it\u2019s time based partitions: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#partitioned-assets>"}, {"question": "This message was deleted.", "replies": ["Okay I was searching way too deep to understand, but i found the solution :\n\n@job functions should not have any \"return\" :slightly_smiling_face: !"], "is_dagster_empl": [false], "gpt4_replies_target": "The solution to your issue is that @job functions in Dagster should not have any \"return\".", "dagster_empl_first_target": "Okay I was searching way too deep to understand, but i found the solution :\n\n@job functions should not have any \"return\" :slightly_smiling_face: !", "dagster_empl_last_target": "Okay I was searching way too deep to understand, but i found the solution :\n\n@job functions should not have any \"return\" :slightly_smiling_face: !"}, {"question": "How is everyone else handling assets that aren\u2019t partitioned but should be historized? E.g. in budget planning, department heads will update spreadsheets, often intraday, and the lake should contain all historical versions of it. Though I know S3 can keep old versions of files, working with this versioning system seems a little cumbersome. My current approach is to append a load timestamp to the filename, but this sometimes causes problems loading the asset downstream (could just be me not fully battle-testing my IO Manager code though)", "replies": ["I just asked this question yesterday. The suggestion we are trying now is to use asset partitions. The safety measure in place is then to not produce an asset if the current day (or whatever frequency you are using) doesn't match up with the partition being generated. See brief thread here: <https://dagster.slack.com/archives/C01U954MEER/p1675888677787539>\n\nSo every new day, the new \"partition\" will be fetched which is really the entire resource snapshotted in time. Then there is a check in that function to either throw an error or something else if the partition window doesn't match up. That will prevent old partitions from getting re-materialized which would not be correct time snapshots.", "Hm, that\u2019s an interesting approach, though I can see my OCD kicking in when looking at the backfill list being mostly empty since the job doesn\u2019t need to run daily. I\u2019d also maybe like to have multiple versions of the asset that pertain to the same day", "yeah, if your partition definition is complicated this might be a pain. and if it fails for whatever reason you will have \"forever missing\" partitions, though that shouldn't be a problem in most downstream cases. The built in partitions definitions have daily, weekly, and monthly I believe.\n\nOtherwise I would probably do it as the IOManager level, like you were suggesting, which was one of the approaches we were considering.\n\nAnother option altogether is to not bother with the `asset` stuff and just schedule an op that pushes to a bucket where you tell it to go."], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "One approach to handle non-partitioned assets that need to be historized is to use asset partitions and ensure that an asset is not produced if the current day doesn't match the partition being generated. Alternatively, you can manage it at the IOManager level or schedule an operation that pushes to a specific bucket.", "dagster_empl_first_target": "I just asked this question yesterday. The suggestion we are trying now is to use asset partitions. The safety measure in place is then to not produce an asset if the current day (or whatever frequency you are using) doesn't match up with the partition being generated. See brief thread here: <https://dagster.slack.com/archives/C01U954MEER/p1675888677787539>\n\nSo every new day, the new \"partition\" will be fetched which is really the entire resource snapshotted in time. Then there is a check in that function to either throw an error or something else if the partition window doesn't match up. That will prevent old partitions from getting re-materialized which would not be correct time snapshots.", "dagster_empl_last_target": "yeah, if your partition definition is complicated this might be a pain. and if it fails for whatever reason you will have \"forever missing\" partitions, though that shouldn't be a problem in most downstream cases. The built in partitions definitions have daily, weekly, and monthly I believe.\n\nOtherwise I would probably do it as the IOManager level, like you were suggesting, which was one of the approaches we were considering.\n\nAnother option altogether is to not bother with the `asset` stuff and just schedule an op that pushes to a bucket where you tell it to go."}, {"question": "Slack invite link on <https://dagster.io/community> is not valid anymore (should not be connected to slack to test it)", "replies": ["Thanks for the head\u2019s up - we are working on fixing that.", "Hi <@U04MEJCSLCF> - this issue has been resolved.  Do let me know if you are still having problems getting people in to our Slack community."], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue with the invalid Slack invite link on the Dagster community page has been acknowledged and resolved.", "dagster_empl_first_target": "Thanks for the head\u2019s up - we are working on fixing that.", "dagster_empl_last_target": "Hi <@U04MEJCSLCF> - this issue has been resolved.  Do let me know if you are still having problems getting people in to our Slack community."}, {"question": "QQ: is there any way to have materialization of software defined assets marked as stale after x-hours or x-days since the last materialization?", "replies": ["I believe \"freshness policy\" is what you should look for in docs", "Yep, that solves it"], "is_dagster_empl": [false, false], "gpt4_replies_target": "Yes, you can mark software defined assets as stale after a certain period by implementing a \"freshness policy\".", "dagster_empl_first_target": "I believe \"freshness policy\" is what you should look for in docs", "dagster_empl_last_target": "Yep, that solves it"}, {"question": "QQ: is there a difference between \"io_manager_def\" and \"io_manager_key\" ?", "replies": ["Yeah! `io_manager_def` takes an <https://docs.dagster.io/_apidocs/io-managers#dagster.IOManagerDefinition|IOManagerDefinition>, whereas `io_manager_key` takes a string which is the resource key that maps to the `io_manager`  object. If you have an io manager defined in your resources dictionary that you pass to your `Definitions` object, you\u2019ll probably want to use `io_manager_key` to reference that io manager.", "Personally, I mostly use `io_manager_key` . But it can be helpful to use `io_manager_def` for one-off io managers that won\u2019t be used elsewhere in your project"], "is_dagster_empl": [false, false], "gpt4_replies_target": "Yes, there is a difference. `io_manager_def` takes an IOManagerDefinition, while `io_manager_key` takes a string which is the resource key that maps to the `io_manager` object.", "dagster_empl_first_target": "Yeah! `io_manager_def` takes an <https://docs.dagster.io/_apidocs/io-managers#dagster.IOManagerDefinition|IOManagerDefinition>, whereas `io_manager_key` takes a string which is the resource key that maps to the `io_manager`  object. If you have an io manager defined in your resources dictionary that you pass to your `Definitions` object, you\u2019ll probably want to use `io_manager_key` to reference that io manager.", "dagster_empl_last_target": "Personally, I mostly use `io_manager_key` . But it can be helpful to use `io_manager_def` for one-off io managers that won\u2019t be used elsewhere in your project"}, {"question": "We are getting this issue do we have an eta on when dbt-core 1.4.0 will be supported?\n\n```dagster_dbt.errors.DagsterDbtCliHandledRuntimeError: Handled error in the dbt CLI (return code 1)\nParsed CLI Messages\nRunning with dbt=1.2.4\nFound 214 models, 245 tests, 0 snapshots, 0 analyses, 287 macros, 0 operations, 0 seed files, 150 sources, 0 exposures, 0 metrics\nConcurrency: 8 threads (target='dev')\n1 of 3 START table model prod_dbt_intermediate.ea_union_surface_water_flooding . [RUN]\n1 of 3 ERROR creating table model prod_dbt_intermediate.ea_union_surface_water_flooding [ERROR in 0.61s]\n2 of 3 SKIP relation prod_dbt_final.ea_surface_water_flooding .................. [SKIP]\n3 of 3 SKIP relation prod_dbt_final_mbtiles.ea_surface_water_flooding_mbtiles .. [SKIP]\nFinished running 3 table models in 0 hours 0 minutes and 2.40 seconds (2.40s).\nCompleted with 1 error and 0 warnings:\nCompilation Error in macro statement (macros/etc/statement.sql)\nObject of type TypeError is not JSON serializable\n\n&gt; in macro materialization_table_bigquery (macros/materializations/table.sql)\n&gt; called by macro statement (macros/etc/statement.sql)\nDone. PASS=0 WARN=0 ERROR=1 SKIP=2 TOTAL=3\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_plan.py\", line 265, in dagster_event_sequence_for_step\n    for step_event in check.generator(step_events):\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py\", line 382, in core_dagster_event_sequence_for_step\n    for user_event in check.generator(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/execution/plan/execute_step.py\", line 94, in _step_output_error_checked_user_event_sequence\n    for user_event in user_event_sequence:\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/execution/plan/compute.py\", line 177, in execute_core_compute\n    for step_output in _yield_compute_results(step_context, inputs, compute_fn):\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/execution/plan/compute.py\", line 145, in _yield_compute_results\n    for event in iterate_with_context(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_utils/__init__.py\", line 457, in iterate_with_context\n    next_output = next(iterator)\n  File \"/usr/local/lib/python3.10/site-packages/dagster_dbt/asset_defs.py\", line 420, in _dbt_op\n    dbt_output = dbt_resource.run(**kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/dagster_dbt/cli/resources.py\", line 145, in run\n    return self.cli(\"run\", models=models, exclude=exclude, select=select, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/dagster_dbt/cli/resources.py\", line 90, in cli\n    return execute_cli(\n  File \"/usr/local/lib/python3.10/site-packages/dagster_dbt/cli/utils.py\", line 146, in execute_cli\n    raise DagsterDbtCliHandledRuntimeError(messages=messages)```", "replies": [":blob_thinking_eyes: `Running with dbt=1.2.4` are you sure this is a dbt 1.4 issue?", "Hi Mitchell, `dbt 1.4.0` is supported in the release going out later today"], "is_dagster_empl": [false, true], "gpt4_replies_target": "The support for `dbt 1.4.0` is expected to be included in the release that is going out later today.", "dagster_empl_first_target": "Hi Mitchell, `dbt 1.4.0` is supported in the release going out later today", "dagster_empl_last_target": "Hi Mitchell, `dbt 1.4.0` is supported in the release going out later today"}, {"question": "Hey Dagster, I have a question about the docker deploy example here - <https://github.com/dagster-io/dagster/blob/1.1.17/examples/deploy_docker/docker-compose.yml#L16-L67> - How would this look if I wanted to use a workspace or a module instead of a single repo.py file?", "replies": ["Hi aaron - the example actually uses a workspace.yaml file: <https://github.com/dagster-io/dagster/blob/1.1.17/examples/deploy_docker/workspace.yaml>\n\nWhat code the server is using is here: <https://github.com/dagster-io/dagster/blob/1.1.17/examples/deploy_docker/Dockerfile_user_code#L23>\n\nYou could reference a module there instead with '-m' instead of a file with '-f'", "Thanks for the reply <@U016C4E5CP8> - What is the gRPC server for? I thought that is for hosting my code files. I mount my module src into the `opt/dagster/app` folder on the code server, is that the right way to go about this? In production would we just copy the module over directly?", "the gRPC server is for loading your Dagster code, yeah", "Mounting a volume is one way to get your code available, yeah - there is some guidance on how to do that here: <https://docs.dagster.io/deployment/guides/docker#mounting-volumes>", "Thanks, I have been reading that. I am at a place now where I have my compose building and starting the code server, daemon and dagit. That being said, I cannot see any of my jobs in dagit. If I try adding the `/opt/dagster/app` as the `-m` module the code server throws an error saying there is no module named. Maybe it has something to do with my module structure within the code server", "are you able to load the jobs locally in dagit?", "yes, let me run again to confirm", "running \"python -m &lt;module name&gt;\" in the same folder as the grpc server would be pretty similar to what the grpc server is doing under the hood, and could help rule out dagster-specific issues", "let me give that a try - I have python 3.11 installed locally so have to run it all through docker because gRPC can't install on 3.11", "I think I understand it better now. Just running into the issue of how do I install the module. Is there a way I can just set the working directory?", "there's a --working-directory argument, yeah", "that determines the root folder for local imports in your python code", "you don't neccesarily have to install the module, it just has to be able to load in python", "ahh, so in this case I would just need to make sure my requirements are installed first", "let me try this", "I keep getting the below error - does that mean it can't load the module?\n```File \"/usr/local/lib/python3.10/site-packages/dagster/_check/__init__.py\", line 1699, in failed\n    raise CheckError(f\"Failure condition: {desc}\")\ndagster._check.CheckError: Failure condition: invalid```", "do you have a full stack trace?", "```root@e2c0c23a4b72:/opt/dagster/app# dagster api grpc -h 0.0.0.0 -p 4000 --working-directory ./data_pipeline_dagster\nTraceback (most recent call last):\n  File \"/usr/local/bin/dagster\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_cli/__init__.py\", line 46, in main\n    cli(auto_envvar_prefix=ENV_PREFIX)  # pylint:disable=E1123\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.10/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_cli/api.py\", line 702, in grpc_command\n    server = DagsterGrpcServer(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_grpc/server.py\", line 916, in __init__\n    self._api_servicer = DagsterApiServer(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_grpc/server.py\", line 256, in __init__\n    self._loaded_repositories: Optional[LoadedRepositories] = LoadedRepositories(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_grpc/server.py\", line 114, in __init__\n    loadable_targets = get_loadable_targets(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_grpc/utils.py\", line 60, in get_loadable_targets\n    check.failed(\"invalid\")\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_check/__init__.py\", line 1699, in failed\n    raise CheckError(f\"Failure condition: {desc}\")\ndagster._check.CheckError: Failure condition: invalid```", "Ah that's a bad error message, but you also need to include a module or a python file", "working directory tells it what base folder to use for local imports, but doesn't tell it what module or file to try to import", "so the command also needs -m?", "Yeah", "Do you have a corresponding python command that's working that you want to map onto the dagster command?", "I do not, we have just been running the daemon locally from the root folder and testing using the dagit ui", "How have you been telling the daemon which code to load?", "haven't been passing any parameters. Just `dagster-daemon run`", "for that to work, there would need to either be a pyproject.toml or a workspace.yaml in the same folder that tells it what code to load", "let me show you my folder structure, maybe that will help", "I think step 1 is getting the code to load via `python` - at that point, getting it to load in `dagster api grpc` should be straighforward", "I see, so once I can run `python -m &lt;module&gt;` it should load through grpc", "yeah", "ok, so let me work through that", "Running `python -m` when I am loading my resources `__init__.py` it can't find the environment variables. I have a `.env` file, but with docker compose would most likely use docker environment variables", "ok, fixed the errors, but now I am getting `'data_pipeline_dagster' is a package and cannot be directly executed`", "does it need to be executed?", "this is when running python? or dagster?", "python", "what's the command exactly?", "`python -m data_pipeline_dagster`", "what version of python are you using?", "locally 3.11.0, in the containers 3.10.9", "I can try from inside the code container", "might be a 3.11 thing then - you could run 'import data_pipeline_dagster' inside a Python shell then", "e.g.\n```python\n&gt;&gt;&gt; import data_pipeline_dagster```", "```\\AppData\\Roaming\\Python\\Python311\\site-packages\\dagster\\_core\\definitions\\input.py:563: ExperimentalWarning: \"asset_key\" is an experimental argument to function \"InputDefinition.__init__\". It may break in future versions, even between dot releases. To mute warnings for experimental functionality, invoke warnings.filterwarnings(\"ignore\", category=dagster.ExperimentalWarning) or use one of the other methods described at <https://docs.python.org/3/library/warnings.html#describing-warning-filters>.  return InputDefinition(\nC:\\Users\\atawil\\AppData\\Roaming\\Python\\Python311\\site-packages\\dagster\\_core\\definitions\\input.py:316: ExperimentalWarning: \"asset_key\" is an experimental argument to function \"InputDefinition.__init__\". It may break in future versions, even between dot releases. To mute warnings for experimental functionality, invoke warnings.filterwarnings(\"ignore\", category=dagster.ExperimentalWarning) or use one of the other methods described at <https://docs.python.org/3/library/warnings.html#describing-warning-filters>.  return InputDefinition(```", "let me try the same from the container", "if there are no errors that's good", "yes, just looks like warnings", "then i would expect 'dagit -m data_pipeline_dagster' to work or 'dagster api grpc -m data_pipeline_dagster -p 4000' to work too", "let me see", "it's working now!", "Some issues with the module itself causing the code server to not initialize properly. Now I just have to clean it up", "really appreciate the help Daniel!"], "is_dagster_empl": [true, false, true, true, false, true, false, true, false, false, true, true, true, false, false, false, true, false, true, true, false, true, true, false, true, false, true, false, true, false, true, false, false, false, false, true, false, true, false, true, false, false, true, true, false, false, true, false, true, false, false, false, false], "gpt4_replies_target": "To use a workspace or a module instead of a single repo.py file in the docker deploy example, you can reference a module with '-m' instead of a file with '-f'. The gRPC server is for loading your Dagster code. You can mount your module src into the `opt/dagster/app` folder on the code server. If you cannot see any of your jobs in dagit, you can try running \"python -m &lt;module name&gt;\" in the same folder as the grpc server to rule out dagster-specific issues.", "dagster_empl_first_target": "Hi aaron - the example actually uses a workspace.yaml file: <https://github.com/dagster-io/dagster/blob/1.1.17/examples/deploy_docker/workspace.yaml>\n\nWhat code the server is using is here: <https://github.com/dagster-io/dagster/blob/1.1.17/examples/deploy_docker/Dockerfile_user_code#L23>\n\nYou could reference a module there instead with '-m' instead of a file with '-f'", "dagster_empl_last_target": "then i would expect 'dagit -m data_pipeline_dagster' to work or 'dagster api grpc -m data_pipeline_dagster -p 4000' to work too"}, {"question": "Can multi-assets be partitioned? I have a static partitioned asset that was working just fine, but am now converting it to a multi-asset since it really produces two distinct outputs. Dagit is telling me `Failure condition: The output has no asset partitions` and I am having trouble connecting that to something in documentation", "replies": ["I think it may be that I am using `context.asset_partition_key_for_output()` in the `@multi_asset`", "Yup, needed to be `context.partition_key`"], "is_dagster_empl": [false, false], "gpt4_replies_target": "Yes, multi-assets can be partitioned. However, you need to use `context.partition_key` instead of `context.asset_partition_key_for_output()` in the `@multi_asset`.", "dagster_empl_first_target": "I think it may be that I am using `context.asset_partition_key_for_output()` in the `@multi_asset`", "dagster_empl_last_target": "Yup, needed to be `context.partition_key`"}, {"question": "Hey there! I was wondering if there are any plans to support DataForm in the near future.", "replies": ["Hi Pablo, not at present-- but, I encourage you to file it as a request on our <https://github.com/dagster-io/dagster/issues|issue tracker>."], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, there are no plans to support DataForm in Dagster, but you are encouraged to file this as a request on their issue tracker.", "dagster_empl_first_target": "Hi Pablo, not at present-- but, I encourage you to file it as a request on our <https://github.com/dagster-io/dagster/issues|issue tracker>.", "dagster_empl_last_target": "Hi Pablo, not at present-- but, I encourage you to file it as a request on our <https://github.com/dagster-io/dagster/issues|issue tracker>."}, {"question": "How can I create an `op` that is a dependency of an asset that isn\u2019t defined explicitly in code as a Python function? This is a more general version of my question <https://dagster.slack.com/archives/C04CW71AGBW/p1675894319440339|here>.", "replies": ["Hi Graham, This functionality should be released next week-- just merged in this PR: <https://github.com/dagster-io/dagster/pull/12091>", "Excellent timing, I\u2019ll be looking forward to giving it a try"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The functionality to create an `op` that is a dependency of an asset not defined explicitly in code as a Python function is currently being developed and should be released next week as per this PR: <https://github.com/dagster-io/dagster/pull/12091>.", "dagster_empl_first_target": "Hi Graham, This functionality should be released next week-- just merged in this PR: <https://github.com/dagster-io/dagster/pull/12091>", "dagster_empl_last_target": "Hi Graham, This functionality should be released next week-- just merged in this PR: <https://github.com/dagster-io/dagster/pull/12091>"}, {"question": "In a multi-container docker deployment, is `max_workers` set in `workspace.yaml` to increase the number of concurrent workers? Or is it set elsewhere?", "replies": ["FWIW, this threw me for a loop because the format is `max_workers` and not `max-workers` like it is with other params in the `CMD` line. I ended up with\n```CMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"--max_workers\", \"3\", \"--module-name\", \"my_code_module\"]```", "Good catch Clay. Definitely a usability problem we will fix: <https://github.com/dagster-io/dagster/pull/12246>"], "is_dagster_empl": [false, true], "gpt4_replies_target": "In a multi-container docker deployment, `max_workers` is set in the `CMD` line, not in `workspace.yaml`, to increase the number of concurrent workers. However, note that the format is `max_workers` and not `max-workers` as it is with other parameters.", "dagster_empl_first_target": "Good catch Clay. Definitely a usability problem we will fix: <https://github.com/dagster-io/dagster/pull/12246>", "dagster_empl_last_target": "Good catch Clay. Definitely a usability problem we will fix: <https://github.com/dagster-io/dagster/pull/12246>"}, {"question": "I'm using a IO manager for html to S3, similar to what is outlined in <https://dagster.slack.com/archives/C01U954MEER/p1671105942227109|this thread>? I need to add metadata to my S3 file when saving it, has anyone been able to successfully do this?", "replies": ["Hi Noah - this isn't currently supported by the built-in S3 IO manager, so you'd need to copy the code an modify it to write the metadata you need\n\nIf you'd be up for filing a github issue with this request, this would help us track it and address it at some point in the future", "Appreciate it! Was able to overwrite the metadata with another asset but will definitely file an issue! "], "is_dagster_empl": [true, false], "gpt4_replies_target": "The current built-in S3 IO manager in Dagster does not support adding metadata to S3 files, so you would need to modify the code to write the necessary metadata.", "dagster_empl_first_target": "Hi Noah - this isn't currently supported by the built-in S3 IO manager, so you'd need to copy the code an modify it to write the metadata you need\n\nIf you'd be up for filing a github issue with this request, this would help us track it and address it at some point in the future", "dagster_empl_last_target": "Hi Noah - this isn't currently supported by the built-in S3 IO manager, so you'd need to copy the code an modify it to write the metadata you need\n\nIf you'd be up for filing a github issue with this request, this would help us track it and address it at some point in the future"}, {"question": "heya, I'm running a sensor in prod but it keeps failing with this error msg -\n```dagster._core.errors.DagsterUnknownPartitionError: Could not find a partition with key `2023-02-09````\nit looks like there's some sort of limit on available partitions with the max being the previous day? is this something we can change?", "replies": ["ah i believe it has something to do with this? <https://docs.dagster.io/_apidocs/partitions#dagster.daily_partitioned_config>", "will try adding `end_offset=1` and re-try", "Hi Joy, did you solve your problem?", "yup that did the trick", "thanks!"], "is_dagster_empl": [false, false, true, false, false], "gpt4_replies_target": "The issue with the sensor failing due to the \"DagsterUnknownPartitionError\" can be resolved by adding `end_offset=1` in the daily partitioned config as per the Dagster documentation.", "dagster_empl_first_target": "Hi Joy, did you solve your problem?", "dagster_empl_last_target": "Hi Joy, did you solve your problem?"}, {"question": "I'm curious if this is a bug... My code location container has this `RUN` line at the end:\n\n```CMD [\"dagster\", \"api\", \"grpc\", \"-h\", \"0.0.0.0\", \"-p\", \"4000\", \"--max_workers\", \"3\", \"--module-name\", \"my_module\"]```\nIf I materialize all of the assets in a job, dagster spins up 3 workers to handle concurrent processing. However, once it is all complete, if I again materialize all of the assets in the job (as I was doing as part of testing), dagster only spins up 2 workers (which is default). Is that expected or should I be specifying `--max_workers` elsewhere, too?", "replies": ["Hey clay - the \"max-workers\" parameter there is referring to a number of threads on the grpc server, it's not directly related to dagster execution. It sounds like you may be looking for the 'max_concurrent' parameter here, which controls how many ops execute at once in a given run: <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#default-job-executor>", "(and is specified in configuration for the job)", "ah, ok! Thanks!", "So, in a docker multi-container deployment, `max_concurrent` in the job will determine how many code location containers are launched to complete the work?", "max_concurrent will determine how many processes get spun up for a particular job. Where those processes are running will depend on which executor you pick", "the default executor does each op in a subprocess", "if you use the docker_executor, each op will execute in a different docker container", "ok, thanks for clarifying!"], "is_dagster_empl": [true, true, false, false, true, true, true, false], "gpt4_replies_target": "The \"max-workers\" parameter refers to the number of threads on the grpc server, not directly related to dagster execution. If you want to control how many ops execute at once in a given run, you should use the 'max_concurrent' parameter in the configuration for the job. The number of processes that get spun up for a particular job will depend on which executor you pick.", "dagster_empl_first_target": "Hey clay - the \"max-workers\" parameter there is referring to a number of threads on the grpc server, it's not directly related to dagster execution. It sounds like you may be looking for the 'max_concurrent' parameter here, which controls how many ops execute at once in a given run: <https://docs.dagster.io/concepts/ops-jobs-graphs/job-execution#default-job-executor>", "dagster_empl_last_target": "if you use the docker_executor, each op will execute in a different docker container"}, {"question": "hey guys, I'm having trouble with loading assets from airbyte, keep getting this error `resource with key 'airbyte' required by op 'load_airbyte_assets' was not provided`any idea what I'm missing here?", "replies": ["I have the airbyte resource defined and registered with the definitions in my main code entry point", "You shouldn't need to rewrap the resource - you should just be able to use `my_airbyte_resource` in your definitions", "there's a detailed example available at <https://docs.dagster.io/integrations/airbyte#using-airbyte-with-dagster>, if you haven't seen that yet.", "i got it working by scrapping my attempt to use the resource and going with the `load_assets_from_airbyte_instance` function"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "It seems like the 'airbyte' resource required by 'load_airbyte_assets' is not defined or registered properly. You can refer to the detailed example at <https://docs.dagster.io/integrations/airbyte#using-airbyte-with-dagster> or try using the `load_assets_from_airbyte_instance` function instead.", "dagster_empl_first_target": "I have the airbyte resource defined and registered with the definitions in my main code entry point", "dagster_empl_last_target": "i got it working by scrapping my attempt to use the resource and going with the `load_assets_from_airbyte_instance` function"}, {"question": "Hi, Is there a repository with a docker compose setup so I can get started quickly just by downloading it and running `docker-compose up`?", "replies": ["Oh, nvm. Found it: <https://github.com/dagster-io/dagster/tree/master/examples/deploy_docker>"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, you can find a repository with a docker compose setup in the following link: <https://github.com/dagster-io/dagster/tree/master/examples/deploy_docker>.", "dagster_empl_first_target": "Oh, nvm. Found it: <https://github.com/dagster-io/dagster/tree/master/examples/deploy_docker>", "dagster_empl_last_target": "Oh, nvm. Found it: <https://github.com/dagster-io/dagster/tree/master/examples/deploy_docker>"}, {"question": "Hi, I'm running dagster locally with sqlite storage. It crashed out and everytime I try to start dagit or dagster-daemon I get the following sqlite3 error. Has anyone else experienced this? Is there an easy way to recover or am I better off deleting the database (it's a testing one so no big deal)?\nThanks...\n```  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1901, in _execute_context\n    cursor, statement, parameters, context\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n    cursor.execute(statement, parameters)\nsqlite3.OperationalError: disk I/O error\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/home/terry/.pyenv/versions/etl_new_env/bin/dagster-daemon\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_daemon/cli/__init__.py\", line 143, in main\n    cli(obj={})  # pylint:disable=E1123\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_daemon/cli/__init__.py\", line 55, in run_command\n    ) if instance_ref else DagsterInstance.get() as instance:\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 479, in get\n    return DagsterInstance.from_config(dagster_home_path)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 495, in from_config\n    return DagsterInstance.from_ref(instance_ref)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/instance/__init__.py\", line 507, in from_ref\n    unified_storage = instance_ref.storage\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/instance/ref.py\", line 463, in storage\n    return self.storage_data.rehydrate() if self.storage_data else None\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_serdes/config_class.py\", line 99, in rehydrate\n    return klass.from_config_value(self, check.not_none(result.value))\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/storage/sqlite_storage.py\", line 81, in from_config_value\n    return DagsterSqliteStorage.from_local(inst_data=inst_data, **config_value)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/storage/sqlite_storage.py\", line 87, in from_local\n    return cls(base_dir, inst_data=inst_data)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/storage/sqlite_storage.py\", line 63, in __init__\n    self._run_storage = SqliteRunStorage.from_local(_runs_directory(base_dir))\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/storage/runs/sqlite/sqlite_run_storage.py\", line 91, in from_local\n    db_revision, head_revision = check_alembic_revision(alembic_config, connection)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/dagster/_core/storage/sql.py\", line 77, in check_alembic_revision\n    db_revision = migration_context.get_current_revision()\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/alembic/runtime/migration.py\", line 487, in get_current_revision\n    heads = self.get_current_heads()\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/alembic/runtime/migration.py\", line 534, in get_current_heads\n    if not self._has_version_table():\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/alembic/runtime/migration.py\", line 552, in _has_version_table\n    self.connection, self.version_table, self.version_table_schema\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/alembic/util/sqla_compat.py\", line 244, in _connectable_has_table\n    return inspect(connectable).has_table(tablename, schemaname)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/reflection.py\", line 283, in has_table\n    return self.dialect.has_table(conn, table_name, schema)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/dialects/sqlite/base.py\", line 2020, in has_table\n    connection, \"table_info\", table_name, schema=schema\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/dialects/sqlite/base.py\", line 2604, in _get_table_pragma\n    cursor = connection.exec_driver_sql(statement)\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1765, in exec_driver_sql\n    future=True,\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1676, in _exec_driver_sql\n    distilled_parameters,\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1944, in _execute_context\n    e, statement, parameters, cursor, context\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 2125, in _handle_dbapi_exception\n    sqlalchemy_exception, with_traceback=exc_info[2], from_=e\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/util/compat.py\", line 211, in raise_\n    raise exception\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/base.py\", line 1901, in _execute_context\n    cursor, statement, parameters, context\n  File \"/home/terry/.pyenv/versions/3.7.11/envs/etl_new_env/lib/python3.7/site-packages/sqlalchemy/engine/default.py\", line 736, in do_execute\n    cursor.execute(statement, parameters)\nsqlalchemy.exc.OperationalError: (sqlite3.OperationalError) disk I/O error\n[SQL: PRAGMA main.table_info(\"alembic_version\")]```", "replies": ["update. deleting didn't help. it started and then crapped out with the graphql error\n\n```Operation name: InstanceWarningQuery\n\nMessage: (sqlite3.OperationalError) disk I/O error\n[SQL: SELECT daemon_heartbeats.body \nFROM daemon_heartbeats]\n(Background on this error at: <https://sqlalche.me/e/14/e3q8>)\n\nPath: [\"instance\",\"daemonHealth\",\"allDaemonStatuses\"]\n\nLocations: [{\"line\":30,\"column\":3}]```", "ok. this was me running out of disk space..."], "is_dagster_empl": [false, false], "gpt4_replies_target": "The error you're experiencing is due to running out of disk space. You can resolve this by freeing up some space on your disk.", "dagster_empl_first_target": "update. deleting didn't help. it started and then crapped out with the graphql error\n\n```Operation name: InstanceWarningQuery\n\nMessage: (sqlite3.OperationalError) disk I/O error\n[SQL: SELECT daemon_heartbeats.body \nFROM daemon_heartbeats]\n(Background on this error at: <https://sqlalche.me/e/14/e3q8>)\n\nPath: [\"instance\",\"daemonHealth\",\"allDaemonStatuses\"]\n\nLocations: [{\"line\":30,\"column\":3}]```", "dagster_empl_last_target": "ok. this was me running out of disk space..."}, {"question": "What is wrong with dagit failing to start the grpc server? It worked previously. So far, I cannot pinpoint the reason.\nI am using dagster 1.1.10 on Windows\n\n```\\dagster\\_core\\workspace\\context.py:584: UserWarning: Error loading repository location data_mundus:Exception: gRPC server exited with return code 0 while starting up with the command: \"\\envs\\data_mundus\\python.exe -m dagster api grpc --lazy-load-user-code --port 59824 --heartbeat --heartbeat-timeout 45 --fixed-server-id a0ee3aa0-d6c0-4242-8b96-3b9b635ebe79 --log-level INFO --inject-env-vars-from-instance --instance-ref {\"__class__\": \"InstanceRef\", \"compute_logs_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"LocalComputeLogManager\", \"config_yaml\": \"base_dir: C:\\\\dagster_home\\\\storage\\n\", \"module_name\": \"dagster.core.storage.local_compute_log_manager\"}, \"custom_instance_class_data\": null, \"event_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"SqliteEventLogStorage\", \"config_yaml\": \"base_dir: C:\\\\dagster_home\\\\history\\\\runs\\\\\\n\", \"module_name\": \"dagster.core.storage.event_log\"}, \"local_artifact_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"LocalArtifactStorage\", \"config_yaml\": \"base_dir: C:\\\\dagster_home\\n\", \"module_name\": \"dagster.core.storage.root\"}, \"run_coordinator_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DefaultRunCoordinator\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster.core.run_coordinator\"}, \"run_launcher_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DefaultRunLauncher\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster\"}, \"run_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"SqliteRunStorage\", \"config_yaml\": \"base_dir: C:\\\\dagster_home\\\\history\\\\\\n\", \"module_name\": \"dagster.core.storage.runs\"}, \"schedule_storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"SqliteScheduleStorage\", \"config_yaml\": \"base_dir: C:\\\\dagster_home\\\\schedules\\n\", \"module_name\": \"dagster.core.storage.schedules\"}, \"scheduler_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DagsterDaemonScheduler\", \"config_yaml\": \"{}\\n\", \"module_name\": \"dagster.core.scheduler\"}, \"secrets_loader_data\": null, \"settings\": {\"telemetry\": {\"enabled\": false}}, \"storage_data\": {\"__class__\": \"ConfigurableClassData\", \"class_name\": \"DagsterSqliteStorage\", \"config_yaml\": \"base_dir: C:\\\\dagster_home\\n\", \"module_name\": \"dagster.core.storage.sqlite_storage\"}} --location-name data_mundus -m data_mundus -d C:\\Users\\heilerg\\development\\projects\\datascience\\data-mundus\"\n\nStack Trace:\n  File \"\\dagster\\_core\\host_representation\\grpc_server_registry.py\", line 219, in _get_grpc_endpoint\n    server_process = GrpcServerProcess(\n  File \"\\dagster\\_grpc\\server.py\", line 1183, in __init__\n    self.server_process, self.port = _open_server_process_on_dynamic_port(\n  File \"\\dagster\\_grpc\\server.py\", line 1119, in _open_server_process_on_dynamic_port\n    server_process = open_server_process(\n  File \"\\dagster\\_grpc\\server.py\", line 1091, in open_server_process\n    wait_for_grpc_server(server_process, client, subprocess_args, timeout=startup_timeout)\n  File \"\\dagster\\_grpc\\server.py\", line 1025, in wait_for_grpc_server\n    raise Exception(\n\n  warnings.warn(\n\\dagster\\_core\\execution\\compute_logs.py:48: UserWarning: WARNING: Compute log capture is disabled for the current environment. Set the environment variable `PYTHONLEGACYWINDOWSSTDIO` to enable.```", "replies": ["On a linux host the rpc is starting fine", "In the recent 1.1.10 release there were some grpc changes - could these be related? I am using Python 3.10", "Hey Georg - if you run the command it tried to run yourself (without the long instance-ref arg), does the server stay running or does it exit?", "It must be this - 1.1.9 is still starting just fine", "1.1.9 is working fine also on windows", "Let me know if you get a chance to run that command, that might help explain why it\u2019s exiting", "Oh yes, if you\u2019re on python 3.10 - try installing grpcio 1.47.2 if you\u2019re using a later version?", "you mean &gt;= ? I thought you are enforcing this already. Let me try", "No ==", "Or &lt;=", "We relaxed a grpcio pin that probably needs to be unrelaxed ", "Understood. let me try -- it will take a while as conda is so slow", "with the pin it is now working again also on windows"], "is_dagster_empl": [false, false, true, false, false, true, true, false, true, true, true, false, false], "gpt4_replies_target": "The issue with dagit failing to start the grpc server might be due to the recent changes in grpc in the 1.1.10 release of dagster and the use of Python 3.10. Installing grpcio 1.47.2 or relaxing a grpcio pin might resolve the issue.", "dagster_empl_first_target": "Hey Georg - if you run the command it tried to run yourself (without the long instance-ref arg), does the server stay running or does it exit?", "dagster_empl_last_target": "We relaxed a grpcio pin that probably needs to be unrelaxed "}, {"question": "Hello, Sorry for the beginner question but I couldn't find this in the docs. I have an op that returns two outputs. However the log shows that the second output just overwrites the first output on disk, so in this example both \"model\" and \"metrics\" point to to the same \"metrics\" file on disk as \"model\" is overwritten. Any ideas?\n```@op(out={\"model\": Out(), \"metrics\": Out()})\ndef train(dm):\n\n    res = sc.run_training(dm)\n    model = res.get_model(\"logreg\")\n    metrics = res.get_model_metrics(\"logreg\")\n\n    return (\n        Output(model, output_name=\"model\"),\n        Output(str(metrics), output_name=\"metrics\"),\n    )```", "replies": ["hmm have you tried yielding the outputs individually? I'm not sure if that's still required in some situations, but maybe try something like\n```@op(out={\"model\": Out(), \"metrics\": Out()})\ndef train(dm):\n\n    res = sc.run_training(dm)\n    model = res.get_model(\"logreg\")\n    metrics = res.get_model_metrics(\"logreg\")\n\n    yield Output(model, output_name=\"model\"),\n    yield Output(str(metrics), output_name=\"metrics\")```", "Thanks for the reply. I've tried a few iterations, including yield instead of return, and returning as a tuple or not. In the log it shows both items are returned, but the path is identical for each", "```Handled output \"metrics\" using IO manager \"io_manager\" path /workspaces/dagster-test/storage/train_results_brw\nHandled output \"model\" using IO manager \"io_manager\" path /workspaces/dagster-test/storage/train_results_brw```\nI wonder if this is the issue, that both outputs are saved to the same path?", "Hmm, I would expect this not to write to the same file.  Are you using the default IO manager? Where is the string \"results_brw\" coming from?", "<@U011CET83FG> Thanks for the reply!  I forgot I still had this up. The issue was resolved by fixing the code in a AssetsDeginition.from_op call"], "is_dagster_empl": [false, false, false, true, false], "gpt4_replies_target": "The issue of both outputs being saved to the same path was resolved by fixing the code in a AssetsDefinition.from_op call.", "dagster_empl_first_target": "Hmm, I would expect this not to write to the same file.  Are you using the default IO manager? Where is the string \"results_brw\" coming from?", "dagster_empl_last_target": "Hmm, I would expect this not to write to the same file.  Are you using the default IO manager? Where is the string \"results_brw\" coming from?"}, {"question": "Hi Everyone. I only recently started using Dagster so I have a somewhat ideological question. Is DataFrame a central part of a Dagster architecture? Is Dagster still a good fit for my use cases in a case if I plan to reduce dependency on pandas as much as possible?", "replies": ["Hi Nikita, Dagster itself has no concept of \u201cDataFrame\u201d so no, it\u2019s not a part of the Dagster architecture.\n\nHowever, Dagster is intended to be agnostic with respect to your data structures. Certainly we expect many of ours users to be processing tabular data, including in data frames. So we provide several integrations and UI niceties specifically geared towards pandas and tabular data, but it\u2019s also not difficult to roll your own solution. I suggest having a look at this guide on using our integration <https://docs.dagster.io/integrations/pandera|dagster-pandera>", "Thanks a lot, I'll be looking into it"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Dagster does not have a concept of \"DataFrame\" as part of its architecture and is agnostic to your data structures, so it can still be a good fit even if you plan to reduce dependency on pandas.", "dagster_empl_first_target": "Hi Nikita, Dagster itself has no concept of \u201cDataFrame\u201d so no, it\u2019s not a part of the Dagster architecture.\n\nHowever, Dagster is intended to be agnostic with respect to your data structures. Certainly we expect many of ours users to be processing tabular data, including in data frames. So we provide several integrations and UI niceties specifically geared towards pandas and tabular data, but it\u2019s also not difficult to roll your own solution. I suggest having a look at this guide on using our integration <https://docs.dagster.io/integrations/pandera|dagster-pandera>", "dagster_empl_last_target": "Hi Nikita, Dagster itself has no concept of \u201cDataFrame\u201d so no, it\u2019s not a part of the Dagster architecture.\n\nHowever, Dagster is intended to be agnostic with respect to your data structures. Certainly we expect many of ours users to be processing tabular data, including in data frames. So we provide several integrations and UI niceties specifically geared towards pandas and tabular data, but it\u2019s also not difficult to roll your own solution. I suggest having a look at this guide on using our integration <https://docs.dagster.io/integrations/pandera|dagster-pandera>"}, {"question": "Is there a way to map a partition key to metadata or something I can attach to an `InputContext` and `OutputContext` ?\n\nI'm adding an asset to a partitioned job with a `StaticPartitionDefinition`.\n\nThe asset keys are in the form \"_&lt;human readable text&gt;:&lt;id&gt;_\".\n\nThe asset I am currently working on is downstream of DBT assets and for those I used the `partition_key_to_vars_fn` to parse out the &lt;_id&gt;_ part out of the key.\nI feel like putting the logic to get the &lt;_id&gt;_ out of the `context.partition_key` into the input handling method of an IO manager is a bad pattern because I want to use the same IO manager in non-partitioned jobs and jobs with different partitions.\n\nI tried making a new `StaticPartitionsDefinition` for this asset specifically which was just the _&lt;id&gt;_ values and then adding a `StaticPartitionMapping` but that results in the following error:\n```dagster._core.errors.DagsterInvalidDefinitionError: Multiple partitioned assets exist in assets job '&lt;job name&gt;'. Selected assets must have the same partitions definitions, but the selected assets have different partitions definitions:```\nThe rest of the error shows the partition keys associated with the upstream assets and the parsed out versions associated with the asset I am working on.", "replies": ["The reason I don't have the keys in just _&lt;id&gt;_ form in the first place is for easy adhoc execution of partitioned jobs / back-fills in dagit", "Hey Martin,\n\n&gt; Is there a way to map a partition key to metadata or something I can attach to an InputContext and OutputContext ?\nWhere were you thinking of doing this? It is possible you could do this in the op by returning an `Output` with associated metadata (which you could derive from the partition key), but I\u2019d recommend against it. The IO manager is the correct place to convert your partition key into a storage address (IIUC in your case extracting the ID).\n\n&gt; I feel like putting the logic to get the &lt;id&gt; out of the context.partition_key into the input handling method of an IO manager is a bad pattern because I want to use the same IO manager in non-partitioned jobs and jobs with different partitions.\nIt\u2019s ok for your IO manager to employ different logic for partitioned and unpartitioned inputs and outputs. This is actually what our builtin IO managers do: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/upath_io_manager.py#L17-L277>\n\nFor the case where you want to use the same IO manager to handle partitions slightly differently, I\u2019d recommend either:\n\n\u2022 Adding a config parameter to the IO manager that determines how the partition key is processed (i.e. is the ID split off)\n\u2022 Using a hook helper function to perform the conversion and subclassing the IO manager", "Thanks <@U01GTMVMGQH> - I think I could tie this to the jobs right? So if I was able to check from within the handling functions \"which job am I running for\" I could then parse the partition key based on the job.\n\nI assume that is available on the context somewhere?", "Yes, `job_name` is a property on both `InputContext` and `OutputContext`", "Would you need a MultiPartitionConfig? Since you have 2 partition keys: &lt;HumanReadableText&gt; and &lt;id&gt;; I ask becuawse I\u2019m looking to do something similar with &lt;client id&gt; and &lt;date&gt; as the partition keys", "Since IIUC &lt;HumanReadableText&gt; isn\u2019t really a separate organizational axis (but a description of what &lt;id&gt; refers to I don\u2019t think a `MultiPartitionConfig` makes sense here.", "That's my thinking too <@U01GTMVMGQH> - they match 1:1"], "is_dagster_empl": [false, true, false, true, false, true, false], "gpt4_replies_target": "Yes, you can map a partition key to metadata or something you can attach to an `InputContext` and `OutputContext`. You can do this in the op by returning an `Output` with associated metadata. However, it's recommended to do this in the IO manager, where you can convert your partition key into a storage address. It's okay for your IO manager to employ different logic for partitioned and unpartitioned inputs and outputs. You can add a config parameter to the IO manager that determines how the partition key is processed, or use a hook helper function to perform the conversion and subclassing the IO manager.", "dagster_empl_first_target": "Hey Martin,\n\n&gt; Is there a way to map a partition key to metadata or something I can attach to an InputContext and OutputContext ?\nWhere were you thinking of doing this? It is possible you could do this in the op by returning an `Output` with associated metadata (which you could derive from the partition key), but I\u2019d recommend against it. The IO manager is the correct place to convert your partition key into a storage address (IIUC in your case extracting the ID).\n\n&gt; I feel like putting the logic to get the &lt;id&gt; out of the context.partition_key into the input handling method of an IO manager is a bad pattern because I want to use the same IO manager in non-partitioned jobs and jobs with different partitions.\nIt\u2019s ok for your IO manager to employ different logic for partitioned and unpartitioned inputs and outputs. This is actually what our builtin IO managers do: <https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/upath_io_manager.py#L17-L277>\n\nFor the case where you want to use the same IO manager to handle partitions slightly differently, I\u2019d recommend either:\n\n\u2022 Adding a config parameter to the IO manager that determines how the partition key is processed (i.e. is the ID split off)\n\u2022 Using a hook helper function to perform the conversion and subclassing the IO manager", "dagster_empl_last_target": "Since IIUC &lt;HumanReadableText&gt; isn\u2019t really a separate organizational axis (but a description of what &lt;id&gt; refers to I don\u2019t think a `MultiPartitionConfig` makes sense here."}, {"question": ":wave: I\u2019ve a problem setting a hook to an asset job, here is the minimal code with the issue:\n\n```from dagster import asset, failure_hook, Definitions, define_asset_job\n\n\n@asset\ndef my_asset():\n    return 1\n\n\n@failure_hook(required_resource_keys={'my_resource'})\ndef my_hook(context):\n    print(\"hello hook\")\n\n\ndefinitions = Definitions(\n    assets=[my_asset],\n    jobs=[define_asset_job(\"my_job\", selection='my_asset', hooks={my_hook})],\n    resources={\n        'my_resource': None\n    }\n)```\nWhen I run Dagit then I get:\n\n```dagster._core.errors.DagsterInvalidDefinitionError: resource with key 'my_resource' required by hook 'my_hook' attached to job 'my_job' was not provided. Please provide a &lt;class 'dagster._core.definitions.resource_definition.ResourceDefinition'&gt; to key 'my_resource', or change the required key to one of the following keys which points to an &lt;class 'dagster._core.definitions.resource_definition.ResourceDefinition'&gt;: ['io_manager']```\nThis error does not happen when instead of creating the job from an asset, I create it from a graph via the `<http://graph.to|graph.to>_job` method.\n\nI\u2019m running on dagster 1.3.11", "replies": ["should I open a new issue on Github?", "Hi Riccardo, thanks for the report-- this looks like a genuine bug in resource binding, yes pls open an issue on GH and we\u2019ll get it addressed.", "here is the issue I\u2019ve opened <https://github.com/dagster-io/dagster/issues/14970>", "There's an already existing issue <https://github.com/dagster-io/dagster/issues/8577|here>", "Ah, thank you Brian, apologies Riccardo I didn\u2019t realize that hooks with SDAs weren\u2019t currently supported!", "according to your docs they should be supported <https://docs.dagster.io/changelog#138-core--0198-libraries>\n\nindeed from that version `define_asset_job` accepts hooks now, before it didn\u2019t", "Sorry, linked to the wrong issue.\n\nIf you check this thread <https://dagster.slack.com/archives/C01U954MEER/p1687557075959809?thread_ts=1686511363.120109&amp;cid=C01U954MEER|here>. Owen mentions a fix is coming in this week's release"], "is_dagster_empl": [false, true, false, false, true, false, false], "gpt4_replies_target": "The issue you're experiencing with setting a hook to an asset job in Dagster seems to be a known bug in resource binding. It's recommended to open an issue on GitHub for this problem, and a fix is expected to be released soon.", "dagster_empl_first_target": "Hi Riccardo, thanks for the report-- this looks like a genuine bug in resource binding, yes pls open an issue on GH and we\u2019ll get it addressed.", "dagster_empl_last_target": "Ah, thank you Brian, apologies Riccardo I didn\u2019t realize that hooks with SDAs weren\u2019t currently supported!"}, {"question": "Hi everyone. I'm encountering an error when connecting Dagster to Azure PostgreSQL database through the pgbouncer on port 6432. Does anyone know how to fix it?\nIn fact, there don't seem to be any issues with the network or port reachability. Pgbouncer on pgsqlserver is also enabled\n\n```psycopg2.OperationalError: connection to server at \"<http://env-client-index-postgresql-server.postgres.database.azure.com|env-client-index-postgresql-server.postgres.database.azure.com>\" (10.10.253.133), port 6432 failed: FATAL:  unsupported startup parameter: options\nconnection to server at \"<http://env-client-index-postgresql-server.postgres.database.azure.com|env-client-index-postgresql-server.postgres.database.azure.com>\" (10.10.253.133), port 6432 failed: FATAL:  unsupported startup parameter: options```", "replies": ["The error message you\u2019re seeing suggests that the PostgreSQL server hosted on Azure is rejecting the connection due to an unsupported startup parameter called \u201coptions\u201d.\nTo troubleshoot and resolve this issue, you can try the following steps:\n1. Verify the connection parameters: Double-check the parameters you\u2019re using for connecting to the PostgreSQL server. Ensure that the hostname, port, username, and password are correct.\n2. Check the PostgreSQL server version: Make sure the PostgreSQL server version on Azure is compatible with the version of psycopg2 you are using. It\u2019s possible that an older server version does not support certain startup parameters.\n3. Update psycopg2 library: Ensure that you have the latest version of psycopg2 library installed. You can update it using `pip`:\n4. pip install --upgrade psycopg2\n5. This will fetch the latest version of psycopg2 available in the Python Package Index.\n6. Verify connection settings: If the previous steps don\u2019t resolve the issue, review the connection settings and ensure that you haven\u2019t set any unsupported or incorrect options during the connection initialization.\nIf the issue persists, you may need to seek assistance from Azure support or consult the documentation specific to the PostgreSQL server you are trying to connect to.", "Thanks Harry Park for the response. <@U05EECQAFQT> I don\u2019t _think_ this is a dagster issue, but pls confirm that if you can identify it is an azure/postgres issue and you can fix your problem."], "is_dagster_empl": [false, true], "gpt4_replies_target": "The error suggests that the PostgreSQL server on Azure is rejecting the connection due to an unsupported startup parameter \"options\". You can troubleshoot this by verifying your connection parameters, checking the PostgreSQL server version compatibility with psycopg2, updating psycopg2 library, and reviewing your connection settings. If the issue persists, consult Azure support or the PostgreSQL server documentation.", "dagster_empl_first_target": "Thanks Harry Park for the response. <@U05EECQAFQT> I don\u2019t _think_ this is a dagster issue, but pls confirm that if you can identify it is an azure/postgres issue and you can fix your problem.", "dagster_empl_last_target": "Thanks Harry Park for the response. <@U05EECQAFQT> I don\u2019t _think_ this is a dagster issue, but pls confirm that if you can identify it is an azure/postgres issue and you can fix your problem."}, {"question": "Hi team, I am expecting my assets to be materialized using this specified Job as shown below.\nHowever, when I manually materialize my asset from dagit UI, it somehow shows job name as \"__ASSET_JOB\".\n\n`my_job = define_asset_job(`\n    `name=\"my_job\",`\n    `selection=AssetSelection.groups(\"my_group\")`\n`)`\n`defs = Definitions(`\n    `assets=all_assets,`\n    `jobs= [my_job],`\n`)`\n\nHowever, when I create a schedule for asset materialization\nusing the same job and the asset is materialized by a schedule it shows the correct Job name.\n\nIs this an expected behavior of Dagster?", "replies": ["We also encountered this. I\u2019m curious to know the answer", "Hi Salman, `__ASSET_JOB` is an implementation detail. You are not required to specify any jobs in your definitions-- you can provide just a list of assets, which will be visible in the global asset graph.\n\nHowever, internally Dagster requires all runs to use a job. `__ASSET_JOB` is a \u201chidden\u201d job created by Dagster internals that includes all the assets passed to your definitions object. This is the job that will be used when materializing from the global asset graph. The reason we don\u2019t use a job that you have included the asset in is that an asset can be included in multiple jobs, so it wouldn\u2019t be clear which one to use from the global asset graph.\n\nIf you want to materialize in dagit using `my_job` instead of `__ASSET_JOB` you should select the assets in the asset graph that is show when viewing `my_job` instead of the global asset graph.", "Thank you! That makes a lot of sense.", "<@U04671W8R50> ^^", "&gt; However, internally Dagster requires all runs to use a job. `__ASSET_JOB` is a \u201chidden\u201d job created by Dagster internals that includes all the assets passed to your definitions object. This is the job that will be used when materializing from the global asset graph. The reason we don\u2019t use a job that you have included the asset in is that an asset can be included in multiple jobs, so it wouldn\u2019t be clear which one to use from the global asset graph.\nHi <@U01GTMVMGQH>, I have another question regarding this. Currently we have some tags in our defined jobs, and these tags can be passed to the runs if we materialise a partition from job page. But if I materialise a partition from the asset graph,`__ASSET_JOB` doesn\u2019t seem to have these tags and won\u2019t pass tags to the run ofc.\n\nAll of our assets only have a single job attached. How can we solve this? Can we pass tags to assets?", "Hey Brian, You can set the `op_tags` param on `@asset`. I\u2019m not sure if that will support all the tags you need though (as some tags are scoped to runs and need to be defined at the job level).", "<@U01GTMVMGQH>, is there a way to replace the `__ASSET_JOB` with our defined job?", "In the left sidebar of the UI, you should see any asset jobs you\u2019ve defined. If you click one, it will show an asset graph view including only the assets defined in the job. If you launch from this view it will use the specified job instead of `__ASSET_JOB`.", "<@U01GTMVMGQH>, we have another use case where our downstream assets are materialized with the auto materialize policy based on upstream assets. Basically, we have scheduled materialization of upstream assets on a specific job. We need a custom job name for downstream assets as well. Is there a way to achieve this with this design?", "<@U01GTMVMGQH> Oh yeah, I\u2019m aware of that. But then there\u2019s still the chance that people will go to the asset view and materialize/backfill partitions from there.", "<@U05BUQYD752>\n\nIIUC you are asking if it is possible to set the job used by the auto-materialize policy?\n\n<@U04UTJ28XC4>\n\nWe don\u2019t currently have a way to set a \u201cpreferred job\u201d to be used when materializing an asset from the global graph-- if you think this would be useful, can you please open a GH issue: <https://github.com/dagster-io/dagster/issues>", "<@U01GTMVMGQH> Yes. Even if we are able to set some tags to these auto-materialized assets we are good if not job name. As per Dagster documentation, these auto-materialized assets will be assigned a tag by default as {\"auto-materialize\":true}. If we need a custom tag we need to add in the global.yaml file. But the problem here as well is that this yaml file will be associated with a Dagster instance. We have only one Dagster instance but multiple code location. The idea here is to keep a unique tag/job name to downstream assets created from each code locations.", "And I believe now that if assets are directly materialized from the graph there is no way a custom job will be ran. Its quite intuitive for users to materialize assets from graph directly as opposed  to first selecting a job and then asset materialization.", "<@U01J51Y6B9D> is there any way to control the job used to materialize an asset by the auto-materialize daemon?\n\n<@U05BUQYD752> looks like you have the same concern as Brian regarding use of a \u201cpreferred\u201d job for materialization from the global asset graph. If either of you can open an issue this will help us track this potential feature.", "Hi <@U05BUQYD752>! Unfortunately, there's not currently a way to customize the job that's executed when an asset is auto-materialized (I believe that sort of functionality would likely fall under a hypothetical \"preferred job\" concept, which I agree would be useful in such situations)\n\nBut to get back to the point of\n\n&gt; The idea here is to keep a unique tag/job name to downstream assets created from each code locations.\nIs this just an organizational tool? If so, dagster automatically sets a `.dagster/repository` tag on runs (which would be the name of the code location that the job lives in). This would allow you to query for runs that were launched from a given code location. Would this be sufficient for your purposes?", "Hi <@U01J51Y6B9D> There are 3 unique scenarios now:\n1. Upstream assets getting materialized on schedule \u2013 There is no problem here as its leveraging the custom job which we have created for the schedule.\n2. Downstream assets getting auto materialized \u2013 Here as well Dagster adds a tag {\u201cauto-materialized\u201d:true}\n3. Ad-Hoc materialization of assets from the asset graph\nIn scenario number 3, only option to use a particular job is to first select the job from Dagit and then materialize it.\nBut this is not intuitive.\nIs there a way at least where we can differentiate these assets which got materialized from the asset graph (without first selecting the job) from those which were either auto materialized or materialized on a schedule.\n\nThis will solve our use case.", "by \"differentiate these assets\", are you talking about differentiating the materializations? and at what layer?\n\nfor a given materialization event, the DagsterEvent will have a `job_name` property which will tell you the name of the job the asset was materialized in (which will start with `__ASSET_JOB` if this was an ad-hoc / auto materialize run)"], "is_dagster_empl": [false, true, false, false, false, true, false, true, false, false, true, false, false, true, true, false, true], "gpt4_replies_target": "The \"__ASSET_JOB\" is a hidden job created by Dagster that includes all the assets passed to your definitions object. This job is used when materializing from the global asset graph. If you want to materialize using a specific job, you should select the assets in the asset graph that is shown when viewing that specific job. Currently, there's no way to customize the job that's executed when an asset is auto-materialized or to set a \"preferred job\" for materialization from the global asset graph.", "dagster_empl_first_target": "Hi Salman, `__ASSET_JOB` is an implementation detail. You are not required to specify any jobs in your definitions-- you can provide just a list of assets, which will be visible in the global asset graph.\n\nHowever, internally Dagster requires all runs to use a job. `__ASSET_JOB` is a \u201chidden\u201d job created by Dagster internals that includes all the assets passed to your definitions object. This is the job that will be used when materializing from the global asset graph. The reason we don\u2019t use a job that you have included the asset in is that an asset can be included in multiple jobs, so it wouldn\u2019t be clear which one to use from the global asset graph.\n\nIf you want to materialize in dagit using `my_job` instead of `__ASSET_JOB` you should select the assets in the asset graph that is show when viewing `my_job` instead of the global asset graph.", "dagster_empl_last_target": "by \"differentiate these assets\", are you talking about differentiating the materializations? and at what layer?\n\nfor a given materialization event, the DagsterEvent will have a `job_name` property which will tell you the name of the job the asset was materialized in (which will start with `__ASSET_JOB` if this was an ad-hoc / auto materialize run)"}, {"question": "when upgrading to a new Dagster version, is the recommended order of operations to update the version on the Daemon and Dagit containers, then update user code locations?", "replies": ["Hi Andy, we maintain backcompatibility across versions both ways so it shouldn\u2019t matter which order you perform the update in."], "is_dagster_empl": [true], "gpt4_replies_target": "The order of updating doesn't matter when upgrading to a new Dagster version as backcompatibility is maintained across versions.", "dagster_empl_first_target": "Hi Andy, we maintain backcompatibility across versions both ways so it shouldn\u2019t matter which order you perform the update in.", "dagster_empl_last_target": "Hi Andy, we maintain backcompatibility across versions both ways so it shouldn\u2019t matter which order you perform the update in."}, {"question": "Hi everyone. I\u00b4m facing some problems with a sensor running in a code server deployment on k8s, with every tick of the sensor failing by timeout, with the following root exception:\n```grpc._channel._MultiThreadedRendezvous: &lt;_MultiThreadedRendezvous of RPC that terminated with:\n\tstatus = StatusCode.DEADLINE_EXCEEDED\n\tdetails = \"Deadline Exceeded\"\n\tdebug_error_string = \"{\"created\":\"@1687870896.003832471\",\"description\":\"Deadline Exceeded\",\"file\":\"src/core/ext/filters/deadline/deadline_filter.cc\",\"file_line\":81,\"grpc_status\":4}\"```\nChecking on the code server deployment, we can see that it consumes a big amount of CPU, in contrast to other code servers under the same dagster instance. However, I cannot reproduce this problem locally, as the sensor will run with no problems, and neither can I see any logs from the sensor using the \"experimental schedule/sensor logging view\" feature.\nI\u00b4m running on dagster `1.3.3`", "replies": ["Hey Juan, What is the sensor doing? Are you trying to access some web API that is maybe timing out due to network issues or bad credentials or something?", "Hi, yes, I have to access an FTP server and it hangs at some point. However the debugging for this is quite poor, since I can\u00b4t view the logs, from `context.log` in the sensor UI.\n\nAlso, I\u00b4ve tested running a script similar to the sensor evaluation directly on the code server's pod, which I belive (please correct me if I am wrong) is where the sensor evaluation runs. The script runs normally with the same access credentials.", "In the end, the problem with the sensor was a check if there was already a materialized dynamic partition created for a specific file, on the sensor job. This check was being performed once for each file, up to 1000 detected files.", "I could see the logs directly from the pod where the code server was deployed.", "Ah, glad to hear you\u2019ve solved the problem!", "Thank you!"], "is_dagster_empl": [true, false, false, false, true, false], "gpt4_replies_target": "The issue with the sensor was due to a check being performed once for each file, up to 1000 detected files, to see if there was already a materialized dynamic partition created for a specific file on the sensor job. This was resolved by viewing the logs directly from the pod where the code server was deployed.", "dagster_empl_first_target": "Hey Juan, What is the sensor doing? Are you trying to access some web API that is maybe timing out due to network issues or bad credentials or something?", "dagster_empl_last_target": "Ah, glad to hear you\u2019ve solved the problem!"}, {"question": "is it possible to have a weekly asset have its partitions named as '20230618_20230624', i.e. including the date range?", "replies": ["Hi Giovanni, I think you can achieve this by passing the `fmt` parameter of `WeeklyPartitionsDefinition`. This parameter is passed to `datetime.strptime`.", "but how do I differentiate partition start and end dates in the formatting string?", "my understanding is that `fmt` is for parsing the `start_date` in the definition", "not the final asset key (.e.g in disk my_partitioned_asset/2023-06-01.pkl)", "I want 2023-06-01_2023-06-07.pkl, for example", "Ah you know what I\u2019m looking at the code now and I can see you\u2019re right-- we\u2019re using the same `fmt` parameter for `strptime` and `strftime`. So I don\u2019t think this is possible at present, but I think it is functionality that could be added (an optional partition key formatting parameter), though unlikely to be prioritized.\n\nIf you\u2019re interested, would you mind opening a GH issue for it?\n\n<https://github.com/dagster-io/dagster/issues>", "sure! i'll open an issue\nthank you", "just so that I get the name right and make it clear in the feature request, what's the proper name of that?", "asset output key?", "\u201cpartition key\u201d is the right name-- \u201casset key\u201d is a separate concept.\n\nThough rereading your messages, if you only care about the name under which it is saved on disk, you can do this with a custom IO manager.", "got it! thank you", "<https://github.com/dagster-io/dagster/issues/14971>\n\nhere it is!"], "is_dagster_empl": [true, false, false, false, false, true, false, false, false, true, false, false], "gpt4_replies_target": "Currently, it's not possible to have a weekly asset with partitions named as '20230618_20230624' in Dagster, but this functionality could potentially be added in the future. You can open a GitHub issue for this feature request.", "dagster_empl_first_target": "Hi Giovanni, I think you can achieve this by passing the `fmt` parameter of `WeeklyPartitionsDefinition`. This parameter is passed to `datetime.strptime`.", "dagster_empl_last_target": "\u201cpartition key\u201d is the right name-- \u201casset key\u201d is a separate concept.\n\nThough rereading your messages, if you only care about the name under which it is saved on disk, you can do this with a custom IO manager."}, {"question": "Hey Team, I was reading through the dagster-snowflake documentations and saw that for the snowflake io manager we don't need to specify the input manager key when we are trying to read from snowflake nor do we need to do it in the resource_defs when we are using it to write into snowflake. I was wondering why this was the case when compared to standard io manager documentation and what is the current \"standard\" way to write into snowflake from dagster.", "replies": ["Ahh I think i found the answer to my own question: \"The I/O manager that's used for handling a particular op output is automatically used for loading it in downstream ops.\"", "Correct Son, let us know if you have any other questions!"], "is_dagster_empl": [false, true], "gpt4_replies_target": "The I/O manager used for handling a particular op output in dagster-snowflake is automatically used for loading it in downstream ops, hence there's no need to specify the input manager key when reading from or writing into Snowflake.", "dagster_empl_first_target": "Correct Son, let us know if you have any other questions!", "dagster_empl_last_target": "Correct Son, let us know if you have any other questions!"}, {"question": "I am trying to get the input resource key for an asset using the graphql interface.  I am not sure which 'required resources' would provide this information since I am not seeing what I would expect.  On every asset, I have tried getting the 'required resources' for the dependencies, the same for the op and the op's input and output definitions.  None of these have the resource I would expect.  Am I looking in the wrong place?  I am seeing the following error in the graphql output:\n\n{    \"message\": \"'GrapheneInputDefinition' object has no attribute '_solid_def_snap'\",\n    \"locations\": [\n     {\n      \"line\": 19,\n      \"column\": 15\n     }\n    ],\n    \"path\": [\n     \"assetsOrError\",\n     \"nodes\",\n     223,\n     \"definition\",\n     \"op\",\n     \"inputDefinitions\",\n     0,\n     \"solidDefinition\"\n    ],\n    \"extensions\": {\n     \"errorInfo\": {\n      \"__class__\": \"SerializableErrorInfo\",\n      \"message\": \"AttributeError: 'GrapheneInputDefinition' object has no attribute '_solid_def_snap'\\n\",\n      \"stack\": [\n       \" File  \\\"/usr/local/lib/python3.7/site-packages/graphql/execution/execute.py\\\", line 521, in execute_field\\n  result = resolve_fn(source, info, **args)\\n\",\n       \" File  \\\"/usr/local/lib/python3.7/site-packages/dagster_graphql/schema/solids.py\\\", line 63, in resolve_solid_definition\\n  return build_solid_definition(self._represented_job, self._<http://solid_def_snap.name|solid_def_snap.name>)\\n\"\n      ],\n      \"cls_name\": \"AttributeError\",\n      \"cause\": null,\n      \"context\": null\n     }\n    }\n   },\n\nCould this error be the reason I am not seeing the resource keys itemized?", "replies": ["Thanks for this report Chris.\n\nThis looks like a bug-- this error should not be thrown.\n\nCan you post the GQL query you\u2019re running and if possible a minimal set of definitions?", "<@U01GTMVMGQH> Ok, I am trying to debug which resource is causing it but suspecting it is one I converted to the new pydantic style.\n\nAlso, I notice that I can get the list of resources using the allTopLevelResource query and it provides a lot of what I am looking for except the input resource relations.  However, it only works for repositories created with Definitions() and not those created with @repository. Is this expected?\n\nI also noticed that credentials are returned in this query, is this something that you will remove at some point because of security risk?  I don't mind it being there since I plan to use it and close the interface from outside.", "&gt; However, it only works for repositories created with Definitions() and not those created with @repository. Is this expected?\nI believe so, because `@repository` doesn\u2019t have a concept of \u201ctop-level\u201d resources-- resources have to be associated to ops/assets etc\n\n&gt; I also noticed that credentials are returned in this query\nCan you provide some more detail? What query, and what field?", "<@U01GTMVMGQH> the credentials are returned when querying for the configuredValues of the resource:\n\n.. on ResourceDetailsList {     results {\n       configuredValues {\n         key\n         type\n         value\n       }\n\nmaybe the issue is that I should be using an environment variable and am not doing that yet.", "yes, we need to be able to expose configured values so that\u2019s not going to change-- the solution is to use an env var"], "is_dagster_empl": [true, false, true, false, true], "gpt4_replies_target": "The error you're seeing could be a bug, and it's recommended to post the GQL query you're running and a minimal set of definitions for further assistance. For repositories, it's expected that the allTopLevelResource query only works for those created with Definitions() and not those created with @repository. As for credentials being returned in the query, it's suggested to use an environment variable for security purposes.", "dagster_empl_first_target": "Thanks for this report Chris.\n\nThis looks like a bug-- this error should not be thrown.\n\nCan you post the GQL query you\u2019re running and if possible a minimal set of definitions?", "dagster_empl_last_target": "yes, we need to be able to expose configured values so that\u2019s not going to change-- the solution is to use an env var"}, {"question": "Hi everyone,\nis it possible to list all previous asset keys inside the script and list the, in order to put them in an asset that should run last and place them inside the non_argument_deps? Essentially i want an asset to only run after all other asset in job/run finished.", "replies": ["Hi rami, your asset definitions can be constructed by arbitrary python code so I don\u2019t see why this isn\u2019t possible. The details depend on how you are building your assets and what you mean by \u201cprevious\u201d.", "Pulled it of with:\n```globals_dict = globals()\nall_assets = {name: value for name, value in globals_dict.items() if isinstance(value, AssetsDefinition)}\nasset_keys = list(all_assets.keys())```\nThanks."], "is_dagster_empl": [true, false], "gpt4_replies_target": "Yes, it is possible to list all previous asset keys inside the script. You can construct your asset definitions with arbitrary python code. You can use a code snippet like this to achieve it: \n```globals_dict = globals()\nall_assets = {name: value for name, value in globals_dict.items() if isinstance(value, AssetsDefinition)}\nasset_keys = list(all_assets.keys())```", "dagster_empl_first_target": "Hi rami, your asset definitions can be constructed by arbitrary python code so I don\u2019t see why this isn\u2019t possible. The details depend on how you are building your assets and what you mean by \u201cprevious\u201d.", "dagster_empl_last_target": "Hi rami, your asset definitions can be constructed by arbitrary python code so I don\u2019t see why this isn\u2019t possible. The details depend on how you are building your assets and what you mean by \u201cprevious\u201d."}, {"question": "Hello,\nIs there any documentation about the new `dagster code-server start` command to launch code server ?\nWhat are the benefits compared to the previous `dagster api grpc` command ?", "replies": ["Here\u2019s a discussion on it <https://github.com/dagster-io/dagster/discussions/14709>"], "is_dagster_empl": [true], "gpt4_replies_target": "You can find information about the `dagster code-server start` command in this discussion: <https://github.com/dagster-io/dagster/discussions/14709>.", "dagster_empl_first_target": "Here\u2019s a discussion on it <https://github.com/dagster-io/dagster/discussions/14709>", "dagster_empl_last_target": "Here\u2019s a discussion on it <https://github.com/dagster-io/dagster/discussions/14709>"}, {"question": "Hi! Similar in spirit to this open github issue (<https://github.com/dagster-io/dagster/issues/6834>), I was wondering if there's a way to have shared memory (ie., a cache; no I/O) across the different processes of a dagster instance? I suspect this may be a work-in-progress but wanted to ask. Thank you!", "replies": ["Hi Vasco, I don\u2019t believe we have any real progress on this-- feel free to upvote the issue you linked. If enough people express interest we will prioritize it.", "Hi Sean, thanks for the update-- just upvoted the issue."], "is_dagster_empl": [true, false], "gpt4_replies_target": "Currently, there is no progress on having shared memory across different processes of a dagster instance, but you can upvote the linked issue to express interest and potentially prioritize it.", "dagster_empl_first_target": "Hi Vasco, I don\u2019t believe we have any real progress on this-- feel free to upvote the issue you linked. If enough people express interest we will prioritize it.", "dagster_empl_last_target": "Hi Vasco, I don\u2019t believe we have any real progress on this-- feel free to upvote the issue you linked. If enough people express interest we will prioritize it."}, {"question": "Hi! Looking for some feedback on managing partitions in a custom IO Manager.\n\nI've subclassed from `ConfigurableIOManager` to access SQL Server. What i'm a little stuck on is the best way to approach partitions.\n\nLooking at the implementations in snowflake/duckdb there seems to be calls to `dagster._core.storage.db_io_manager` to help manage some of the details like timewindows. these look like protected (as much as you can) in python apis. For the dev side, are we expected to utilize that for partitions?\n\nRight now i settled on using the `partition_expr` format for identifying the partition columns, and using a dict for multipartitions\n\n```@asset(\n        partitions_def=MultiPartitionsDefinition(\n            {\n                'date': DailyPartitionsDefinition(start_date=datetime(2023,6,1)),\n                'tag': StaticPartitionsDefinition([\n                    \"value1\",\n                    \"value2\"\n                ])\n            }\n        ), \n        io_manager_key='mssql_io_manager', \n        key_prefix=['data'],\n        metadata={\n            'partition_expr': {\n                'date': 'data_datetime',\n                'tag': 'source'\n            }\n        }\n    )\n\n#--- non multi\n\n@asset(\n        partitions_def=DailyPartitionsDefinition(start_date=datetime(2023,6,1)), \n        io_manager_key='mssql_io_manager', \n        key_prefix=['data'],\n        metadata={\n            'partition_expr': 'data_datetime'\n        }\n    )\n\n#--static version\n@asset(\n        partitions_def=StaticPartitionsDefinition([\n                    \"value1\",\n                    \"value2\"\n                ]), \n        io_manager_key='mssql_io_manager', \n        key_prefix=['data'],\n        metadata={\n            'partition_expr': 'source'\n        }\n    )```\n\nThen in my IO manager i access it by checking for a multi, then trying to parse from there. Seems clunky though, any suggestions on a better structure? I've basically made it fixed to look for `date` if there is a date partition, so i'll need to always add that, which is ok. Open to ideas.\n\n```def _partition_where_clause(self, context: OutputContext) -&gt; str:\n        if not context.has_asset_partitions:\n            return\n        \n        partition_expr = context.metadata.get('partition_expr')\n        \n        x = []\n        if isinstance(context.partition_key, MultiPartitionKey):\n            for _ in partition_expr:\n                if _ == 'date':\n                    # date stuff\n                    ...\n                    tw = self._time_window_where_clause(\n                        context.asset_partitions_time_window,\n                        partition_expr[_]\n                    )\n                    x.append(tw)\n                else:\n                    partition_keys = context.asset_partition_key.keys_by_dimension\n                    x.append(self._static_where_clause(\n                        partition_keys[_],\n                        partition_expr[_]\n                    ))\n        \n        else:\n            try:\n                time_window = context.asset_partitions_time_window\n                x.append(self._time_window_where_clause(time_window, partition_expr))\n            except:\n                x.append(self._static_where_clause(context.partition_key, partition_expr))\n        \n        joined_data = \" AND \".join(x)\n        final_where = f'WHERE {joined_data}'\n        return final_where```\nFull code here: <https://gist.github.com/namur007/c01ba67835619bc1221bfbe15d20b6e0>\n\nHappy to share an implementation afterwards as well for others looking to access sql server/pandas", "replies": ["Hi Cody,\n\n&gt; Looking at the implementations in snowflake/duckdb there seems to be calls to dagster._core.storage.db_io_manager to help manage some of the details like timewindows. these look like protected (as much as you can) in python apis. For the dev side, are we expected to utilize that for partitions?\nSo, `DbIOManager` is internal (not part of the public API) and isn\u2019t finalized-- so I would  say it can serve as a good source of inspiration for your own design, and you _could_ try copying the pattern of snowflake/duckdb (inheriting from `ConfigurableIOManagerFactory` and generating instances of `DbIOManager`) but we can\u2019t guarantee it won\u2019t break.\n\n&gt; Seems clunky though, any suggestions on a better structure?\nUnfortunately I can\u2019t really offer a better suggestion at present-- when we make `DbIOManager` public then we can provide provide a streamlined API/guide/pattern, but for now what you\u2019re doing seems reasonable.", "I can only dream of moving off prem before you finalize so I can sunset this task\u2026 if not I look forward to the updates! Cheers!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The `DbIOManager` is currently not part of the public API and may not be finalized, so while you can use it as a reference for your own design, there's no guarantee it won't break. Your current approach seems reasonable given the circumstances.", "dagster_empl_first_target": "Hi Cody,\n\n&gt; Looking at the implementations in snowflake/duckdb there seems to be calls to dagster._core.storage.db_io_manager to help manage some of the details like timewindows. these look like protected (as much as you can) in python apis. For the dev side, are we expected to utilize that for partitions?\nSo, `DbIOManager` is internal (not part of the public API) and isn\u2019t finalized-- so I would  say it can serve as a good source of inspiration for your own design, and you _could_ try copying the pattern of snowflake/duckdb (inheriting from `ConfigurableIOManagerFactory` and generating instances of `DbIOManager`) but we can\u2019t guarantee it won\u2019t break.\n\n&gt; Seems clunky though, any suggestions on a better structure?\nUnfortunately I can\u2019t really offer a better suggestion at present-- when we make `DbIOManager` public then we can provide provide a streamlined API/guide/pattern, but for now what you\u2019re doing seems reasonable.", "dagster_empl_last_target": "Hi Cody,\n\n&gt; Looking at the implementations in snowflake/duckdb there seems to be calls to dagster._core.storage.db_io_manager to help manage some of the details like timewindows. these look like protected (as much as you can) in python apis. For the dev side, are we expected to utilize that for partitions?\nSo, `DbIOManager` is internal (not part of the public API) and isn\u2019t finalized-- so I would  say it can serve as a good source of inspiration for your own design, and you _could_ try copying the pattern of snowflake/duckdb (inheriting from `ConfigurableIOManagerFactory` and generating instances of `DbIOManager`) but we can\u2019t guarantee it won\u2019t break.\n\n&gt; Seems clunky though, any suggestions on a better structure?\nUnfortunately I can\u2019t really offer a better suggestion at present-- when we make `DbIOManager` public then we can provide provide a streamlined API/guide/pattern, but for now what you\u2019re doing seems reasonable."}, {"question": "Hey team, this might be a dumb question but how do we load multiple definitions together? I am trying to have the same IO manager for a certain set of assets without having to manually specify the io manager for each individual asset like the following example, but I don't know how to get this to work with an existing definition I have in my highest level `__init__.py` file.\n\n```defs = Definitions(\n    assets=[iris_dataset, iris_harvest_data, iris_cleaned],\n    resources={\n        \"io_manager\": SnowflakePandasIOManager(\n            account=\"abc1234.us-east-1\",\n            user=EnvVar(\"SNOWFLAKE_USER\"),\n            password=EnvVar(\"SNOWFLAKE_PASSWORD\"),\n            database=\"FLOWERS\",\n            role=\"writer\",\n            warehouse=\"PLANTS\",\n            schema=\"IRIS\",\n        )\n    },\n)```", "replies": ["I think a code location can only have one definition. It would be nice if one could apply an IO manager to a group of assets, but not sure how to do it.\n<https://docs.dagster.io/concepts/code-locations#definitions-versus-repositories>", "Ahhh i see thanks Justin!", "Hi Son, Justin is correct that there is one `Definitions` object per code location.\n\nIf you want to specify an IO manager for multiple assets, the only way to do that currently is to set it on each asset."], "is_dagster_empl": [false, false, true], "gpt4_replies_target": "In Dagster, there can only be one `Definitions` object per code location and to specify an IO manager for multiple assets, you need to set it on each asset individually.", "dagster_empl_first_target": "Hi Son, Justin is correct that there is one `Definitions` object per code location.\n\nIf you want to specify an IO manager for multiple assets, the only way to do that currently is to set it on each asset.", "dagster_empl_last_target": "Hi Son, Justin is correct that there is one `Definitions` object per code location.\n\nIf you want to specify an IO manager for multiple assets, the only way to do that currently is to set it on each asset."}, {"question": "Hello, I am having trouble using an ssh resource in the following sensor:\n```@sensor(job=j01_dml_ops, minimum_interval_seconds=60)\ndef new_file_sensor(context):\n    now = datetime.now()\n    for filename in context.resources.ssh.get_connection().listdir(\"./in\"):\n        if filename == \"Socket*\" + now.strftime(\"%Y-%m-%d\") + \".csv\":\n            context.resources.ssh.sftp_get(\"./in/Socket*\" + now.strftime(\"%Y-%m-%d\") + \".csv\", \"./socket_load_stage/socket-homes-passed.csv\")\n            yield RunRequest()```\nHere is where I have the ssh resource defined:\n```defs = Definitions(\n    jobs = [j01_dml_ops, j02_copy_csv, j03_load_new, j04_copy_new, j05_load_old, j06_load_updated_address, j07_copy_updated, j08_load_updated_other],\n    schedules = [full_job_schedule],\n    sensors = [full_job_sensor, new_file_sensor, slack_alert, email_on_run_failure],\n    resources = {\n        \"ssh\": ssh_resource.configured({\n            \"remote_host\": \"remote_host\",\n            \"username\": \"username\",\n            \"password\": \"password\",\n        })\n    }\n)```\nHowever, I am receiving the following error:\n```Sensor daemon caught an error for sensor new_file_sensor\nTraceback (most recent call last):\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_daemon/sensor.py\", line 520, in _process_tick_generator\n    yield from _evaluate_sensor(\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_daemon/sensor.py\", line 583, in _evaluate_sensor\n    sensor_runtime_data = code_location.get_external_sensor_execution_data(\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_core/host_representation/code_location.py\", line 845, in get_external_sensor_execution_data\n    return sync_get_external_sensor_execution_data_grpc(\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_api/snapshot_sensor.py\", line 78, in sync_get_external_sensor_execution_data_grpc\n    raise DagsterUserCodeProcessError.from_error_info(result.error)\ndagster._core.errors.DagsterUserCodeProcessError: dagster._core.errors.DagsterUnknownResourceError: Unknown resource `ssh`. Specify `ssh` as a required resource on the compute / config function that accessed it.\n\nStack Trace:\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_grpc/impl.py\", line 369, in get_external_sensor_execution\n    return sensor_def.evaluate_tick(sensor_context)\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_core/definitions/sensor_definition.py\", line 690, in evaluate_tick\n    result = list(self._evaluation_fn(context))\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_core/definitions/sensor_definition.py\", line 969, in _wrapped_fn\n    for item in result:\n  File \"/Users/noahnewton/dev/noah-dagster/socket-load-stage/socket_load_stage/__init__.py\", line 101, in new_file_sensor\n    for filename in context.resources.ssh.get_connection().listdir(\"./in\"):\n  File \"/Users/noahnewton/mambaforge/envs/dag3.10/lib/python3.10/site-packages/dagster/_core/definitions/scoped_resources_builder.py\", line 29, in __getattr__\n    raise DagsterUnknownResourceError(name)```\nI was trying to follow the documentation for Dagster's SSH/SFTP integration, but there isn't a lot of examples available for it and I can't figure out how to use the ssh resource within the sensor. What am I doing wrong?", "replies": ["Hi Noah, I think if you pass `required_resource_keys={\"ssh\"}` as an argument to `@sensor` it should work.", "Please lmk when you see this if that approach worked for you.", "Yes that worked, thanks!", "<@U059HLUM33Q> you\u2019ve put your username and password into a public channel."], "is_dagster_empl": [true, true, false, false], "gpt4_replies_target": "To resolve the issue, you should pass `required_resource_keys={\"ssh\"}` as an argument to `@sensor`.", "dagster_empl_first_target": "Hi Noah, I think if you pass `required_resource_keys={\"ssh\"}` as an argument to `@sensor` it should work.", "dagster_empl_last_target": "Please lmk when you see this if that approach worked for you."}, {"question": "Hello, is there a way to have a job require RunConfig in launchpad but not require mapping the variables to ops? i.e skip config_mapping\n```class AbcConfig(Config):\n    config_a: str\n    config_b: str\n\n@op\ndef op_a(context):\n    <http://context.log.info|context.log.info>(context.run_config.get('config_a'))\n\n@graph\ndef graph_abc():\n    op_a()\n\ngraph_abc_job = graph_abc.to_job(name=\"graph_abc_job\",\n                                 config=AbcConfig\n                                 )\n\nlaunchpad input:\nconfig_a: \"a\"\nconfig_b: \"b\"```", "replies": ["Hi Peter, I don\u2019t think this is possible-- if you want to specify as config params at the top level for a job, you need to use `ConfigMapping`", "So is the convention to always use configmapping and map to ops?\n\nAlso is there any way to have certain op outputs pass values back into \u201crun configs\u201d? Thank you!!", "or is passing values through yield/return better for inter ops?", "&gt; So is the convention to always use configmapping and map to ops?\nI think it\u2019s probably more common to skip config mapping entirely and just directly configure ops, but I\u2019m not sure.\n\n&gt; is there any way to have certain op outputs pass values back into \u201crun configs\u201d?\nNo, the run config is fixed before the run is launched, and op outputs are generated during the run. If you want to pass values between ops you should use dependencies between ops (so yes, passing with `yield`/`return`)."], "is_dagster_empl": [true, false, false, true], "gpt4_replies_target": "In Dagster, if you want to specify config parameters at the top level for a job, you need to use `ConfigMapping`. It's more common to directly configure ops and skip config mapping. The run config is fixed before the run is launched, and op outputs are generated during the run. If you want to pass values between ops, you should use dependencies between ops, passing with `yield`/`return`.", "dagster_empl_first_target": "Hi Peter, I don\u2019t think this is possible-- if you want to specify as config params at the top level for a job, you need to use `ConfigMapping`", "dagster_empl_last_target": "&gt; So is the convention to always use configmapping and map to ops?\nI think it\u2019s probably more common to skip config mapping entirely and just directly configure ops, but I\u2019m not sure.\n\n&gt; is there any way to have certain op outputs pass values back into \u201crun configs\u201d?\nNo, the run config is fixed before the run is launched, and op outputs are generated during the run. If you want to pass values between ops you should use dependencies between ops (so yes, passing with `yield`/`return`)."}, {"question": "trying to use pandera-dagster for validation and keep running into this error (:thread: )", "replies": ["```dagster._core.errors.DagsterTypeCheckDidNotPass: Type check failed for op \"generate_fct_table\" output \"result\" - expected type \"CreatedCollections\". Description: Unexpected error during validation: Data type 'dbdate' not understood by Engine.\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_plan.py\", line 273, in dagster_event_sequence_for_step\n    for step_event in check.generator(step_events):\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py\", line 369, in core_dagster_event_sequence_for_step\n    for user_event in check.generator(\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py\", line 90, in _step_output_error_checked_user_event_sequence\n    for user_event in user_event_sequence:\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/compute.py\", line 192, in execute_core_compute\n    for step_output in _yield_compute_results(step_context, inputs, compute_fn):\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/compute.py\", line 161, in _yield_compute_results\n    for event in iterate_with_context(\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_utils/__init__.py\", line 445, in iterate_with_context\n    next_output = next(iterator)\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/compute_generator.py\", line 124, in _coerce_op_compute_fn_to_iterator\n    result = invoke_compute_fn(\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/execution/plan/compute_generator.py\", line 118, in invoke_compute_fn\n    return fn(context, **args_to_pass) if context_arg_provided else fn(**args_to_pass)\n  File \"/Users/kevin/zora/zora-backend/data/zora/dags/derived_tables/graph.py\", line 66, in build_table\n    data = generate_fct_table()\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/definitions/op_definition.py\", line 482, in __call__\n    return op_invocation_result(self, context, *args, **kwargs)\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/definitions/op_invocation.py\", line 183, in op_invocation_result\n    return _type_check_output_wrapper(op_def, result, bound_context)\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/definitions/op_invocation.py\", line 425, in _type_check_output_wrapper\n    return _type_check_function_output(op_def, result, context)\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/definitions/op_invocation.py\", line 435, in _type_check_function_output\n    _type_check_output(output_defs_by_name[event.output_name], event, context)\n  File \"/Users/kevin/.pyenv/versions/3.9.12/envs/default/lib/python3.9/site-packages/dagster/_core/definitions/op_invocation.py\", line 458, in _type_check_output\n    raise DagsterTypeCheckDidNotPass(```", "here's my pandera schema\n```\nclass CreatedCollections(pa.SchemaModel):\n    \"\"\"Created collections\"\"\"\n\n    ds: pd.Timestamp = pa.Field(description=\"datestamp of creation\")\n    chain: str = pa.Field(description=\"The chain a collection is created on\")\n    creates: int = pa.Field(ge=1, description=\"Number of creates\")\n    wallet_address: str = pa.Field(description=\"wallet_address\")```", "the datestamp column is problematic - its being read from bigquery as date but not sure what validation I need to add here", "Hey Kevin, IIUC the `ds` column of the pandas dataframe you are returning is not actually a `pd.Timestamp`, but is instead a `dbdate`-- is that right? (I\u2019m a little confused by this because I don\u2019t think `dbdate` is a pandas type?)\n\nCan you convert the column to `pd.Timestamp` before returning the dataframe?", "yeah, I just ended up setting `coerce=True` and it worked fine", "thanks!", "any idea when you will be improving the integration btw pandera and dagster or introducing native data validation like this ?", "&gt; any idea when you will be improving the integration btw pandera and dagster\nWe don\u2019t have anything planned atm but if there are features you\u2019d like to see or bugs to fix, you should open an issue\n\n&gt; introducing native data validation\nThere is a feature we\u2019re currently calling \u201cAsset Expectations\u201d on the roadmap for the next several months. It will be a framework for running arbitrary data quality checks on assets. I\u2019m not sure how much the functionality will overlap with dagster-pandera."], "is_dagster_empl": [false, false, false, true, false, false, false, true], "gpt4_replies_target": "The error you're encountering is due to the 'ds' column in your dataframe not being a 'pd.Timestamp', but a 'dbdate'. You can resolve this by converting the column to 'pd.Timestamp' before returning the dataframe. If you have any suggestions for improving the integration between pandera and dagster, feel free to open an issue. Additionally, a feature called \"Asset Expectations\" for running arbitrary data quality checks on assets is on the roadmap for the next several months.", "dagster_empl_first_target": "Hey Kevin, IIUC the `ds` column of the pandas dataframe you are returning is not actually a `pd.Timestamp`, but is instead a `dbdate`-- is that right? (I\u2019m a little confused by this because I don\u2019t think `dbdate` is a pandas type?)\n\nCan you convert the column to `pd.Timestamp` before returning the dataframe?", "dagster_empl_last_target": "&gt; any idea when you will be improving the integration btw pandera and dagster\nWe don\u2019t have anything planned atm but if there are features you\u2019d like to see or bugs to fix, you should open an issue\n\n&gt; introducing native data validation\nThere is a feature we\u2019re currently calling \u201cAsset Expectations\u201d on the roadmap for the next several months. It will be a framework for running arbitrary data quality checks on assets. I\u2019m not sure how much the functionality will overlap with dagster-pandera."}, {"question": "I have assets that are using a static partition definition. Four assets are used in one downstream asset. Again, all using the same static partition def. When I materialize a single partition, everything is great. If I materialize all (in my case, 10) partitions at the same time, my downstream asset appears to receive the results from a different partition. I am 99% sure I am not imagining this. Searching through documentation, but have not stumbled onto anything to help me troubleshoot.", "replies": ["I am using a custom IO manager and I bet the issue is in my `load_input()`", "Yup, not factoring in `context.has_partition_key` and `context.asset_partition_key` to pull the right partition. Resolved!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue was with the custom IO manager in the `load_input()` function, which was not factoring in `context.has_partition_key` and `context.asset_partition_key` to pull the correct partition. The issue has been resolved.", "dagster_empl_first_target": "I am using a custom IO manager and I bet the issue is in my `load_input()`", "dagster_empl_last_target": "Yup, not factoring in `context.has_partition_key` and `context.asset_partition_key` to pull the right partition. Resolved!"}, {"question": "Hi Guys,\n\nThanks for bringing dagster to the world !\nI have a *newbie* question :\n\nWhy is there two different resources for S3 :\n\u2022  S3FileManagerResource\n\u2022  S3resource\nWho seem to be doing almost the same thing.\nIt is not entirely clear to me which one is to be used.\n Is there a difference ?\n Which one should one use ? in which case ?\n\n Thanks in advance,\n Lucas", "replies": ["\u2022 `S3Resource`: this gives you an s3 client, basically an interface to your bucket for doing i/o in your ops (it's a boto s3 client underneath, to be specific, but it's in convenient and familiar dagster language so it can be used and configured like your other dagster resources)\n\u2022 `S3FileManagerResource`: this is also an interface to your s3 storage, so you _could_ use it as an interface to do the same kind of i/o as `S3Resource` (above), but it also conforms to <https://docs.dagster.io/_apidocs/internals#file-manager-experimental|dagster's representation of a file manager>, which has a bunch of filesystem-agnostic methods available. this would a useful way to connect to s3 if you wanted to write the same code for different kinds of storage locations. so you could have file-related operations that work with s3 storage, a local filesystem, or somewhere else in the cloud. ", "i put `S3Resource` first because it's probably the way to go most of the time. easy access to any of your buckets from an op! \n\n...but if you have a variety of s3 and non-s3 storage locations that need to be interchangeable, i could see why you would want the level of abstraction the `S3FileManagerResource` provides. personally i am not using any dagster `FileManager`s, but the regular `S3Resource` is very flexible and useful all over the place.", "<@U05EUJB6TMF> nailed it.", "Thank you very much for your answer !"], "is_dagster_empl": [false, false, true, false], "gpt4_replies_target": "The `S3Resource` provides an interface to your S3 bucket for I/O operations, while the `S3FileManagerResource` also provides an interface to your S3 storage but conforms to Dagster's representation of a file manager, which is useful if you want to write the same code for different kinds of storage locations.", "dagster_empl_first_target": "<@U05EUJB6TMF> nailed it.", "dagster_empl_last_target": "<@U05EUJB6TMF> nailed it."}, {"question": "Hi team - Do we have Dagster custom I/O manager for elasticsearch or opensearch?", "replies": ["Hi Abhishek, There is no official dagster integration for either of these, so no we do not provide an IO manager.", "If you'd like to write one for yourself, here's an example of how that can be done: <https://docs.dagster.io/concepts/io-management/io-managers#a-custom-io-manager-that-stores-pandas-dataframes-in-tables>\n\nYou would just need to define how you want the data to be read into memory and persisted into storage with those methods.", "Thanks <@U01GTMVMGQH> and <@U03AJ8UAPDW>\nI'll take a look.."], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "There is no official Dagster integration for Elasticsearch or Opensearch, but you can create a custom I/O manager by defining how you want the data to be read into memory and persisted into storage.", "dagster_empl_first_target": "Hi Abhishek, There is no official dagster integration for either of these, so no we do not provide an IO manager.", "dagster_empl_last_target": "If you'd like to write one for yourself, here's an example of how that can be done: <https://docs.dagster.io/concepts/io-management/io-managers#a-custom-io-manager-that-stores-pandas-dataframes-in-tables>\n\nYou would just need to define how you want the data to be read into memory and persisted into storage with those methods."}, {"question": "May I suggest as update to the documentation <https://docs.dagster.io/guides/dagster/using-environment-variables-and-secrets#from-dagster-configuration> regarding `EnvVar`. I had tried to pass an env var into an IO manager using string interpolation, e.g. `base_path=f\"{BUCKET}/key\"` which didn't work since outside of configurable dagster objects it would resolve into the key passed in... :blob_facepalm:", "replies": ["Hi YH, Thanks for the feedback-- I\u2019m a little unclear on what confused you in the docs, could you elaborate a little?", "```\"stg_s3_pandas_io_manager\": S3PandasParquetIOManager(\n    base_path=f\"s3://{EnvVar(STG_BUCKET)}\",\n    aws_access_key_id=EnvVar(\"AWS_ACCESS_KEY_ID\"),\n    aws_secret_access_key=EnvVar(\"AWS_SECRET_ACCESS_KEY\"),\n),```\nIf we were to interpret the docs as-is where `Configurable Dagster objects - such as ops, assets, resources, I/O managers, and so on - can accept configuration from environment variables` one would think that the above config would work. However, because a string interpolation was done for `base_path` `EnvVar` would resolve to the `STG_BUCKET` instead of the actual env var. More of a gotcha than anything.", "Ah, I see what you\u2019re saying and how you could get confused here, particularly if you\u2019re not very experienced in Python.\n\nHowever, assuming solid knowledge of Python, I don\u2019t think the docs here are unclear (though I see how one could make the mistake)-- any value that is used in a string interpolation must be converted to a string, so it is impossible to pass an `EnvVar` object to dagster as part of a string interpolation.\n\nThat said, we\u2019ll make a clarification if we see other users run into trouble here. Thanks again for the feedback."], "is_dagster_empl": [true, false, true], "gpt4_replies_target": "The user suggested an update to the Dagster documentation regarding the use of `EnvVar` in string interpolation, which they found confusing. The expert clarified that any value used in string interpolation must be converted to a string, so it's impossible to pass an `EnvVar` object to Dagster as part of a string interpolation. They agreed to make a clarification if other users also find this confusing.", "dagster_empl_first_target": "Hi YH, Thanks for the feedback-- I\u2019m a little unclear on what confused you in the docs, could you elaborate a little?", "dagster_empl_last_target": "Ah, I see what you\u2019re saying and how you could get confused here, particularly if you\u2019re not very experienced in Python.\n\nHowever, assuming solid knowledge of Python, I don\u2019t think the docs here are unclear (though I see how one could make the mistake)-- any value that is used in a string interpolation must be converted to a string, so it is impossible to pass an `EnvVar` object to dagster as part of a string interpolation.\n\nThat said, we\u2019ll make a clarification if we see other users run into trouble here. Thanks again for the feedback."}, {"question": "Hello :wave_anim:\nI\u2019m defining a partitioned job using :\n```@graph_asset(partitions_def=payment_page_partition)\ndef my_asset():\n   ...\n\nmy_job = define_asset_job(\"my_job\", \"my_asset\")```\nI then have a sensor returning RunRequests with a config (specific to each partition):\n```@asset(job=my_job)\ndef my_asset:\n   SensorResult(\n        run_requests=[ RunRequest(\n        partition_key=\"my_key\",\n        run_key=\"my_key\",\n        run_config={config for this key}\n    )],\n        dynamic_partitions_requests=[\n            dynamic_partition.build_add_request(\"my_key\")\n \n        ]\n    )```\nHowever, the config passed this way to the RunRequest is not being picked up when running the job.\nDoes anybody know if I\u2019m missing out on something conceptually?\nWe do this usually for our non-asset jobs and it works fine there", "replies": ["Hi Pierre, do the underlying ops called in `graph_asset` accept a configuration argument?\nMy setup is slightly different in the sense that I'm using `@graph` , `AssetsDefinition.from_graph`, and `@sensor`, but otherwise seems similar.\nAlso, have you tried materializing a partition with the launchpad in dagit?", "Thanks Justin,\nThe underlying graph actually has ops that need to be configured.\nI just tried from the launchpad, and I\u2019m able to materialise a single partition filling in the scaffolded config.\nedit: I\u2019m using config_schema actually for the ops, I\u2019m wondering if that makes a difference", "Yes, it would be interesting if you get different behavior when you subclass `dagster.Config` and provide those as arguments to the ops. I haven't used the `config_schema` functionality yet. At first glance, it seems like it is accomplishing the same goal as passing `Config` as an argument to the ops, but maybe it works differently under the hood.", "I will run an experiment :wink:", "```@op\ndef experiment_op(config: ExperimentAssetConfig):\n    return config.name\n\n@graph_asset(partitions_def=experiment_partition)\ndef experiment_asset():\n    return experiment_op()\n\nexperiment_asset_job = define_asset_job(\"experiment_job\", \"experiment_asset\")\n\n@sensor(job=experiment_asset_job)\ndef experiment_sensor():\n    return SensorResult(\n        run_requests=[RunRequest(\n            run_key=\"experiment\",\n            run_config={\"ops\": {\"experiment_asset\": {\"ops\": {\"experiment_op\": {\"config\": {\"name\": \"experiment\"}}}}}},\n            partition_key=\"experiment\",\n        ),\n        RunRequest(\n            run_key=\"experiment2\",\n            run_config={\"ops\": {\"experiment_asset\": {\"ops\": {\"experiment_op\": {\"config\": {\"name\": \"experiment2\"}}}}}},\n            partition_key=\"experiment2\",\n        )\n        ],\n        dynamic_partitions_requests=[\n            experiment_partition.build_add_request([\"experiment\", \"experiment2\"])\n        ])\n    ```", "My bad it was full of typos, this actually works! Thanks a million <@U0522FABZGQ> :daggy-love:", "Awesome! Glad it worked and thanks for reporting your findings <@U051LKD88JZ>"], "is_dagster_empl": [false, false, false, false, false, false, false], "gpt4_replies_target": "The issue was resolved by ensuring the underlying ops in `graph_asset` accepted a configuration argument and by correctly passing the `Config` as an argument to the ops. The user also found that using `config_schema` for the ops made a difference.", "dagster_empl_first_target": "Hi Pierre, do the underlying ops called in `graph_asset` accept a configuration argument?\nMy setup is slightly different in the sense that I'm using `@graph` , `AssetsDefinition.from_graph`, and `@sensor`, but otherwise seems similar.\nAlso, have you tried materializing a partition with the launchpad in dagit?", "dagster_empl_last_target": "Awesome! Glad it worked and thanks for reporting your findings <@U051LKD88JZ>"}, {"question": "I use PDM inside my code location server container but the Dagster Helm chart's dagsterApiGrpcArgs expects arguments for \"dagster api grpc ....\" when I actually want it to run \"pdm run dagster api grpc ...\". How do I fix this issue?", "replies": ["Hi rma, by PDM do you mean Python Dependency Manager?", "Just checking in, have you managed to deal with your issue?", "I\u2019m going to mark this as resolved, but feel free to come back to this thread if you are still having issues."], "is_dagster_empl": [true, true, true], "gpt4_replies_target": "To fix the issue of Dagster Helm chart's dagsterApiGrpcArgs expecting arguments for \"dagster api grpc ....\" when you want it to run \"pdm run dagster api grpc ...\", you may need to customize the Helm chart to accommodate the \"pdm run\" command.", "dagster_empl_first_target": "Hi rma, by PDM do you mean Python Dependency Manager?", "dagster_empl_last_target": "I\u2019m going to mark this as resolved, but feel free to come back to this thread if you are still having issues."}, {"question": "Hi,\nWe have a repository where we create a job per client. The list of clients along their configurations is fetched through a REST endpoint. Loading this repository is taking very long (~40 seconds) compared to the cadence that Dagster uses to reload it automatically (every minute). We are trying to move to the lazy loading approach, but we couldn't figure out how to parametrise the callables used by the lazy loading as we need to know at least the client id in order to be able to build their assets graph.\n\u2022 Going for option 2 here: <https://docs.dagster.io/_apidocs/repositories>\n\u2022 Consider the following simplified case:\n```import datetime\n\nfrom dagster import (\n    define_asset_job,\n    AssetSelection,\n    TimeWindowPartitionsDefinition,\n    JobDefinition,\n    repository\n)\n\n\nclass MyService:\n\n    def to_job_name(self, client) -&gt; JobDefinition:\n        return f\"my_job_{client.id}\"\n\n    def get_job_for_client(self, client) -&gt; JobDefinition:\n        assets_partitions = TimeWindowPartitionsDefinition(\n            start=datetime.date(2023, 1, 1),\n            fmt=\"%Y-%m-%d\",\n            cron_schedule=client.config.running_schedule\n        )\n        my_assets = ...\n        run_config = ...  # run config is built based on the client config\n\n        my_job = define_asset_job(\n            name=self.to_job_name(client),\n            selection=AssetSelection.assets(*my_assets),\n            partitions_def=assets_partitions,\n            config={\n                \"ops\": run_config\n            }\n        )\n\n\n@repository(name=\"my_repo\")\ndef dagster():\n    api = MyAPI()\n    service = MyService()\n    clients = api.load_all_clients()\n    jobs = {\n        service.to_job_name(client): service.get_job_for_client(client) # this call is not conformant with the interface specified by Dagster\n        for client in clients\n    }\n\n    repository_data = {\n        'jobs': jobs,\n        # 'schedules': ...\n    }\n\n    return repository_data```\nThank you!", "replies": ["Hi, not sure I fully understand but I think you could just wrap the `service.get_job_for_client(client) in a lambda:\n\n```@repository(name=\"my_repo\")\ndef dagster():\n    api = MyAPI()\n    service = MyService()\n    clients = api.load_all_clients()\n    jobs = {\n        service.to_job_name(client): lambda: service.get_job_for_client(client)\n        for client in clients\n    }```\nThat should lazy load jobs as needed, though the call to `api.load_all_clients()` appears unavoidable. If it doesn\u2019t change much you could use some caching scheme though and just use the cached result until X minutes have elapsed.", "<@U01GTMVMGQH> the issue here is that dagster expects a parameterless function to generate the job (see <https://docs.dagster.io/_apidocs/repositories> : lazy-loaded repository).\n\nthe expected interface is:\n```'jobs': Dict[str, Callable[[], JobDefinition]]```\nTherefore a mapping of\n```job_name: get_job_for_client(client)```\nwould not work due to the client parameter being present", "```  jobs = {\n        service.to_job_name(client): lambda: service.get_job_for_client(client)\n        for client in clients\n    }```\nMaybe I\u2019m being dense here but `lambda: service.get_job_for_client(client)` is a parameterless function, no? The cilent id here is defined in the closure, not a function parameter.", "The issue here is that `lambda: service.get_job_for_client(client)`  does not defer the resolution of `client` . That means that we have client A and client B the result is:\n```jobs = {\n  \"A\": service.get_job_for_client(\"B\"),\n  \"B\": service.get_job_for_client(\"B\"),\n}```\nwhich is not correct.\n\nBut thanks to your hint I adapted the lambda to be parameterized, but can be called without parameters through defaults.\nThe final solution to get it working is\n```jobs = {\n        service.to_job_name(client): lambda c=client: service.get_job_for_client(c)\n        for client in clients\n    }```", "Ah classic tricky variable scoping gotcha. Good solution. FWIW you can also get around it by creating the lambda in a separate function (scope):\n\n```funcs = []\n\ndef make_func(x):\n    return lambda: x\n\nfor x in [1, 2, 3]:\n    funcs.append(make_func(x))\n    \nprint([f() for f in funcs])  # =&gt; [1, 2, 3]```"], "is_dagster_empl": [true, false, true, false, true], "gpt4_replies_target": "To lazy load jobs in Dagster, you can wrap the `service.get_job_for_client(client)` in a lambda function. However, to avoid incorrect resolution of `client`, you can parameterize the lambda but make it callable without parameters through defaults, like so: `lambda c=client: service.get_job_for_client(c)`.", "dagster_empl_first_target": "Hi, not sure I fully understand but I think you could just wrap the `service.get_job_for_client(client) in a lambda:\n\n```@repository(name=\"my_repo\")\ndef dagster():\n    api = MyAPI()\n    service = MyService()\n    clients = api.load_all_clients()\n    jobs = {\n        service.to_job_name(client): lambda: service.get_job_for_client(client)\n        for client in clients\n    }```\nThat should lazy load jobs as needed, though the call to `api.load_all_clients()` appears unavoidable. If it doesn\u2019t change much you could use some caching scheme though and just use the cached result until X minutes have elapsed.", "dagster_empl_last_target": "Ah classic tricky variable scoping gotcha. Good solution. FWIW you can also get around it by creating the lambda in a separate function (scope):\n\n```funcs = []\n\ndef make_func(x):\n    return lambda: x\n\nfor x in [1, 2, 3]:\n    funcs.append(make_func(x))\n    \nprint([f() for f in funcs])  # =&gt; [1, 2, 3]```"}, {"question": "Hi. The docs seem to currently suggest two different ways of limiting concurrency using tags:\n\u2022 <https://docs.dagster.io/guides/limiting-concurrency-in-data-pipelines#limiting-specific-runs-using-tags>\n\u2022 <https://docs.dagster.io/deployment/dagster-instance#run-coordinator>\n(you'll have to click the inline tabs to get to the relevant examples)\n\nAny preference between the two, and/or should the docs be updated for one example to reference the other approach?\n\n```run_queue:\n  max_concurrent_runs: 15\n  tag_concurrency_limits:\n    - key: \"database\"\n      value: \"redshift\" # applies when the `database` tag has a value of `redshift`\n      limit: 4\n    - key: \"dagster/backfill\" # applies when the `dagster/backfill` tag is present, regardless of value\n      limit: 10```\n```run_coordinator:\n  module: dagster.core.run_coordinator\n  class: QueuedRunCoordinator\n  config:\n    max_concurrent_runs: 25\n    tag_concurrency_limits:\n      - key: \"database\"\n        value: \"redshift\"\n        limit: 4\n      - key: \"dagster/backfill\"\n        limit: 10```", "replies": ["Hey Jens! Thanks for pointing this out - I think the second example (run coordinator page) was missed when I made that concurrency guide. I\u2019ll get that updated.\n\nHaving a top-level `run_queue` key like that in the first example will enable the `QueuedRunCoordinator` and apply any tag limits you\u2019ve defined.", "Great! Thanks for confirming :slightly_smiling_face:"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The second example (run coordinator page) was missed when the concurrency guide was made, having a top-level `run_queue` key like that in the first example will enable the `QueuedRunCoordinator` and apply any tag limits you\u2019ve defined.", "dagster_empl_first_target": "Hey Jens! Thanks for pointing this out - I think the second example (run coordinator page) was missed when I made that concurrency guide. I\u2019ll get that updated.\n\nHaving a top-level `run_queue` key like that in the first example will enable the `QueuedRunCoordinator` and apply any tag limits you\u2019ve defined.", "dagster_empl_last_target": "Hey Jens! Thanks for pointing this out - I think the second example (run coordinator page) was missed when I made that concurrency guide. I\u2019ll get that updated.\n\nHaving a top-level `run_queue` key like that in the first example will enable the `QueuedRunCoordinator` and apply any tag limits you\u2019ve defined."}, {"question": "Hi, I\u2019m running dagster using web browser vscode on jupyterhub that runs on kubernetes.\nthe flow is,\n\u2022 I have a pod that runs the vscode\n\u2022 I build my docker image with dagit and dagster.\n\u2022 I access to <https://golf.omic.link/testbench/user/eldan.hamdani/vscode/proxy/3000/> - got status code 200\n\u2022 but all `css` and `js` link are in root which is <https://golf.omic.link/static/js/main.01fb5b54.js> (they should be in <https://golf.omic.link/static/js/main.01fb5b54.js> (if I run this manually I get a response).\nhow can I fixed it? I tried to add `-l \"/testbench/user/eldan.hamdani/vscode/proxy/3000\"` to dagit command on the entrypoint but it still not working, any dea?", "replies": ["", "Hi Eldan, Just want you to know I\u2019ve seen this and am looking for an engineer with the expertise to answer.", "Ah actually I can see that you posted this in another channel and prha responded, so I\u2019m going to close this and let the discussion continue there."], "is_dagster_empl": [false, true, true], "gpt4_replies_target": "The user is experiencing issues with the paths of `css` and `js` links while running dagster on a web browser vscode on jupyterhub that runs on kubernetes, and is seeking assistance to fix it.", "dagster_empl_first_target": "Hi Eldan, Just want you to know I\u2019ve seen this and am looking for an engineer with the expertise to answer.", "dagster_empl_last_target": "Ah actually I can see that you posted this in another channel and prha responded, so I\u2019m going to close this and let the discussion continue there."}, {"question": "Hello everyone,\n\nI\u2019m querying a table that only allow data from the last 7 days to be queried.\n\nI\u2019m wondering if there\u2019s a way to block dagster partitions to be rebuild after that 7 days period?\n\nThe reason is that if I rebuild something from more than 7 days I\u2019ll receive an error or and empty string from the API, and I\u2019ll update the records with that and I don\u2019t want that at all.\n\nIs there a way to achieve this?", "replies": ["Have you seen this example in the documentation? It might be overkill for what you are trying to do, but seems like it is getting at the same problem.\n<https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#monitoring-daily-assets-to-materialize-a-weekly-asset-using-a-multi_asset_sensor|https://docs.dagster.io/concepts/partitions-schedules-sensors/asset-sensors#monitoring-daily-assets-to-materia[\u2026]ly-asset-using-a-multi_asset_sensor>", "Hi Jacob, When you say \u201crebuild\u201d, do you mean \u201crematerialized\u201d? Or are you talking about changing the partitions definition itself?", "Just checking in to see if you\u2019ve made progress on this issue?", "<@U01GTMVMGQH> I mean `rematerialized` sorry for the confusion.\n\nAnd oh thank you I didn\u2019t see those comments at all pop out in slack.\n\nI\u201dll look at the propose solution now. Thank you very much.", "<@U0522FABZGQ> The documentation you sent me seems to be link to something else. Aka the materialization of downstream assets based of if a `materialized` group of parent asset exist or not.\n\nMy question was more. If the `client` I\u2019m using. Isn\u2019t able to query more than X days in the past. is it possible to restrict users from `rematerializing` assets that are `x days` in the past from now.\n\nI know that I can restrict that in the `client` itself. Like `return` an `error` in the partition I\u2019m trying to rebuild is older than `x days`. But I was wondering if that could be done in Dagster itself.\n\nLike a way to blury the materialization or something. Something that tell me, than this partition is `completed` (green) or `not` (red), but there\u2019s no way for me to rebuild it by clicking on `materialize`  .", "IIUC Jacob, a simple model of your situation is that you have a single partitioned asset, like this:\n\n```@asset(partitions_def=DailyPartitionsDefinition(start_date=datetime(2020, 1, 1))\ndef my_asset(context: AssetExecutionContext):\n    return client.query(context.partition_key)```\nAnd your question is whether it is possible to _block_ any attempts to materialize certain partitions (those older than N days). Is that right?", "Exactly.", "That is not possible right now, though I see how this could be useful. If you want to see this feature, the best thing is to open a GH issue: <https://github.com/dagster-io/dagster/issues>", "I know it\u2019s possible in the asset itself. But was wondering if Dagster has any flag that block the rebuild and show it to the user.\n\nAny `immutable` records that I get from dagster, shouldn\u2019t be replayable IMO.\n\nBut having the partitions setup so I know if I miss any is still very usefull.\n\nThank <@U01GTMVMGQH> I\u2019ll fill a feature request :slightly_smiling_face:\n\nHave a great day."], "is_dagster_empl": [false, true, true, false, false, true, false, true, false], "gpt4_replies_target": "Currently, Dagster does not have a feature to block the rematerialization of partitions older than a certain number of days. However, you can implement this restriction in the client itself. If you want this feature in Dagster, you can open a feature request on their GitHub page.", "dagster_empl_first_target": "Hi Jacob, When you say \u201crebuild\u201d, do you mean \u201crematerialized\u201d? Or are you talking about changing the partitions definition itself?", "dagster_empl_last_target": "That is not possible right now, though I see how this could be useful. If you want to see this feature, the best thing is to open a GH issue: <https://github.com/dagster-io/dagster/issues>"}, {"question": "Hi,\nI am struggling to understand how do I pass `run_config` for `build_schedule_from_partitioned_job` ?", "replies": ["my partition definition for the asset and schedule:\n```MultiPartitionsDefinition(\n    {\n        \"date\": DailyPartitionsDefinition(\n            start_date=\"2023-06-25\", timezone=\"UTC\", fmt=\"%Y-%m-%d\"\n        ),\n        \"type\": StaticPartitionsDefinition([\"a\", \"b\", \"c\"]),\n    }\n)```", "Hi Deividas, a run config can\u2019t be associated with a schedule, but to be either associated with the job as a whole or provided on a per-run basis. The recommended way to provide a per-run run config to the runs launched by a schedule on a partitioned job is to create the job using a `PartitionedConfig` object. See here: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#date-partitioned-job>", "Thanks, got my way around with `PartitionedConfig` :sweat_smile:"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "To pass `run_config` for `build_schedule_from_partitioned_job`, you should create the job using a `PartitionedConfig` object, as run config can't be associated with a schedule but with the job as a whole or provided on a per-run basis.", "dagster_empl_first_target": "Hi Deividas, a run config can\u2019t be associated with a schedule, but to be either associated with the job as a whole or provided on a per-run basis. The recommended way to provide a per-run run config to the runs launched by a schedule on a partitioned job is to create the job using a `PartitionedConfig` object. See here: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#date-partitioned-job>", "dagster_empl_last_target": "Hi Deividas, a run config can\u2019t be associated with a schedule, but to be either associated with the job as a whole or provided on a per-run basis. The recommended way to provide a per-run run config to the runs launched by a schedule on a partitioned job is to create the job using a `PartitionedConfig` object. See here: <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#date-partitioned-job>"}, {"question": "Hey there, is there a way to throw an errow when running a scheduled job if an upstream asset has not been materialzed?", "replies": ["Hey Pablo, Can you elaborate a little? Do you mean an upstream source asset? Since regular assets should materialize in topological order during the run.", "Yeah basically we have a set of DBT models. The first set will materialize at some time. And the second set is scheduled a few hours later but depends on the first set. If the first set fails for some reason I want the second set to throw an error instead of attempting to run with missing data.", "Hmm I think the way to go here is to check for the data or materialization events inside your schedule evaluation function, and then return a `SkipReason` if it does not exist: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules>", "Ah sweet that makes sense thank you!]\\"], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "In Dagster, you can check for the data or materialization events inside your schedule evaluation function, and then return a `SkipReason` if it does not exist.", "dagster_empl_first_target": "Hey Pablo, Can you elaborate a little? Do you mean an upstream source asset? Since regular assets should materialize in topological order during the run.", "dagster_empl_last_target": "Hmm I think the way to go here is to check for the data or materialization events inside your schedule evaluation function, and then return a `SkipReason` if it does not exist: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules>"}, {"question": "Is there a way to provide a mapping from function arguments to resources, as there is for assets? For example, I can do:\n\n```@asset\ndef upstream_asset():\n    return 1\n\n@asset(ins={\"upstream\": AssetIn(\"upstream_asset\")})\ndef downstream_asset(upstream):\n    return upstream + 4```\nCan I also do something along the lines of\n\n```resource_a = MyConfigurableResource(some_prop = \"a\")\nresource_b = MyConfigurableResource(some_prop = \"b\")\n\n@asset(ins={\"resource\": ResourceIn(\"resource_a\")})\ndef downstream_asset(resource: MyConfigurableResource):\n    return resource.get_client().do_something()```", "replies": ["This is in the context of setting up an asset factory (<https://github.com/dagster-io/dagster/discussions/11045|like this example> from <@U011CET83FG>), but some of the assets require `database_a`, while others need `database_b`, and Postgres requires a new connection per database (so I can't use a `use database some_database` clause to switch).", "Hi Muhammad, so to clarify the issue is that you want to dynamically create your assets, so you can\u2019t just specify `resource_a` or `resource_b` as the parameter name in the function?", "Yeah. Right now, I'm basically creating two copies of the function, each with one of the resource's keys as a function argument, but it's a relatively clunky setup.\n\n```from dagster import asset\nfrom .resources import MyConfigurableResource\n\nresource_a = MyConfigurableResource(prop = 'a')\nresource_b = MyConfigurableResource(prop = 'b')\n\ndef asset_factory(spec):\n  def asset_using_a(resource_a: MyConfigurableResource):\n    # some logic\n    ...\n\n  def asset_using_b(resource_b: MyConfigurableResource):\n    # some logic that's a repetition of what's in asset_using_a\n    ...\n\n  if spec[\"resource_required\"] == \"a\":\n    return asset(asset_using_a)\n  else:\n    return asset(asset_using_b)```\nThat's what I'm doing right now, but it would be nice to have a resource mapping (like what's available for assets using `ins` and `AssetIn`) so that I can have just one inner function that maps to the right resource, instead of duplicating the logic in two functions, just so I can change the name of the argument.", "I just reached out to the engineer who develops resources the most and he said this is not currently implemented but he has a feature branch that does it on the back burner. So for now we don\u2019t have a solution for you but we\u2019ll probably release it in the next month or so.", "Got it, thanks for checking! It would definitely be a nice feature to have in the future. :crossed_fingers:"], "is_dagster_empl": [false, true, false, true, false], "gpt4_replies_target": "Currently, Dagster does not support a mapping from function arguments to resources like it does for assets, but this feature is under development and may be released in the future.", "dagster_empl_first_target": "Hi Muhammad, so to clarify the issue is that you want to dynamically create your assets, so you can\u2019t just specify `resource_a` or `resource_b` as the parameter name in the function?", "dagster_empl_last_target": "I just reached out to the engineer who develops resources the most and he said this is not currently implemented but he has a feature branch that does it on the back burner. So for now we don\u2019t have a solution for you but we\u2019ll probably release it in the next month or so."}, {"question": "Hello friends, how do I connect to the graphql api to launch jobs when I use Dagster Cloud?\nI have tried\n```from dagster_graphql import DagsterGraphQLClient\n\nclient = DagsterGraphQLClient(\"dagit.dagster.OUR_COMPANY_NAME\")```\nbut it doesn't seem to work. Also shouldn't there be any token or something for authorization?", "replies": ["Here\u2019s a github discussion that shows you how to do this: <https://github.com/dagster-io/dagster/discussions/7772|https://github.com/dagster-io/dagster/discussions/7772>", "Thanks <@U01AV4K3W14>! curious what the difference is between `repository_location_name` and `repository_name` in `client.submit_job_execution` is?", "This worked <@U01AV4K3W14>. Thank you so much!"], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "To connect to the graphql API to launch jobs when using Dagster Cloud, you need to use the DagsterGraphQLClient with the correct URL and also provide an authorization token. You can find more details on how to do this in the following GitHub discussion: https://github.com/dagster-io/dagster/discussions/7772.", "dagster_empl_first_target": "Here\u2019s a github discussion that shows you how to do this: <https://github.com/dagster-io/dagster/discussions/7772|https://github.com/dagster-io/dagster/discussions/7772>", "dagster_empl_last_target": "Here\u2019s a github discussion that shows you how to do this: <https://github.com/dagster-io/dagster/discussions/7772|https://github.com/dagster-io/dagster/discussions/7772>"}, {"question": "How to use ScheduleEvaluationContext", "replies": ["you could do something like this\n```def should_execute(context: ScheduleEvaluationContext):\n    return True # or False as needed\nschedule = ScheduleDefinition(&lt;other_params&gt;, should_execute: should_execute)```", "Deividas is correct. <@U05EBV117NE> if you want to use a schedule you don\u2019t need to use a sensor at the same time. You might want to look at our schedule/sensor docs: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules>"], "is_dagster_empl": [false, true], "gpt4_replies_target": "To use ScheduleEvaluationContext in Dagster, you can define a function like `should_execute(context: ScheduleEvaluationContext)` that returns either True or False as needed, and then use this function in your ScheduleDefinition. For more details, you can refer to the Dagster schedule/sensor documentation.", "dagster_empl_first_target": "Deividas is correct. <@U05EBV117NE> if you want to use a schedule you don\u2019t need to use a sensor at the same time. You might want to look at our schedule/sensor docs: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules>", "dagster_empl_last_target": "Deividas is correct. <@U05EBV117NE> if you want to use a schedule you don\u2019t need to use a sensor at the same time. You might want to look at our schedule/sensor docs: <https://docs.dagster.io/concepts/partitions-schedules-sensors/schedules>"}, {"question": "Has anyone here gotten AWS ECS + Docker Compose + AWS SSO to work together? I keep getting this error when trying to follow the ECS deployment strategy for Dagster OSS.\n\n```$ docker --context dagster-ecs compose --project-name dagster up\n\nNoCredentialProviders: no valid providers in chain. Deprecated.\n        For verbose messaging see aws.Config.CredentialsChainVerboseErrors```\nThe rest of the AWS CLI works fine on this profile\u2026", "replies": ["Ahh \u2014 Docker doesn\u2019t implement SSO by choice\n\n<https://github.com/docker/compose-cli/issues/671>", "You pointed me in the right direction but instead of configuring SSO I chose to use AWS ENV vars when creating the context instead of using an existing profile. That worked for me. :pray:"], "is_dagster_empl": [false, false], "gpt4_replies_target": "Docker does not implement SSO by choice, but you can use AWS ENV vars when creating the context instead of using an existing profile to resolve the issue.", "dagster_empl_first_target": "Ahh \u2014 Docker doesn\u2019t implement SSO by choice\n\n<https://github.com/docker/compose-cli/issues/671>", "dagster_empl_last_target": "You pointed me in the right direction but instead of configuring SSO I chose to use AWS ENV vars when creating the context instead of using an existing profile. That worked for me. :pray:"}, {"question": "I\u2019m trying to set up a custom IO manager that handles partition keys, and I\u2019m a bit confused about how to implement `load_input` properly.\n\nIn the case where a partitioned asset materialization is loading a non-partitioned asset, it seems like `InputContext.partition_key` is still set to the partition key of the materialization. Does that mean I cannot deduce from the context passed to `load_input` whether the upstream asset is partitioned or not?", "replies": ["After looking at `UPathIOManager` , which is the base class for the `fs_io_manager` , it seems like looking at `context.has_asset_partitions`  and `context.asset_partition_keys` is the way to go:\n<https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/upath_io_manager.py#L134-L205>"], "is_dagster_empl": [false], "gpt4_replies_target": "Yes, you can deduce whether the upstream asset is partitioned or not by looking at `context.has_asset_partitions` and `context.asset_partition_keys` as shown in the `UPathIOManager` base class for the `fs_io_manager`.", "dagster_empl_first_target": "After looking at `UPathIOManager` , which is the base class for the `fs_io_manager` , it seems like looking at `context.has_asset_partitions`  and `context.asset_partition_keys` is the way to go:\n<https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/upath_io_manager.py#L134-L205>", "dagster_empl_last_target": "After looking at `UPathIOManager` , which is the base class for the `fs_io_manager` , it seems like looking at `context.has_asset_partitions`  and `context.asset_partition_keys` is the way to go:\n<https://github.com/dagster-io/dagster/blob/master/python_modules/dagster/dagster/_core/storage/upath_io_manager.py#L134-L205>"}, {"question": "Hello! One of my Definitions has almost a hundred assets and I\u2019m expecting to add more. For organization\u2019s sake I thought it would make sense to have multiple Definitions where each Definition is a collection of assets that are at different stages of our full data pipeline. These Definitions would be connected using SourceAssets. *Is there an easy way to convert a collection of AssetDefinitions to SourceAssets*? My solution is to gather the assets from the upstream Definition and create SourceAssets from the Assets\u2019 key and io_manager. Is there a more graceful way to do this? *Is this a reasonable way to structure large dagster projects?*", "replies": ["It\u2019s hard to know whether that\u2019s a reasonable organization strategy without knowing the use case, but you can also make use of asset groups to help organize things - since having multiple definitions will require additional operational burden (as you\u2019ve identified with the need to have source assets bridging the gap)", "We do have utility functions for creating source assets out of assets definition objects, but they aren\u2019t public and I\u2019m not 100% sure why that is. cc <@U011CET83FG> thoughts on making those fxns public?", "Thanks! Maybe I\u2019ll keep all of the assets in one definition for now. If think get unruly I\u2019ll explore creating a second Definition with SourceAssets.  A utility function that creates source assets from asset definitions sounds nice!", "you can call `to_source_assets()` on an asset to get a list of `SourceAsset`s", "^ `to_source_assets` isn\u2019t public tho right?", "ah sorry missed what you said <@U017KUAENS0>. yeah, we should probably make it public, but you can still use it and I think it's unlikely that it will change", "<@U02VAARNQ65> if you'd be up for filing a github issue about making it public, that would help us track it", "Great thanks y\u2019all! I\u2019ll create an issue. Is `to_source_assets` used for this situation where you want to link assets from different Definitions?", "I think that's a very legitimate way to use it", "if all you care about is organizing how the assets show up in Dagit, you might find it easier to set group names on your assets", "Sounds good. Is it common to have &gt; 100 assets in a definition?", "definitely", "with large dbt projects, we sometimes see a few thousand", "Oh great. thanks for the help!"], "is_dagster_empl": [true, true, false, true, true, true, true, false, true, true, false, true, true, false], "gpt4_replies_target": "Yes, you can convert a collection of AssetDefinitions to SourceAssets, and it's a reasonable way to structure large Dagster projects. You can use the `to_source_assets()` function on an asset to get a list of `SourceAsset`s, even though it's not public. For organization, you can also use asset groups or set group names on your assets. It's common to have more than 100 assets in a definition.", "dagster_empl_first_target": "It\u2019s hard to know whether that\u2019s a reasonable organization strategy without knowing the use case, but you can also make use of asset groups to help organize things - since having multiple definitions will require additional operational burden (as you\u2019ve identified with the need to have source assets bridging the gap)", "dagster_empl_last_target": "with large dbt projects, we sometimes see a few thousand"}, {"question": "Am i missing something, or is it not possible to fill in default values in the launch pad on jobs?", "replies": ["Yes you can\n\nJust do:\n\n\n```@job(\n    config={\n        'ops': {\n            'your_op_name': {\n                'config': {\n                    'some_key': 10\n                }\n            }\n        }\n    }\n)```", "then it will autofill it in the launchpasd", "I was more thinking to easily insert the values i have as default in the graphs etc. I can see them in the right side of the screen, but not easily auto fill them in.", "This feels like a reasonable request- I added an issue for it <https://github.com/dagster-io/dagster/issues/11828>", "thanks!"], "is_dagster_empl": [false, false, false, true, false], "gpt4_replies_target": "Yes, it is possible to fill in default values in the launch pad on jobs in dagster. You can do this by configuring your job with the default values. However, auto-filling these values is not currently supported, but an issue has been raised for this feature request.", "dagster_empl_first_target": "This feels like a reasonable request- I added an issue for it <https://github.com/dagster-io/dagster/issues/11828>", "dagster_empl_last_target": "This feels like a reasonable request- I added an issue for it <https://github.com/dagster-io/dagster/issues/11828>"}, {"question": "Hi, I did not find anything related in the Slack Archives so here is my problem (using 1.1.9 deployed on K8S): my Backfill daemon does not send heartbeats anymore and is in \u201cNot Running\u201d status. I found the following daemon logs:\n```Traceback (most recent call last):\n  File \"/usr/local/bin/dagster-daemon\", line 8, in &lt;module&gt;\n    sys.exit(main())\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/cli/__init__.py\", line 127, in main\n    cli(obj={})  # pylint:disable=E1123\n  File \"/usr/local/lib/python3.7/site-packages/click/core.py\", line 1130, in __call__\n    return self.main(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/click/core.py\", line 1055, in main\n    rv = self.invoke(ctx)\n  File \"/usr/local/lib/python3.7/site-packages/click/core.py\", line 1657, in invoke\n    return _process_result(sub_ctx.command.invoke(sub_ctx))\n  File \"/usr/local/lib/python3.7/site-packages/click/core.py\", line 1404, in invoke\n    return ctx.invoke(self.callback, **ctx.params)\n  File \"/usr/local/lib/python3.7/site-packages/click/core.py\", line 760, in invoke\n    return __callback(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/cli/__init__.py\", line 43, in run_command\n    _daemon_run_command(instance, kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_core/telemetry.py\", line 110, in wrap\n    result = f(*args, **kwargs)\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/cli/__init__.py\", line 55, in _daemon_run_command\n    controller.check_daemon_loop()\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/controller.py\", line 268, in check_daemon_loop\n    self.check_daemon_heartbeats()\n  File \"/usr/local/lib/python3.7/site-packages/dagster/_daemon/controller.py\", line 239, in check_daemon_heartbeats\n    raise Exception(\"Stopped dagster-daemon process due to thread heartbeat failure\")\nException: Stopped dagster-daemon process due to thread heartbeat failure```\nAnd:\n```Stopping dagster-daemon process since the following threads are no longer sending heartbeats: ['BACKFILL']\nShutting down daemon threads...\nThread for BACKFILL did not shut down gracefully.```\nFor more context, it happened after I tried to launch a backfill of few runs.\nHow can I revert the daemon into a stable state ?", "replies": ["Hi alexis - would it be possible to share the full logs from your daemon? This error indicates that the backfill daemon failed, which means the root cause of the issue likely happened before this message", "Before this message, I only have logs of Dagster starting and the SchedulerDaemon / SensorDaemon doing some of their regular checks before the BackfillDaemon crash. All of this happen in the 2 minutes following the container start. I will try to do an export asap", "", "and this happens reliably for you every time you run the daemon?", "Yes, it happens on every restart of the container without exception", "Any chance you could run py-spy dump in the container during the period of time when its starting up? <https://github.com/benfred/py-spy/blob/master/README.md#how-do-i-run-py-spy-in-docker> Surprised that there's no logging if the thread is seemingly failing, but usually py-spy is helpful for understanding what's going on at the individual thread level", "(If it's on k8s its a bit trickier and often requires adding that securityContext field that they mention to the daemon pod: <https://github.com/benfred/py-spy/blob/master/README.md#how-do-i-run-py-spy-in-kubernetes>, but we have done it successfully in the past)", "Do you recall what exactly happened between the last time this was working and when the problem started happening?", "Were these asset backfills / how many partitions were being backfilled?", "Unfortunately I can\u2019t ship the py-spy as this problem is occuring only on our production cluster and we do not want to put py-spy in production. I will try to reproduce the steps in another environment if I manage to do so.\n\nThe only thing that happened was my attempt to backfill some partitions this morning. It was around 20-30 partitions of a massively partitioned asset (around 10k partitions for this asset I think). After I filled the partitions in the partition selection modal, the message requesting a backfill appeared with its id and the daemon stopped working few moments after that. I did remember spotting this issue as I was not seeing any runs being queued.", "Is there any chance you'd be able to share the code of the partitioned asset with the body of the op removed? We could try on our end to see if we can reproduce the problem\n\nIs there a chance that the daemon is hitting a memory or CPU limit / is that something you're able to monitor during that minute before it goes down for spikes?", "We do have some performance improvements for large partitioned assets coming out in the release today, but I'm struggling to match them with the specific symptoms that you're seeing of the daemon thread crashing (if you were seeing timeouts or heartbeat failures, then sure - but I would not expect the thread to die)", "Here is the asset with its partition:\n```fifteen_minute_partitions = TimeWindowPartitionsDefinition(\n    cron_schedule=\"*/15 * * * *\",\n    start=datetime(2022, 1, 1, 0, 0, 0),\n    fmt=\"%Y-%m-%d %H:%M\",\n    timezone=\"Europe/Paris\",\n)\n\n@asset(\n    group_name=\"group_name\",\n    io_manager_key=settings.FS_IO_MANAGER,\n    key_prefix=\"some_prefix\",\n    partitions_def=fifteen_minute_partitions,\n    required_resource_keys={\"api\"},\n)\ndef tmp_asset(context) -&gt; DataFrame:\n    \"\"\"Some desc.\"\"\"\n    columns = [\n        \"field1\",\n        \"field2\"\n    ]\n    start_dt, end_dt = context.output_asset_partitions_time_window()\n    req_params = RequestParameters(\n        ...\n    )\n    return context.resources.api.fetch(context, req_params, columns)```", "<@U041Q9CD3MM> did you kick of the backfill from the asset graph or from the asset job page?", "<@U011CET83FG> I honestly don\u2019t remember, I think it was from the asset graph where I selected this particular asset out of the graph.\n<@U016C4E5CP8> I just checked and there was no problem with the CPU and memory in the cluster at that given period.", "Oh, you know what, I misread the error message. The thread isn't dying, it's just taking a (very) long time to run. That is much less mysterious", "I have a short term workaround that may help here while we sort this out - if you set the DAGSTER_DAEMON_HEARTBEAT_TOLERANCE env var on your daemon pod to some larger number in seconds (say 7200) it will allow the backfill daemon to take longer to heartbeat without bringing down the whole daemon while we sort this out", "Jumping on thread, because I am hardening my Kube deployment too, Daniel was your prognosis to tweak the HEARTBEAT_TOLERANCE due to the error message `Stopped dagster-daemon process due to thread heartbeat failure` ?", "that's a short-term workaround... it will 'harden' it in the sense that will leave the other threads running for longer when one of them runs into an issue", "the env var in question is is DAGSTER_DAEMON_HEARTBEAT_TOLERANCE", "sorry, yes, I noted the full name, but typed here the short :stuck_out_tongue: , and just for curiosity, why do you call it short term, do you envision that there\u2019s a more \u201cappropriate\u201d fix", "?", "Yeah the appropriate fix would be to squash whatever is causing the backfill daemon to hang - but its a good question about whether we should have the daemon keep the other threads running when one of them runs into issues - seems like that could be a configurable setting at least", "that would only work robustly if there was a built-in self healing mechanism of some sort, else that particular backfill thread will always keep on erroring", "today setting DAGSTER_DAEMON_HEARTBEAT_TOLERANCE to the largest number you can think of would essentially do that", "That's right- I think the thinking was that on some kind of transient failure we wouldn't want to leave the scheduler down forever. I think putting each daemon in its own pod would probably give us the best of both worlds here (at the expense of more pods/resoures used)", "good point, i think the scale of the subsequent partition count would justify some kind of load distribution on the daemon", "<@U041Q9CD3MM> do you have this set to a certain value in your Helm chart values.yaml? i'm a little confused why its giving up so quickly\n\n```dagsterDaemon:\n  heartbeatTolerance:```", "OK, after digging into this a bit more, I think i understand why its giving up so quickly and there's a fix out that issue - it appears to be specific to asset backfills. We think that some perf improvements that are coming out in 1.1.11 later today will help - and separately, setting that heartbeatTolerance value to a very high number may help as well. Thanks for reporting and bearing with us while we sort this out", "I will try to update to 1.1.11 to see if it resolves the problem, and if not I will go for the DAGSTER_DAEMON_HEARTBEAT_TOLERANCE env variable trick. Thanks for your amazing support !", "The 1.1.11 solved it and the daemon is now back to its normal state :partydagster:"], "is_dagster_empl": [true, false, false, true, false, true, true, true, true, false, true, true, false, true, false, true, true, false, true, true, false, false, true, false, true, true, false, true, true, false, false], "gpt4_replies_target": "The issue with the Backfill daemon not sending heartbeats was likely due to a performance issue with large partitioned assets. The user was advised to update to Dagster 1.1.11 which includes performance improvements for large partitioned assets. As a short-term workaround, they were also advised to set the DAGSTER_DAEMON_HEARTBEAT_TOLERANCE environment variable to a larger number to allow the backfill daemon more time to heartbeat.", "dagster_empl_first_target": "Hi alexis - would it be possible to share the full logs from your daemon? This error indicates that the backfill daemon failed, which means the root cause of the issue likely happened before this message", "dagster_empl_last_target": "OK, after digging into this a bit more, I think i understand why its giving up so quickly and there's a fix out that issue - it appears to be specific to asset backfills. We think that some perf improvements that are coming out in 1.1.11 later today will help - and separately, setting that heartbeatTolerance value to a very high number may help as well. Thanks for reporting and bearing with us while we sort this out"}, {"question": "I am new to Dagster, and I have an issue right now that I need help. I have a scheduled job, it failed last 2 schedules. I tried to re-run the Mar 24 several times, but the Tick history is still showing the schedule is failed.  What should I do next?", "replies": ["could anyone help on this issue? or point me to the documentation that I can take a look at it myself. thx,", "Hi Sean, Have you looked at the run logs (clicked through on the link in the \u201cRuns\u201d column? They should contain info about what is failing)", "<@U01GTMVMGQH> thanks for help, I knew the issues. I have re-run the jobs, you can see the second screenshot. the re-runs are successful, but the schedule on the left screenshot is still showing failed on Mar 24.", "Hi Sean, after looking into this, this is intended behavior-- if the run fails, then the tick status should show green, regardless of whether you later successfully re-executed. If you find this unintuitive (I\u2019m sort of undecided myself) I encourage you to file an issue on Github.", "<@U01GTMVMGQH> do you mean `--if the run fails, then the tick status should show ~green~` red here? if yes, understand now."], "is_dagster_empl": [false, true, false, true, false], "gpt4_replies_target": "If a scheduled run fails in Dagster, the tick status will show as failed, regardless of whether you later successfully re-executed it. This is the intended behavior. If you find this unintuitive, you can file an issue on Github.", "dagster_empl_first_target": "Hi Sean, Have you looked at the run logs (clicked through on the link in the \u201cRuns\u201d column? They should contain info about what is failing)", "dagster_empl_last_target": "Hi Sean, after looking into this, this is intended behavior-- if the run fails, then the tick status should show green, regardless of whether you later successfully re-executed. If you find this unintuitive (I\u2019m sort of undecided myself) I encourage you to file an issue on Github."}, {"question": "Hi all. I get the following exception during asset reconciliation sensor evaluation after removing an asset from code:\n```AttributeError: 'NoneType' object has no attribute 'deserialize_subset'\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/errors.py\", line 206, in user_code_error_boundary\n    yield\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_grpc/impl.py\", line 328, in get_external_sensor_execution\n    return sensor_def.evaluate_tick(sensor_context)\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/definitions/sensor_definition.py\", line 421, in evaluate_tick\n    result = list(self._evaluation_fn(context))\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/definitions/sensor_definition.py\", line 586, in _wrapped_fn\n    result = fn(context)\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/definitions/asset_reconciliation_sensor.py\", line 896, in _sensor\n    AssetReconciliationCursor.from_serialized(\n  File \"/usr/local/lib/python3.10/site-packages/dagster/_core/definitions/asset_reconciliation_sensor.py\", line 180, in from_serialized\n    ).deserialize_subset(serialized_subset)```\nit fails here:\n```    178             materialized_or_requested_root_partitions_by_asset_key[key] = cast(\n    179                 PartitionsDefinition, asset_graph.get_partitions_def(key)\n--&gt; 180             ).deserialize_subset(serialized_subset)```\nhaving\n```ipdb&gt; !key\nAssetKey(['editor_success_counts'])\nipdb&gt; !asset_graph.get_partitions_def(key)\nipdb&gt;   # get_partitions_def returns None, as it is also shown by exception```\nShould I report this on github, and how to remove from dagster database the information about an obsolete asset?", "replies": ["Fixed it by clearing the sensor cursor."], "is_dagster_empl": [false], "gpt4_replies_target": "The issue was resolved by clearing the sensor cursor in the Dagster database.", "dagster_empl_first_target": "Fixed it by clearing the sensor cursor.", "dagster_empl_last_target": "Fixed it by clearing the sensor cursor."}, {"question": "Hi! How would I set up a Dagster resource so that I can have a configured instance of it where configuration is taken from environment variables if they are provided otherwise it takes default or simply None value?\n\n```from dagster import resource, Field, StringSource\n\n@resource(config_schema={\n    \"user\": Field(StringSource, description=\"Username.\"),\n    \"pass\": Field(StringSource, description=\"Password.\"),\n    \"proxy\": Field(StringSource, is_required=False, description=\"HTTP proxy to use if provided.\"),\n})\ndef service_credentials(context):\n    \"\"\"S3FileSystem Dagster Resource.\"\"\"\n    return context.resource_config\n\nservice_credentials_from_env = service_credentials.configured({\n    \"user\": {\"env\": \"SERVICE_USER\"},\n    \"pass\": {\"env\": \"SERVICE_PASS\"},\n    \"proxy\": {\"env\": \"SERVICE_HTTP_PROXY\"},\n})```\nIf I do the case above it simply errors with\n```dagster._core.errors.DagsterInvalidConfigError: Error in config for resource service_credentials\n    Error 1: Post processing at path root:config:proxy of original value {'env': 'SERVICE_HTTP_PROXY'} failed:\ndagster._config.errors.PostProcessingError: You have attempted to fetch the environment variable \"SERVICE_HTTP_PROXY\" which is not set. In order for this execution to succeed it must be set in this environment.```", "replies": ["Ah, it seems even default value fallback is not supported. This really takes away the value from StringSource, IntSource and BoolSource IMO. <https://github.com/dagster-io/dagster/issues/3947>", "Hi Anton, I agree this is a problem-- I assigned this issue to myself and will try to knock it out in the next couple weeks.", "Thanks Sean!"], "is_dagster_empl": [false, true, false], "gpt4_replies_target": "The current implementation of Dagster does not support default value fallback for environment variables, which is a known issue and is being worked on for future updates.", "dagster_empl_first_target": "Hi Anton, I agree this is a problem-- I assigned this issue to myself and will try to knock it out in the next couple weeks.", "dagster_empl_last_target": "Hi Anton, I agree this is a problem-- I assigned this issue to myself and will try to knock it out in the next couple weeks."}, {"question": "Hi! Is it possible to cache the repositories (and not the assets) so that you don't have to rebuild them at each run?", "replies": ["Hi Jordan, Could you elaborate a bit on what you mean by \u201crebuild repository\u201d? A repository in Dagster is a python abstraction that holds a bunch of definitions.", "Of course, I'm actually looking to reuse RepositoryDefinition objects so I don't have to re-read the entire code every time I run a job.\nYou can find more context on what I'm trying to do in this previous <https://dagster.slack.com/archives/C01U954MEER/p1674513243459199|conversation>.", "cc <@U01J51Y6B9D> since you have the context from previous discussion", "hi <@U033GD09BJ5> the quick answer is that no there is currently no way to cache anything about the repository other than the asset definitions. CacheableAssetsDefinitions can produce an arbitrary set of assets from their cacheable data, so you could get tricky with things if you wanted (i.e. bundle up the whole API response into a single cached AssetsDefinitionCacheableData for the repository, then unpack that object into many individual asset definitions), but I'm not sure how much this would help performance.\n\nI think the most promising route here would be to separate each of your repositories into a different <https://docs.dagster.io/concepts/code-locations#code-locations|code location>"], "is_dagster_empl": [true, false, true, true], "gpt4_replies_target": "Currently, there is no way to cache anything about the repository other than the asset definitions in Dagster. However, you could potentially improve performance by separating each of your repositories into a different code location.", "dagster_empl_first_target": "Hi Jordan, Could you elaborate a bit on what you mean by \u201crebuild repository\u201d? A repository in Dagster is a python abstraction that holds a bunch of definitions.", "dagster_empl_last_target": "hi <@U033GD09BJ5> the quick answer is that no there is currently no way to cache anything about the repository other than the asset definitions. CacheableAssetsDefinitions can produce an arbitrary set of assets from their cacheable data, so you could get tricky with things if you wanted (i.e. bundle up the whole API response into a single cached AssetsDefinitionCacheableData for the repository, then unpack that object into many individual asset definitions), but I'm not sure how much this would help performance.\n\nI think the most promising route here would be to separate each of your repositories into a different <https://docs.dagster.io/concepts/code-locations#code-locations|code location>"}, {"question": "Hi folks, I'm brand new and having trouble getting grpcio installed at the required version for a local venv install on my PC. First it fails to build the wheel, then it tries to install using setup.py and both times I get a 3000+ line wall of text from VC++ compiler. I can \"pip install grpcio\" and I get version 1.51.1 in no-time-flat but the dagster/dagit requirements want version 1.47 and will uninstall the newer version. Win10 22H2, Python 3.11.1, pip 22.3.1. What am I missing? TIA", "replies": ["It\u2019s a build problem with grpcio and not a dagster issue. Use python 3.7 and it will build without issue. ", "It's def a build problem. :smile: I'll try 3.7, thanks."], "is_dagster_empl": [false, false], "gpt4_replies_target": "The issue you're experiencing with grpcio is a build problem, not a dagster issue; try using Python 3.7 to build without issue.", "dagster_empl_first_target": "It\u2019s a build problem with grpcio and not a dagster issue. Use python 3.7 and it will build without issue. ", "dagster_empl_last_target": "It's def a build problem. :smile: I'll try 3.7, thanks."}, {"question": "I have a resource defined via `make_values_resource` which holds some values shared among multiple assets in a job. Is there a way to run validation on these values (beyond just type checking) when a job is initiated so that the job will not start executing unless validations pass? For example,\n```\"some_resource\": make_values_resource(\n    some_value=int,\n)```\ncould I setup a validation that asserts `some_value` is an odd number?", "replies": ["Hi Tom, there is no hook in `make_values_resource` for arbitrary validation logic. If you want to do that, you could define a custom resource that is similar to `make_values_resource`:\n\n```@resource(config_schema=&lt;config_schema&gt;)\ndef my_values_resource(context):\n    value = context.resource_config\n    # \u2026 arbitrary validation on `value`\n    return value```"], "is_dagster_empl": [true], "gpt4_replies_target": "There is no built-in validation hook in `make_values_resource`, but you can create a custom resource with your own validation logic by defining a resource with `@resource(config_schema=<config_schema>)`.", "dagster_empl_first_target": "Hi Tom, there is no hook in `make_values_resource` for arbitrary validation logic. If you want to do that, you could define a custom resource that is similar to `make_values_resource`:\n\n```@resource(config_schema=&lt;config_schema&gt;)\ndef my_values_resource(context):\n    value = context.resource_config\n    # \u2026 arbitrary validation on `value`\n    return value```", "dagster_empl_last_target": "Hi Tom, there is no hook in `make_values_resource` for arbitrary validation logic. If you want to do that, you could define a custom resource that is similar to `make_values_resource`:\n\n```@resource(config_schema=&lt;config_schema&gt;)\ndef my_values_resource(context):\n    value = context.resource_config\n    # \u2026 arbitrary validation on `value`\n    return value```"}, {"question": "Hi. So I need this resource in every op. How do I do this without having to specify it in every op, but in the job definition instead? I think this is possible, but I cannot find a specification in the documentation for exactly what the job config looks like. Thanks!", "replies": ["Hi Leigh, thanks for the question-- relevant docs here: <https://docs.dagster.io/concepts/resources#using-resources-with-ops>\n\nYou\u2019ll need to specify the resource key for each op, but the resource definition need only be passed to the job:\n\n```@op(required_resource_keys={\"foo_resource\"})\ndef foo_op(context):\n    return context.resources.foo_resource\n\n@op(required_resource_keys={\"foo_resource\"})\ndef bar_op(context, foo):\n    return foo + context.resources.foo_resource\n\n@job(resource_defs={\"foo_resource\": foo_resource})\ndef foo_graph():\n    bar_op(foo_op())```", "Yep, I read that. Thanks. I think its a bit unwieldy for a job with lots of ops, but okay. Also, where is the precise description of the job decorator configuration (the schema). I\u2019ve resorted to passing in what I know is invalid so I can see it in the error handler. :slightly_smiling_face:", "Do you mean the `@job` API doc? <https://docs.dagster.io/_apidocs/jobs#dagster.job>", "Yes, notice the \u201cconfig\u201d has no definition of the schema.", "Ah I see what you mean-- that\u2019s an oversight, we have this elsewhere in our docs, or it at least used to render. I\u2019ve created an issue and will investigate. Thanks for the feedback!\n\n<https://github.com/dagster-io/dagster/issues/12316>"], "is_dagster_empl": [true, false, true, false, true], "gpt4_replies_target": "To use a resource in every op without specifying it in each op, you need to specify the resource key for each op and pass the resource definition to the job. However, the exact schema for the job decorator configuration is currently not defined in the documentation.", "dagster_empl_first_target": "Hi Leigh, thanks for the question-- relevant docs here: <https://docs.dagster.io/concepts/resources#using-resources-with-ops>\n\nYou\u2019ll need to specify the resource key for each op, but the resource definition need only be passed to the job:\n\n```@op(required_resource_keys={\"foo_resource\"})\ndef foo_op(context):\n    return context.resources.foo_resource\n\n@op(required_resource_keys={\"foo_resource\"})\ndef bar_op(context, foo):\n    return foo + context.resources.foo_resource\n\n@job(resource_defs={\"foo_resource\": foo_resource})\ndef foo_graph():\n    bar_op(foo_op())```", "dagster_empl_last_target": "Ah I see what you mean-- that\u2019s an oversight, we have this elsewhere in our docs, or it at least used to render. I\u2019ve created an issue and will investigate. Thanks for the feedback!\n\n<https://github.com/dagster-io/dagster/issues/12316>"}, {"question": "really enjoyed the migration webinar today - I\u2019m trying to use the `make_dagster_repo_from_airflow_dags_path` and it works great until a certain task that uses snowflake - `TypeError: run() got an unexpected keyword argument 'handler'` is the error that\u2019s returned\u2026but if I re-execute that step, the task runs succesfully. Any insights?", "replies": ["Closing since this was apparently resolved in another channel."], "is_dagster_empl": [true], "gpt4_replies_target": "The issue you encountered with the 'handler' argument in the `make_dagster_repo_from_airflow_dags_path` function was apparently resolved in another channel.", "dagster_empl_first_target": "Closing since this was apparently resolved in another channel.", "dagster_empl_last_target": "Closing since this was apparently resolved in another channel."}, {"question": "Quickly checking, is there anything that supports retaining a history of asset materializations? Our use case: we have an external source we are querying that we want to keep entire snapshots of over time.", "replies": ["If we implement this ourselves, are there recommendations? Dynamic `AssetKey` with the date?", "A partitioned asset may be what you\u2019re looking for. <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions|https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions>\n\nIf the snapshots occur on a regular time interval, those could be your partitions", "We are already using partitioned assets but not sure it\u2019s appropriate in this case. The upstream resource changes day to day and we want to snapshot moments in time. If we  set it up as partitions, it would break if we ever reran a partition by accident (since it will be replaced with the current days info)", "you could add a check at the start of your asset/op that the partition key is valid to prevent that", "So basically if partition key != today then NOOP", "I don't have a better answer than <@U03Q27RQY76>, but I filed an issue to track supporting this in a more first-class way: <https://github.com/dagster-io/dagster/issues/12253>", "Thank you :+1::skin-tone-3: "], "is_dagster_empl": [false, false, false, false, false, true, false], "gpt4_replies_target": "Dagster currently does not have a built-in feature to retain a history of asset materializations, but you can implement this by using a dynamic `AssetKey` with the date or a partitioned asset if the snapshots occur on a regular time interval. However, be careful with reruns as it could replace the current data. An issue has been filed to support this feature in a more first-class way.", "dagster_empl_first_target": "I don't have a better answer than <@U03Q27RQY76>, but I filed an issue to track supporting this in a more first-class way: <https://github.com/dagster-io/dagster/issues/12253>", "dagster_empl_last_target": "I don't have a better answer than <@U03Q27RQY76>, but I filed an issue to track supporting this in a more first-class way: <https://github.com/dagster-io/dagster/issues/12253>"}, {"question": "I have a similar question. I would like my asset to append to an existing snowflake table (with a timestamp column) at the end of materialization instead of replacing an existing snowflake table. Is there a place in the examples or documentation that shows how to do this?", "replies": ["I don't think dagster-snowflake io manager supports appending. However, you can do the write to snowflake in your asset op and then log the asset materialization directly. In other words, use any python tooling you like to write the data however you like into the database. Then, tell dagster what you did using the asset materialization.", "<@U031ST9MT6F> How do you handle telling dagster what you did without it materializing the asset itself?", "Your op code materializes the asset (by appending to the database). The way you tell Dagster that you did that is by adding an AssetMaterialization to the op. That is the part that tells Dagster that the asset has actually been materialized.", "<https://docs.dagster.io/concepts/assets/asset-materializations>"], "is_dagster_empl": [false, false, false, false], "gpt4_replies_target": "While the dagster-snowflake io manager doesn't support appending, you can write to the snowflake in your asset operation and then log the asset materialization directly using any python tooling. You can inform Dagster about this by adding an AssetMaterialization to the operation.", "dagster_empl_first_target": "I don't think dagster-snowflake io manager supports appending. However, you can do the write to snowflake in your asset op and then log the asset materialization directly. In other words, use any python tooling you like to write the data however you like into the database. Then, tell dagster what you did using the asset materialization.", "dagster_empl_last_target": "<https://docs.dagster.io/concepts/assets/asset-materializations>"}, {"question": "Newbie question again, I (now, thanks <@U04M8B4R7NU>) have Dagster and Dagit installed in a Python 3.7.9 venv on my local computer. I would like to use the example project project_fully_featured to explore with. I downloaded the project according to the docs and ran \"pip install -e .\" in the created my_project folder. I saw a handful of dependency conflict errors as pip was doing its work. If I try to start dagster, I get an error `UserWarning: Error loading repository location project_fully_featured:ValueError: None is not a valid IndirectSelection` I'm not great at reading stack traces but it seems to be failing to load assets. Can somebody point me in the right direction to iron this out?", "replies": ["hi <@U04N8F9KPK9>! I think this is stemming from a compatibility issue between dagster and dbt version 1.4 -- a fix is going out tomorrow, but I believe if you `pip install \"dbt-core&lt;1.4\"` then you should be good until then", "Well, that gave me the confidence to go install a variety of things at specific versions until pip said \"no broken dependencies\". :slightly_smiling_face: DBT was definitely one of the things I downgraded, and now the project starts at least. Thanks for the tip!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The error you're experiencing is likely due to a compatibility issue between Dagster and dbt version 1.4. A fix is expected soon, but for now, you can resolve the issue by installing dbt-core version less than 1.4 using the command `pip install \"dbt-core<1.4\"`.", "dagster_empl_first_target": "hi <@U04N8F9KPK9>! I think this is stemming from a compatibility issue between dagster and dbt version 1.4 -- a fix is going out tomorrow, but I believe if you `pip install \"dbt-core&lt;1.4\"` then you should be good until then", "dagster_empl_last_target": "hi <@U04N8F9KPK9>! I think this is stemming from a compatibility issue between dagster and dbt version 1.4 -- a fix is going out tomorrow, but I believe if you `pip install \"dbt-core&lt;1.4\"` then you should be good until then"}, {"question": "Hi, is there any documentation or working example regarding the `--container-context` option for `dagster api grpc`? I'm looking for a way to configure the environment of the container launched by the `DockerRunLauncher`. Using `env_vars` within the launcher section of the `dagster.yaml` file is not a viable option for me because I'd like to set different variables for different grpc servers.", "replies": ["Hi Marco - it's a JSON string that can look like this to set env vars in Docker for example:\n\n```'{\"docker\": {\"env_vars\": [\"FOO_ENV_VAR=bar_value\", \"BAZ_ENV_VAR\"] } }'```\nIt's not very well documented currently, we're hoping to move to a place where you set that config in a place like workspace.yaml instead of needing to pass around JSON strings - but it should work for the forseeable future"], "is_dagster_empl": [true], "gpt4_replies_target": "The `--container-context` option for `dagster api grpc` can be configured using a JSON string like this: `{\"docker\": {\"env_vars\": [\"FOO_ENV_VAR=bar_value\", \"BAZ_ENV_VAR\"] } }`, which sets environment variables in Docker.", "dagster_empl_first_target": "Hi Marco - it's a JSON string that can look like this to set env vars in Docker for example:\n\n```'{\"docker\": {\"env_vars\": [\"FOO_ENV_VAR=bar_value\", \"BAZ_ENV_VAR\"] } }'```\nIt's not very well documented currently, we're hoping to move to a place where you set that config in a place like workspace.yaml instead of needing to pass around JSON strings - but it should work for the forseeable future", "dagster_empl_last_target": "Hi Marco - it's a JSON string that can look like this to set env vars in Docker for example:\n\n```'{\"docker\": {\"env_vars\": [\"FOO_ENV_VAR=bar_value\", \"BAZ_ENV_VAR\"] } }'```\nIt's not very well documented currently, we're hoping to move to a place where you set that config in a place like workspace.yaml instead of needing to pass around JSON strings - but it should work for the forseeable future"}, {"question": "Hi Everyone, I am using the Dagster with Azure Blob storage but unable to save data in containers, can someone help me with the utility to use it with some lines of sample codes.\n\u2022 The file we save is PDF from internet.\n\u2022 If possible and available, where can I find the existing thread in community regarding for any such help.\nThanks", "replies": ["You might find this example helpful:\n```from argus.dagster.adls2 import adls2_resource\nfrom azure.storage.blob import BlobServiceClient\nfrom dagster import Definitions, OpExecutionContext, asset\n\n\n@asset(required_resource_keys={\"adls2\"})\ndef example(context: OpExecutionContext):\n    client: BlobServiceClient = context.resources.adls2.blob_client\n    container = client.create_container(\"test\")\n    container.upload_blob(\"myfile.pdf\", data=...)\n\n\ndefs = Definitions(assets=[example], resources={\"adls2\": adls2_resource.configured({...})})```\n"], "is_dagster_empl": [false], "gpt4_replies_target": "You can use the Azure Blob Storage with Dagster by using the BlobServiceClient to create a container and upload your PDF file, as shown in the provided code snippet. Unfortunately, I couldn't find a specific thread in the community regarding this, but you can always start a new one if you need more help.", "dagster_empl_first_target": "You might find this example helpful:\n```from argus.dagster.adls2 import adls2_resource\nfrom azure.storage.blob import BlobServiceClient\nfrom dagster import Definitions, OpExecutionContext, asset\n\n\n@asset(required_resource_keys={\"adls2\"})\ndef example(context: OpExecutionContext):\n    client: BlobServiceClient = context.resources.adls2.blob_client\n    container = client.create_container(\"test\")\n    container.upload_blob(\"myfile.pdf\", data=...)\n\n\ndefs = Definitions(assets=[example], resources={\"adls2\": adls2_resource.configured({...})})```\n", "dagster_empl_last_target": "You might find this example helpful:\n```from argus.dagster.adls2 import adls2_resource\nfrom azure.storage.blob import BlobServiceClient\nfrom dagster import Definitions, OpExecutionContext, asset\n\n\n@asset(required_resource_keys={\"adls2\"})\ndef example(context: OpExecutionContext):\n    client: BlobServiceClient = context.resources.adls2.blob_client\n    container = client.create_container(\"test\")\n    container.upload_blob(\"myfile.pdf\", data=...)\n\n\ndefs = Definitions(assets=[example], resources={\"adls2\": adls2_resource.configured({...})})```\n"}, {"question": "Hi,\nI am running dagster locally with `dagster dev`\nI am using `docker_container_op` to start docker container with dbt project\nI see this in terminal where I started `dagster dev`\n```b'\\x1b[0m14:36:02  Running with dbt=1.5.0-b1\\n'\nb'\\x1b[0m14:36:02  Unable to do partial parsing because of a version mismatch\\n'\nb'\\x1b[0m14:36:03  Found 16 models, 0 tests, 0 snapshots, 0 analyses, 425 macros, 0 operations, 0 seed files, 9 sources, 0 exposures, 0 metrics, 0 groups\\n'\nb'\\x1b[0m14:36:03  \\n'\nb\"\\x1b[0m14:36:04  Concurrency: 4 threads (target='dev')\\n\"\nb'\\x1b[0m14:36:04  \\n'\nb'\\x1b[0m14:36:04  1 of 2 START sql table model DBT_DJ.stg_transformed__stg_model1 ........ [RUN]\\n'\nb'\\x1b[0m14:36:08  1 of 2 OK created sql table model DBT_DJ.stg_transformed__stg_model1 ... [\\x1b[32mSUCCESS 1\\x1b[0m in 3.93s]\\n'\nb'\\x1b[0m14:36:08  2 of 2 START sql table model DBT_DJ.transformed__model1 ................ [RUN]\\n'\nb'\\x1b[0m14:36:10  2 of 2 OK created sql table model DBT_DJ.transformed__model1 ........... [\\x1b[32mSUCCESS 1\\x1b[0m in 1.78s]\\n'\nb'\\x1b[0m14:36:10  \\n'\nb'\\x1b[0m14:36:10  Finished running 2 table models in 0 hours 0 minutes and 7.32 seconds (7.32s).\\n'\nb'\\x1b[0m14:36:10  \\n'\nb'\\x1b[0m14:36:10  \\x1b[32mCompleted successfully\\x1b[0m\\n'\nb'\\x1b[0m14:36:10  \\n'\nb'\\x1b[0m14:36:10  Done. PASS=2 WARN=0 ERROR=0 SKIP=0 TOTAL=2\\n' ```\nBut I do not see this in `dagit`\nWhat and how should I configure to see docker container output in `dagit` ?", "replies": ["Think I found it\nthe `Raw conpute logs` button :joy:"], "is_dagster_empl": [false], "gpt4_replies_target": "To see the docker container output in `dagit`, you can click on the `Raw Compute Logs` button.", "dagster_empl_first_target": "Think I found it\nthe `Raw conpute logs` button :joy:", "dagster_empl_last_target": "Think I found it\nthe `Raw conpute logs` button :joy:"}, {"question": "Hi,\nI have two docker jobs like this:\n```@job\ndef dbt_transformed():\n    docker_config = {\n        \"image\": \"dbt-image\",\n        \"command\": [\n            \"build\",\n            \"--select\",\n            \"tag:transformed\",\n        ]\n    }\n    docker_container_op.configured(\n        docker_config,\n        name=\"dbt_transformed_op\",\n    )()\n\n\n@job\ndef dbt_transformed_other():\n    docker_config = {\n        \"image\": \"dbt-image\",\n        \"command\": [\n            \"build\",\n            \"--select\",\n            \"tag:transformed_other\",\n        ]\n    }\n    docker_container_op.configured(\n        docker_config,\n        name=\"dbt_transformed_other_op\",\n    )()```\nThey work, but the code is almost the same\u2026\nIs there any way to do something like this?:\n```job_config = {\n   \"dbt_transformed\": \"tag:transformed\", \n   \"dbt_transformed_other\": \"tag:transformed_other\"\n}\n\njobs = []\n\nfor job_name, select in job_config.items():\n    docker_config = {\n        \"image\": \"dbt-image\",\n        \"command\": [\n            \"build\",\n            \"--select\",\n            select,\n        ]\n    }\n    op = docker_container_op.configured(\n        docker_config,\n        name=f\"{job_name}_op\",\n    )\n    job = define_job(name, op) # function that I miss\n    jobs.append(job)```\nI can not find a function like `define_job` to accomplish what I need\nAny way to define jobs not with a decorator but with a function passing a name and an op ?", "replies": ["You should be able to just define a simple factory function like this:\n\n```job_config = {\n   \"dbt_transformed\": \"tag:transformed\", \n   \"dbt_transformed_other\": \"tag:transformed_other\"\n}\n\njobs = []\n\ndef define_job(job_name, select):\n    @job(name=job_name)\n    def job_template():\n        docker_config = {\n        \"image\": \"dbt-image\",\n        \"command\": [\n            \"build\",\n            \"--select\",\n            select,\n        ]\n        }\n        op = docker_container_op.configured(\n            docker_config,\n            name=f\"{job_name}_op\",\n        )\n    return job_template\n\nfor job_name, select in job_config.items():\n    job = define_job(name, select)\n    jobs.append(job)```", "Thanks! That did the trick!"], "is_dagster_empl": [false, false], "gpt4_replies_target": "You can define a factory function to create jobs dynamically in Dagster, by using the @job decorator inside the function and passing the job name as an argument.", "dagster_empl_first_target": "You should be able to just define a simple factory function like this:\n\n```job_config = {\n   \"dbt_transformed\": \"tag:transformed\", \n   \"dbt_transformed_other\": \"tag:transformed_other\"\n}\n\njobs = []\n\ndef define_job(job_name, select):\n    @job(name=job_name)\n    def job_template():\n        docker_config = {\n        \"image\": \"dbt-image\",\n        \"command\": [\n            \"build\",\n            \"--select\",\n            select,\n        ]\n        }\n        op = docker_container_op.configured(\n            docker_config,\n            name=f\"{job_name}_op\",\n        )\n    return job_template\n\nfor job_name, select in job_config.items():\n    job = define_job(name, select)\n    jobs.append(job)```", "dagster_empl_last_target": "Thanks! That did the trick!"}, {"question": "I'm using Dagster with DBT and i was wondering how can i run a custom command , i'm talking about this : <https://docs.dagster.io/_apidocs/libraries/dagster-dbt#resources-dbt-core>\nlet's say i want to run the command `build` from dbt using the dbt cli ressource .\ni understood that if i want to do an `@op` i can pass a `context` parameter to use it in the function .\ni tried this but it doesn't work\n```resources = {\n    \"dbt\": dbt_cli_resource.configured(\n        {\n            \"project_dir\": DBT_PROJECT_PATH,\n            \"profiles_dir\": DBT_PROFILES,\n        },\n    )\n}\n\n\n@op(required_resource_keys={\"dbt\"})\ndef custom_run(context):\n    context.build(models=[\"mymodel\"])```\nThe end goal is to do a custom `dbt build --select mymodel+`", "replies": ["Hi! The <https://docs.dagster.io/concepts/ops-jobs-graphs/ops#op-context|op context object> provides access to all of your op config and resources so you need to specify that you want to access the dbt resource. Assuming the rest of the code is correct (I don\u2019t know enough to verify your kwargs in the build call) the correct way to call is:\n\n```@op(required_resource_keys={\"dbt\"})\ndef custom_run(context):\n    context.resources.dbt.build(models=[\"mymodel\"])```", "Indeed i was missing this resource_key . thank you very much.", "No problem! Always happy to help :daggy-love:"], "is_dagster_empl": [false, false, false], "gpt4_replies_target": "To run a custom command in Dagster with DBT, you need to specify that you want to access the dbt resource in your op context. You can do this by calling `context.resources.dbt.build(models=[\"mymodel\"])` in your custom_run function.", "dagster_empl_first_target": "Hi! The <https://docs.dagster.io/concepts/ops-jobs-graphs/ops#op-context|op context object> provides access to all of your op config and resources so you need to specify that you want to access the dbt resource. Assuming the rest of the code is correct (I don\u2019t know enough to verify your kwargs in the build call) the correct way to call is:\n\n```@op(required_resource_keys={\"dbt\"})\ndef custom_run(context):\n    context.resources.dbt.build(models=[\"mymodel\"])```", "dagster_empl_last_target": "No problem! Always happy to help :daggy-love:"}, {"question": "Hi All, I started using Dagster, and a scenario came up that I wasn't able to get working; documentation is also absent on that.\n\nWe have a schedule that runs periodically. One of the downstream jobs, JobA, from the schedule has an output. We want to persist this output to BigQuery, after some transformations, and thought using an Asset to encapsulate the table made sense. We thought we should be able to connect the output of JobA to the input of the Asset (JobA -&gt; Asset), but it appears the documentation has nothing on this. Hacking at it also hasn't worked so far. Is this a supported case?", "replies": ["I think one way to model this would be to turn JobA into a <https://docs.dagster.io/concepts/assets/graph-backed-assets|graph-backed asset>, then have your BigQuery asset define the transformations that have to be applied to JobA's output. I don't think assets can depend on jobs directly."], "is_dagster_empl": [false], "gpt4_replies_target": "In Dagster, you can model this scenario by transforming JobA into a graph-backed asset, then define the necessary transformations to JobA's output in your BigQuery asset, as assets cannot directly depend on jobs.", "dagster_empl_first_target": "I think one way to model this would be to turn JobA into a <https://docs.dagster.io/concepts/assets/graph-backed-assets|graph-backed asset>, then have your BigQuery asset define the transformations that have to be applied to JobA's output. I don't think assets can depend on jobs directly.", "dagster_empl_last_target": "I think one way to model this would be to turn JobA into a <https://docs.dagster.io/concepts/assets/graph-backed-assets|graph-backed asset>, then have your BigQuery asset define the transformations that have to be applied to JobA's output. I don't think assets can depend on jobs directly."}, {"question": "Hey, This seems like a basic question, but I can\u2019t find the info anywhere :disappointed:.\nI\u2019ve seen a lot of docs for `assets` or for `ops` but none with both of them together.\n\nIf I\u2019m building my DAG using `software defined assets` and I don\u2019t want to save the result of some of the `assets`, my thoughts would be to convert the to an `Op` so no `io_manager` would be configured and that would result on not having this `Op` getting save to disk.\n\nThe problem I have is I\u2019m not able to convert `assets` to `op` the same way I\u2019m linking my `assets`.\n\nWhat\u2019s the `Dagster Way` of doing this?\n\nShould I just create basic functions for thoses calls?\n\nIf you take the example below.\n```@asset(required_resource_keys={\"snowflake\"})\ndef get_data_from_snowflake(context):\n    query = \"select * from my_table\"\n\n    df = context.resources.snowflake.execute_query(query, use_pandas_result=True)\n\n    return df.unique().tolist()\n\n\n@asset\ndef get_all_data(context, get_data_from_snowflake):\n    client = MyClient()\n\n    resp = client.get_data(get_data_from_snowflake)\n\n    return resp```\nHow can I change the function `get_data_from_snowflake` so it\u2019s not getting save to disk, but while still keeping the option to use that as the input of `get_all_data`?", "replies": ["Hey there! It sounds like you have a pipeline of assets, and one of those assets doesn't actually want to persist its result to its I/O manager?\n\nI'll start by providing a solution to problem you outline in your code, and let me know if you'd like to answers to any of the tangential questions you've also asked.\n\nThe tl;dr is that this page should hopefully have the \"Dagster Way\" practices that you seek <https://docs.dagster.io/tutorial/assets/non-argument-deps>\n\nIn your `get_data_from_snowflake`, I'd recommend modifying your SQL query to do all that compute and storage in Snowflake, ex.\n\n```create or replace table unique_records as (\n   select distinct * from my_table\n)```\nside-note: I would suggest de-duping with a `row_number`+`qualify` on a specific primary key to reduce those query costs :wink:\n\nThen, don't return anything in your `get_data_from_snowflake` asset. Assets don't need to return data or use I/O managers if you don't need them to.\n\nNow you can modify your `get_all_data` asset to not have the `get_data_from_snowflake` asset as a parameter, but rather add it as a `non_argument_deps` to the `@asset` decorator. This way, if you have a job of these two assets, `get_all_data` won't try materializing without `get_data_from_snowflake` materializing first. That said, if you didn't want your data from `get_all_data` saved via an I/O manager, you likely also don't want your data from get_all_data to persist too. So you'll might want to consider following the same pattern in this asset too.\n\n\n---\n\nIf you want an even more \"Dagster way\" to do this, I'd recommend using the Snowflake I/O manager and using partitions to reduce the amount of data loaded into memory during assets. You get 3 benefits this way:\n\u2022 minimized compute from Snowflake because you're only querying a partition of data at a time\n\u2022 with reasonable partitioning, you should likely be able to fit a whole partition of data into memory for compute\n\u2022 the Snowflake I/O manager will read and write data from memory into Snowflake tables for you, so none of your data ever gets stored to the disk of the compute engine", "Thank you for taking the time to explain all of this.\n\nMay I ask other questions?\n\nSo I think I wasn\u2019t clear enough in what I want to achieve. Sorry about that.\n\nThe `non_argument_deps` makes Dagster wait for the completion of the parent asset before executing any downstream assets. That\u2019s perfect.\n\nBut, in my usecase my `get_data_from_snowflake` will be use as an input in my `get_all_data`.\nLet\u2019s take an example: I query Snowflake to get a list of account in `get_data_from_snowflake` , and I\u2019ll query this list on another service with `get_all_data`.\n\nIf I set up a `non_argument_deps` I won\u2019t have the list in accounts in `get_all_data` . And I don\u2019t want to save them with an IO/Manager, because they are already in Snowflake and this would create duplicates of the same info for nothing.\n\nI know that I could use Dagster Partitions, but my keys are `time` and `static_values` and since the multi_partition_key feature is still experimental, I would prefer not to use it.\n\nBasically, I would like to create Python functions that have preconfigured Snowflake query, so all my pipeline can use them. And since those records are already coming from my Data Warehouse I would like to only load them into memory and not persist them anywhere.\n\nFrom what I understand. (And here is where I\u2019m probably wrong)\n\u2022 If I use `non_argument_deps` in a downstream asset. The Parent will still be materialized. (In our example `get_data_from_snowflake`\n\u2022 If I don\u2019t return any value, I won\u2019t have the required data to execute the downstream asset `get_all_data`\n\u2022 If I return a value,  it will get persist somewhere. If I configure an IO/Manager using _`io_manager_key`,_ this IO Manager will be choosen. If I don\u2019t configure any, the default one will be selected.\n\u2022 Is there something like `io_manager_key=store_data_in_memory` , that will store the data for the current run, and drop it when the dagster container for the run finish?\nTLDR; I would like to create Python functions that have preconfigured Snowflake query that are not persist on disk, but can be used as input in other assets,", "Ok so it appears that there\u2019s 2 built in io_manager for this `fs_io_manager` and `mem_io_manager`\n\nI think this is what I\u2019m looking for.\n\nThe `mem_io_manager` seems to be erroring if I use it with my setup as shown in the image linked to this thread.\n\nIs it not possible to use `mem_io_manager` with another persistent io_manager?\nSorry for all of those questions ahaha.\n\nThanks for your help.", "Ok update on this.\nTurns out when you create a ressource called `io_manager` this override the default io_manager `fs_io_manager`.\n\nThat\u2019s why nothing was getting saved to disk by default when I was not configuring any io_managers in my asset decorator.\n\nSo with that sort out.\n\nThe only question left is, is it possible to convert `fs_io_manager` to `mem_io_manager` or I should keep `fs_io_manager` as it\u2019s the `dagster` way?", "Using `mem_io_manager` is very reasonable if you're using the in_process_executor. If you're using an executor that involves multiple processes, then you need an IO manager that can put data in a place that other processes can get at it", "Ok so, hopefully, this would be my last message on this topic.\nReally sorry to bother you both again.\n\nSo if my goal is to create a function that contains an SQL query to be able to use it with different assets with some minor filtering.\nWhich option should I take?\nI\u2019m really trying to find out a \u201cgood\u201d solution, but none of the option I\u2019ve found seems to be optimal.\n\nHere are the option that I could take... But all seems terrible.\n\nOption 1 (Create a Snowflake class and function and pass a variable directly)\nThis seems like the best option of all, but I\u2019m sad that I can\u2019t use the built in function of Dagster to provide the ressource to Snowflake the function.\n```class SnowflakeClient:\n    def __init__(self):\n        self.client = snowflake.connector.connect(\n            account=os.getenv(\"SNOWFLAKE_ACCOUNT\", \"\"),\n            user=os.getenv(\"SNOWFLAKE_USER\", \"\"),\n            password=os.getenv(\"SNOWFLAKE_PASSWORD\", \"\"),\n            warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"\"),\n        )\n\n    def get_delegator_accounts(self, ticker):\n        query = f\"select distinct operator_address from mt_table where ticker = '{ticker}'\"\n        df = self.client.cursor().execute(query).fetch_pandas_all()\n\n        return df[\"OPERATOR_ADDRESS\"].tolist()\n\n@asset\ndef get_all_grt_rewards(context) -&gt; dict:\n    grt_client = GRTClient()\n    snowflake_client = SnowflakeClient()\n    result = snowflake_client.get_delegator_accounts(\"GRT\")\n\n    resp = grt_client.get_rewards_for_multiple_addresses(result)\n\n    return resp```\nOption 2 (Create an asset for the query)\nThis is good, but the problem is that I can\u2019t add any parameter to the function.\n```@asset(\n    required_resource_keys={\"snowflake\"},\n)\ndef get_delegator_accounts_2(context):\n    query = \"select distinct operator_address from my_table where ticker = 'GRT'\"\n\n    df_accounts = context.resources.snowflake.execute_query(query, use_pandas_result=True)\n    \n    return df_accounts[\"OPERATOR_ADDRESS\"].tolist()\n\n@asset\ndef get_all_grt_rewards(context, get_delegator_accounts_2) -&gt; dict:\n    grt_client = GRTClient()\n    \n    resp = grt_client.get_rewards_for_multiple_addresses(get_delegator_accounts_2)\n\n    return resp```\nOption 3 (Not working, since `ticker` can\u2019t be provided)\n```@asset(\n    required_resource_keys={\"snowflake\"},\n)\ndef get_delegator_accounts_3(context, ticker):\n    query = f\"select distinct operator_address from my_table where ticker = '{ticker}'\"\n\n    df_accounts = context.resources.snowflake.execute_query(query, use_pandas_result=True)\n    \n    return df_accounts[\"OPERATOR_ADDRESS\"].tolist()```\n\nOption 4 (This works, but I all partition would be run for all of my downstream assets, and the filter should filter out which asset receive the results)\n```@asset(\n    partitions_def=StaticPartitionsDefinition([\"GRT\", \"OTHER\"]),\n    required_resource_keys={\"snowflake\"},\n)\ndef get_delegator_accounts_3(context):\n    partition = context.asset_partition_key_for_output()\n    query = f\"select distinct operator_address from my_table where ticker = '{partition}'\"\n\n    df_accounts = context.resources.snowflake.execute_query(query, use_pandas_result=True)\n    \n    return df_accounts[\"OPERATOR_ADDRESS\"].tolist()```\n\nOption 5 (This works, but the configuration seems overly complicated to be able to route the good partition to the good downstream asset)\n```@multi_asset(\n    partitions_def=StaticPartitionsDefinition([\"GRT\", \"OTHER\"]),\n    required_resource_keys={\"snowflake\"},\n    outs={\n        \"GRT\": AssetOut(\n            metadata={\n                \"priority\": \"high\",\n            },\n            io_manager_key=\"io_manager\",\n            key=AssetKey(\"get_all_grt_rewards\"),\n        ),\n        \"OTHER\": AssetOut(\n            is_required=False,\n            metadata={\n                \"priority\": \"low\",\n            },\n            io_manager_key=\"io_manager\",\n            key=AssetKey(\"other_assets\"),\n        ),\n    },\n    can_subset=False,\n)\ndef get_delegator_accounts_3(context):\n    partition = context.asset_partition_key_for_output()\n    query = f\"select distinct operator_address from my_table where ticker = '{partition}'\"\n\n    df_accounts = context.resources.snowflake.execute_query(query, use_pandas_result=True)\n    \n    yield Output(value=df_accounts[\"OPERATOR_ADDRESS\"].tolist(), output_name=partition) #Partition here could be the name of the configured OUT```\nI\u2019m sure I\u2019m missing a basic concept so this is resolved, but I can\u2019t find which one :disappointed:", "is there a reason you can't do\n\n```class SnowflakeClient:\n    def __init__(self):\n        self.client = snowflake.connector.connect(\n            account=os.getenv(\"SNOWFLAKE_ACCOUNT\", \"\"),\n            user=os.getenv(\"SNOWFLAKE_USER\", \"\"),\n            password=os.getenv(\"SNOWFLAKE_PASSWORD\", \"\"),\n            warehouse=os.getenv(\"SNOWFLAKE_WAREHOUSE\", \"\"),\n        )\n\n    def get_delegator_accounts(self, ticker):\n        query = f\"select distinct operator_address from mt_table where ticker = '{ticker}'\"\n        df = self.client.cursor().execute(query).fetch_pandas_all()\n\n        return df[\"OPERATOR_ADDRESS\"].tolist()\n\n@resource\ndef snowflake_client():\n    return SnowflakeClient()\n\n@resourc\ndef grt_client():\n    return GRTClient()\n\n@asset\ndef get_all_grt_rewards(context) -&gt; dict:\n    grt_client = context.resources.grt_client\n    snowflake_client = context.resources.snowflake_client\n    result = snowflake_client.get_delegator_accounts(\"GRT\")\n\n    resp = grt_client.get_rewards_for_multiple_addresses(result)\n\n    return resp```\nthat seems the cleanest to me. Does that address your comment \"...but I\u2019m sad that I can\u2019t use the built in function of Dagster to provide the ressource to Snowflake the function\"", "Oh right, I think this would work yeah.\n\nThat looks great.\n\nYahoo. :slightly_smiling_face:\n\nThank you <@U0300665S8G>"], "is_dagster_empl": [true, false, false, false, true, false, false, false], "gpt4_replies_target": "To avoid saving the result of some assets in Dagster, you can modify your SQL query to do all the compute and storage in Snowflake and not return anything in your asset. You can also use the `non_argument_deps` to ensure that `get_all_data` won't try materializing without `get_data_from_snowflake` materializing first. If you want to store data only for the current run and drop it when the Dagster container for the run finishes, you can use `mem_io_manager`.", "dagster_empl_first_target": "Hey there! It sounds like you have a pipeline of assets, and one of those assets doesn't actually want to persist its result to its I/O manager?\n\nI'll start by providing a solution to problem you outline in your code, and let me know if you'd like to answers to any of the tangential questions you've also asked.\n\nThe tl;dr is that this page should hopefully have the \"Dagster Way\" practices that you seek <https://docs.dagster.io/tutorial/assets/non-argument-deps>\n\nIn your `get_data_from_snowflake`, I'd recommend modifying your SQL query to do all that compute and storage in Snowflake, ex.\n\n```create or replace table unique_records as (\n   select distinct * from my_table\n)```\nside-note: I would suggest de-duping with a `row_number`+`qualify` on a specific primary key to reduce those query costs :wink:\n\nThen, don't return anything in your `get_data_from_snowflake` asset. Assets don't need to return data or use I/O managers if you don't need them to.\n\nNow you can modify your `get_all_data` asset to not have the `get_data_from_snowflake` asset as a parameter, but rather add it as a `non_argument_deps` to the `@asset` decorator. This way, if you have a job of these two assets, `get_all_data` won't try materializing without `get_data_from_snowflake` materializing first. That said, if you didn't want your data from `get_all_data` saved via an I/O manager, you likely also don't want your data from get_all_data to persist too. So you'll might want to consider following the same pattern in this asset too.\n\n\n---\n\nIf you want an even more \"Dagster way\" to do this, I'd recommend using the Snowflake I/O manager and using partitions to reduce the amount of data loaded into memory during assets. You get 3 benefits this way:\n\u2022 minimized compute from Snowflake because you're only querying a partition of data at a time\n\u2022 with reasonable partitioning, you should likely be able to fit a whole partition of data into memory for compute\n\u2022 the Snowflake I/O manager will read and write data from memory into Snowflake tables for you, so none of your data ever gets stored to the disk of the compute engine", "dagster_empl_last_target": "Using `mem_io_manager` is very reasonable if you're using the in_process_executor. If you're using an executor that involves multiple processes, then you need an IO manager that can put data in a place that other processes can get at it"}, {"question": "Is there a way to pass configuration value overrides when launching an asset job (as defined with `define_asset_job` ) in Dagit? I can do this with the launchpad tab for my non-Asset job but I don\u2019t see it for an asset job I\u2019m launching, but I might be missing it somewhere", "replies": ["Hi Spencer, if you shift-click \u201cMaterialize all\u201d when viewing your asset job that will present you with the config modal", "Wow, okay! Glad it\u2019s possible, but I wish it were more obvious.", "Thanks for the feedback-- there is a tooltip when you hover but we could definitely do better here. Eventually we want to unify the launch experience for asset and regular jobs"], "is_dagster_empl": [true, false, true], "gpt4_replies_target": "You can pass configuration value overrides when launching an asset job in Dagit by shift-clicking \u201cMaterialize all\u201d when viewing your asset job, which will present you with the config modal.", "dagster_empl_first_target": "Hi Spencer, if you shift-click \u201cMaterialize all\u201d when viewing your asset job that will present you with the config modal", "dagster_empl_last_target": "Thanks for the feedback-- there is a tooltip when you hover but we could definitely do better here. Eventually we want to unify the launch experience for asset and regular jobs"}, {"question": "Hi! I have a job with several assets, when the job runs via the Schedule I don't need any particular config (I use an empty string). Sometimes I need to run the job manually by specifying a config, the config I want to define is always the same for each asset (for example `database_1`). The solution below works fine but I am forced to write the database as many times as there are assets. Is there a more concise method to answer my need that would allow me for example to have a shared config between assets?", "replies": ["```@asset(config_schema={\"database\": str})\ndef asset_1(context):\n    pass\n\n@asset(config_schema={\"database\": str})\ndef asset_2(asset_1):\n    pass\n\n@asset(config_schema={\"database\": str})\ndef asset_3(asset_2):\n    pass\n\njob = define_asset_job(\n    \"my_job\",\n    selection=[asset_1, asset_2, asset_3],\n    config={\n        \"ops\": {\n            \"asset_1\": {\"config\": {\"database\": \"\"}},\n            \"asset_2\": {\"config\": {\"database\": \"\"}},\n            \"asset_3\": {\"config\": {\"database\": \"\"}},\n        }\n    },\n)```", "Maybe use the new `Config` API and set `database_1` as the default?\n\n```class MyConfig(dagster.Config):\n    database: str = \"database_1\"\n\n@asset\ndef asset_1(config: MyConfig):\n    pass\n\n@asset\ndef asset_2(config: MyConfig, asset_1):\n    pass\n\n@asset\ndef asset_3(config: MyConfig, asset_2):\n    pass```", "Ah yes indeed, I had not paid attention to the new release. It's a great way to share configurations. However, I don't know if this is a bug or not but it is always the default value that is considered, the resource is not dynamic.", "I tested the code below:\n```class MyConfig(ConfigurableResource):\n    database: str\n\n    def get_database(self) -&gt; str:\n        return self.database\n\n@asset\ndef asset_1(context, my_config: MyConfig):\n    <http://context.log.info|context.log.info>(my_config.get_database())\n\ndefs = Definitions(\n    assets=[asset_1],\n    resources={\n        \"my_config\": MyConfig(database=\"database_1\"),\n    },\n)```\nwith the following configuration and I got `database_1` in the log", "Update: I used `configure_at_launch()` to solve the above problem"], "is_dagster_empl": [false, false, false, false, false], "gpt4_replies_target": "You can use the new `Config` API to set a default database, which allows you to share configurations across assets. If you need the resource to be dynamic, you can use `configure_at_launch()`.", "dagster_empl_first_target": "```@asset(config_schema={\"database\": str})\ndef asset_1(context):\n    pass\n\n@asset(config_schema={\"database\": str})\ndef asset_2(asset_1):\n    pass\n\n@asset(config_schema={\"database\": str})\ndef asset_3(asset_2):\n    pass\n\njob = define_asset_job(\n    \"my_job\",\n    selection=[asset_1, asset_2, asset_3],\n    config={\n        \"ops\": {\n            \"asset_1\": {\"config\": {\"database\": \"\"}},\n            \"asset_2\": {\"config\": {\"database\": \"\"}},\n            \"asset_3\": {\"config\": {\"database\": \"\"}},\n        }\n    },\n)```", "dagster_empl_last_target": "Update: I used `configure_at_launch()` to solve the above problem"}, {"question": "Hi guys I am trying to generate a graph asset by using 2 ops:\n1. the first op load a passed folder to Databricks\n2. the second op read the folder as CSV using spark and generate the table\nI get an error in the second op as it cannot find the passed path. I assume because the first op return a Databricks path saved using the local IO manager (local fs IO manager) and therefore that is not present in the code uploaded to Databricks (the second op). That is my interpretation of the error.\n\nPlease see the code in this thread", "replies": ["this code is in the `test_graph_asset_upload_and_run_on_databricks` folder", "and this file is located at the same level of the folder above", "Error in `dagit`:\n```dagster._core.errors.DagsterExecutionLoadInputError: Error occurred while loading input \"path\" of step \"dev_srg_test__test_table.spark_read_csv\":\n\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_plan.py\", line 269, in dagster_event_sequence_for_step\n    for step_event in check.generator(step_events):\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/execute_step.py\", line 346, in core_dagster_event_sequence_for_step\n    for event_or_input_value in step_input.source.load_input_object(step_context, input_def):\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/inputs.py\", line 520, in load_input_object\n    yield from _load_input_with_input_manager(input_manager, load_input_context)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/inputs.py\", line 835, in _load_input_with_input_manager\n    value = input_manager.load_input(context)\n  File \"/usr/lib/python3.9/contextlib.py\", line 135, in __exit__\n    self.gen.throw(type, value, traceback)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/utils.py\", line 85, in op_execution_error_boundary\n    raise error_cls(\n\nThe above exception was caused by the following exception:\nFileNotFoundError: [Errno 2] No such file or directory: '/tmp/tmp0odzuhmj/storage/f763280b-17b8-4732-9a98-173cfa2c5c64/dev_srg_cmn_test__test_table.upload_folder_to_databricks/result'\n\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/utils.py\", line 55, in op_execution_error_boundary\n    yield\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/execution/plan/inputs.py\", line 835, in _load_input_with_input_manager\n    value = input_manager.load_input(context)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/storage/upath_io_manager.py\", line 195, in load_input\n    return self._load_single_input(path, context)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/storage/upath_io_manager.py\", line 150, in _load_single_input\n    raise e\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/storage/upath_io_manager.py\", line 138, in _load_single_input\n    obj = self.load_from_path(context=context, path=path)\n  File \"/local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.9/site-packages/dagster/_core/storage/fs_io_manager.py\", line 172, in load_from_path\n    with path.open(\"rb\") as file:\n  File \"/usr/lib/python3.9/pathlib.py\", line 1242, in open\n    return io.open(self, mode, buffering, encoding, errors, newline,\n  File \"/usr/lib/python3.9/pathlib.py\", line 1110, in _opener\n    return self._accessor.open(self, flags, mode)```", "where are you running dagster/dagit? is it in dagster cloud?", "no locally", "but in prod runs in a container in AWS", "ah then you can use s3 io manager to pass data between ops\n\n<https://docs.dagster.io/deployment/guides/aws#using-s3-for-io-management>", "Thank you! :pray:"], "is_dagster_empl": [false, false, false, true, false, false, true, false], "gpt4_replies_target": "The error you're encountering is due to the local file system IO manager not being able to find the path in Databricks. A solution could be to use the S3 IO manager to pass data between operations, as suggested in the Dagster AWS deployment guide.", "dagster_empl_first_target": "where are you running dagster/dagit? is it in dagster cloud?", "dagster_empl_last_target": "ah then you can use s3 io manager to pass data between ops\n\n<https://docs.dagster.io/deployment/guides/aws#using-s3-for-io-management>"}, {"question": "In a graph, I have two ops, `load_parquet_from_s3` and `transfer_to_elastic`. The `load_parquet_from_s3` op reads a parquet file from S3 using a provided key, and `transfer_to_elastic` syncs it to ElasticSearch. I\u2019ve noticed that because I\u2019m using `dagster_aws.s3_pickle_io_manager` as my io_manager and `load_parquet_from_s3` is an op, every return value gets saved to S3 for `transfer_to_elastic` to use. However, its return value is a large JSON object. If I use `InMemoryIOManager`, it throws an error because `InMemoryIOManager` can\u2019t be used across processes.\nSo, under these circumstances, should I convert `load_parquet_from_s3` and `transfer_to_elastic` into regular Python functions and merge them into one op, or is there another solution?\n```@op(\n    required_resource_keys={\"es_client\"},\n)\ndef transfer_to_elastic(context: OpExecutionContext, data):\n    es_client = context.resources.es_client\n\n    def get_es_actions(item):\n        for item in data:\n            es_doc = Document(meta={'id': item['id']})\n            yield es_doc.to_dict(include_meta=True)\n\n    helpers.bulk(es_client, get_es_actions(data))\n\n\n@op(\n    required_resource_keys={\"s3\"},\n)\ndef load_parquet_from_s3(context: OpExecutionContext, key: str):\n    s3 = context.resources.s3\n    <http://context.log.info|context.log.info>(f\"Loading {key} from S3\")\n\n    obj = s3.get_object(Bucket='bucket', Key=key)\n    df = pd.read_parquet(BytesIO(obj[\"Body\"].read()))\n\n    return df.to_dict(orient=\"records\")\n\n\n@graph()\ndef process_load_parquet_transfer_to_elastic(key):\n    df = load_parquet_from_s3(key)\n    return transfer_to_elastic(df)```", "replies": ["I run into the same issue all the time in spark pipelines. If there isn't any need for parallelization you could try using the <https://docs.dagster.io/_apidocs/execution#dagster.in_process_executor|in_process_executor> with the InMemoryIOManager - this will keep the execution of all ops in the graph in a single process and make it easier to pass around large objects without dumping to disk. Note that you won't be able to retry from a specific step i.e. if  `transfer_to_elastic` fails you'll have to restart the whole graph, because the intermediate won't be saved anywhere.", "<@U0300665S8G> Thank you for your response.\nHowever, to my knowledge, the `execution: in_process` can only be set at the job level. It\u2019s obviously not feasible to have the entire job run in a single thread.", "The job is linear though isn't it? In that you're only executing one process at a time anyway?", "I guess it's not immediately obvious to me why it can't run in a single process. Your other option, converting the two ops into vanilla python functions and combining them into one op, is basically the same end result right?", "<@U0300665S8G> In my code, `process_load_parquet_transfer_to_elastic` is not a job but an execution function of `DynamicOut`. In my original code, it was a regular Python function defined within a job. So, I want the return value of `load_parquet_from_s3` to be passed directly as an argument to `transfer_to_elastic` without going through the distributed scheduler. Because doing so would result in storing a large intermediate state value to S3. Additionally, I want both functions to be treated as `op` so that they can be viewed and retried in the dashboard.\nPerhaps I\u2019m being overly fixated on this, and it might not be the best practice to approach it in this way.\n```@job(\n    name='sync_company_to_elastic',\n    description='Sync the parquet data on S3 to Elastic Search.',\n    tags={'kind': 'sync elastic'},\n)\ndef sync_company_job():\n    # DynamicOut\n    keys = get_company_s3_keys()\n\n    def process_load_parquet_transfer_to_elastic(key):\n        df = load_parquet_from_s3(key)\n        return transfer_to_elastic(df)\n\n    results = keys.map(process_load_parquet_transfer_to_elastic)```", "ah yeah that's quite a bit different of a graph than what you showed in your original question", "In that case yeah it seems like your options are to merge those two ops into one, or to schedule an op that cleans up the directory your s3_pickle_io_manager is writing to. I'd probably do the former, it seems pretty superfluous to have them as separate ops - I know it's nice to have the eye candy in the UI, but it's easy to get fixated on it when it sometimes doesn't bring a lot of value beyond seeing another green rectangle in the graph", "<@U0300665S8G>\nYes, I\u2019m a beginner in dagster and also new to data science. I\u2019ve only been working with dagster for three days.\nI\u2019ve been experimenting, and I\u2019ve noticed that in my example, when I define `process_load_parquet_transfer_to_elastic` as a regular Python function, the `transfer_to_elastic` task is executed only after all the `load_parquet_from_s3` tasks are completed. However, when I define `process_load_parquet_transfer_to_elastic` as a graph, the corresponding `transfer_to_elastic` task is executed right after the completion of each `load_parquet_from_s3` task. That\u2019s why I chose to define `process_load_parquet_transfer_to_elastic` as a `graph`.", "Got it, that makes sense!", "Also, you might need to define `process_load_parquet_transfer_to_elastic` as an op outside of your job. Generally speaking graph and job definitions should only define relationships between ops that are already defined", "<@U0300665S8G> I think you\u2019re right, I\u2019ve been getting too fixated on the details.", "<@U0300665S8G> Thank you for your answer.", "I've been there before, it's so much fun to see all your steps broken down in a nice graph UI. But sometimes you realize certain steps really don't benefit from being broken down more, or in fact are more difficult to deal with when broken down further.", "No problem, happy Dagstering!"], "is_dagster_empl": [false, false, false, false, false, false, false, false, false, false, false, false, false, false], "gpt4_replies_target": "You could try using the in_process_executor with the InMemoryIOManager to keep the execution of all ops in a single process, or you could merge the two ops into one. However, be aware that using in_process_executor won't allow you to retry from a specific step if a failure occurs.", "dagster_empl_first_target": "I run into the same issue all the time in spark pipelines. If there isn't any need for parallelization you could try using the <https://docs.dagster.io/_apidocs/execution#dagster.in_process_executor|in_process_executor> with the InMemoryIOManager - this will keep the execution of all ops in the graph in a single process and make it easier to pass around large objects without dumping to disk. Note that you won't be able to retry from a specific step i.e. if  `transfer_to_elastic` fails you'll have to restart the whole graph, because the intermediate won't be saved anywhere.", "dagster_empl_last_target": "No problem, happy Dagstering!"}, {"question": "Can i get access to the metadata of an asset/job/op in a sensor? based on meta data i'd prefer to do different things, if possible.", "replies": ["it was possible by getting the asset definition and using `metadata_by_key`  for the static metadata, and a attribute on the event for the run-time metadata", "id like to be able to do this - how did you get the asset definition in the sensor?", "I'll have to look through our git history. gimme a sec", "`metadata = asset.metadata_by_key.get(asset_key)` and i got the asset via `asset = context.assets_defs_by_key[asset_key]` (remember to assert that these things are not None). And the asset key, for completion sake:\n```for (\n            asset_key,\n            materialization,\n        ) in context.latest_materialization_records_by_key().items():```", "wow great thanks! will give that a go"], "is_dagster_empl": [false, false, false, false, false], "gpt4_replies_target": "Yes, you can access the metadata of an asset/job/op in a sensor. You can get the asset definition and use `metadata_by_key` for the static metadata, and an attribute on the event for the run-time metadata. You can get the asset via `asset = context.assets_defs_by_key[asset_key]` and the asset key via `context.latest_materialization_records_by_key().items()`.", "dagster_empl_first_target": "it was possible by getting the asset definition and using `metadata_by_key`  for the static metadata, and a attribute on the event for the run-time metadata", "dagster_empl_last_target": "wow great thanks! will give that a go"}, {"question": "Hello, I've a question about a file processing pattern.\nSay I have 100+ files with names like &lt;sysname&gt;.&lt;domain&gt;.some.thing\nwhere there are a dozen possible &lt;sysname&gt;,\na dozen possible &lt;domain&gt;.\nIn the asset graph, I want to group assets together by &lt;domain&gt;, which, as you see, can be retrieved from the names of files.\nCurrently, I'm re-processing all files even if just one upstream file changes, through the same jupy-notebook.\n\nAm considering breaking things out, so that if just a file in &lt;domain1&gt; is updated (timestamp modified), I run the jupy-notebook to process domain1 files.\nIn the end, I can run another script/nb to combine the csv files generated by each of the above domains.\nIn my case, it's fine to rerun the whole thing, it only takes a few mins anyway.\nBut I want to use this opportunity to get a structured solution and showcase dagster.\n\nSuggestions, please?", "replies": ["You might want to look at static partitions; you could use domain as a partition.", "Hi Sean, I'm not sure I follow the example given here - <https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions#defining-a-job-with-static-partitions>\n\nAlthough it looks like what I'm after, I don't fully get the example, I'm afraid.", "Hi Kekron, thanks for your question-- Sean is right that partitions are a good solution here. If you create a job (or asset representing your CSV file) with an associated `PartitionsDefinition`, where each partition corresponds to one of your domains, then you can process only the files for a specific domain in each run of the job. I highly recommend reading the full \u201cPartitions assets and jobs\u201d concept page to get a sense of what is possible:\n\n<https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions>", "Thank you, I'll read up :+1:", "Hi, congrats on the 1.2 release.\nI see that Dynamically Partitioned Assets is available now, which seems like a good candidate for my requirement.", "However, reading the example is only irritating me.\nI don't understand at all what this is supposed to mean:\n\n\n\nWhat does this mean?\n`if not context.instance.has_dynamic_partition(images_partitions_def.name, img_filename)`\n\nAs someone new to the data orchestration world (no prior Airflow experience), at least I feel that there are way too many keywords to remember.", "Hi Kekron,\n\nLet me break that line down for you:\n\n\u2022 the `context` is the `SensorEvaluationContext`\n\u2022 it gives you access to the Dagster Instance via `instance`. The `instance` provides APIs for all persisted data in Dagster\n\u2022 there is an instance method called `has_dynamic_partition` on the instance. You can use this to check if a partition already exists for a particular partitions definition (in this case, `images_partitions_def`).\nThis feature is still under active development, so your feedback is appreciated.", "Hi Sean,\n\nThanks for writing back with a clarification. I understood this after looking at it for 5 minutes (it's entirely possible that I'm slow with code in general).\nWhat annoyed me was that a lot of the things refer back to some definition or the other (which is elsewhere); so jumping b/w things made it frustrating.\n\nSo, image_partition_defs defines a dynamic partitioning instance.", "But then what decides what the dynamic partitioning is based on?\nI see the answer is img_filename, but where is that defined/decided?", "Can it not be something as simple as a glob, hiding the if not.context.instance.has_dynamic_partition under the covers?\nDynamicPartitionsDefinition(name=\"some-grouping\", partition-by=\"&lt;method&gt;, &lt;param-for-method&gt;\")\n\nEx: DynamicPartitionsDefinition(name='systems', partition-by=(glob, 'filename-2023-05-*.csv'))", "BTW, apologies for the lack of proper code formatting. Paste, for some reason, isn't behaving well for me in slack.", "&gt; But then what decides what the dynamic partitioning is based on? I see the answer is img_filename, but where is that defined/decided?\n`img_filename` is just a local variable in the list comprehension that defines `new_images`-- `images_partitions_def` has no reference to it.\n\nIn fact the definition of `images_partitions_def` is dead simple:\n\n```images_partitions_def = DynamicPartitionsDefinition(name=\"images\")```\nBasically it\u2019s just a container for arbitrary string partition keys. In this case, we happen to be filling it with keys derived from the filenames found inside the `MY_DIRECTORY` directory, but `images_partitions_def` knows nothing about this-- it\u2019s more or less just a set of strings."], "is_dagster_empl": [false, false, true, false, false, false, true, false, false, false, false, true], "gpt4_replies_target": "To process files based on their domain, you can use Dagster's static or dynamic partitions. Create a job with an associated `PartitionsDefinition`, where each partition corresponds to one of your domains. This way, you can process only the files for a specific domain in each run of the job. For dynamic partitions, you can use the `has_dynamic_partition` method to check if a partition already exists for a particular partitions definition.", "dagster_empl_first_target": "Hi Kekron, thanks for your question-- Sean is right that partitions are a good solution here. If you create a job (or asset representing your CSV file) with an associated `PartitionsDefinition`, where each partition corresponds to one of your domains, then you can process only the files for a specific domain in each run of the job. I highly recommend reading the full \u201cPartitions assets and jobs\u201d concept page to get a sense of what is possible:\n\n<https://docs.dagster.io/concepts/partitions-schedules-sensors/partitions>", "dagster_empl_last_target": "&gt; But then what decides what the dynamic partitioning is based on? I see the answer is img_filename, but where is that defined/decided?\n`img_filename` is just a local variable in the list comprehension that defines `new_images`-- `images_partitions_def` has no reference to it.\n\nIn fact the definition of `images_partitions_def` is dead simple:\n\n```images_partitions_def = DynamicPartitionsDefinition(name=\"images\")```\nBasically it\u2019s just a container for arbitrary string partition keys. In this case, we happen to be filling it with keys derived from the filenames found inside the `MY_DIRECTORY` directory, but `images_partitions_def` knows nothing about this-- it\u2019s more or less just a set of strings."}, {"question": "Hello folks :wave:\nAs we switched our logging to JSON (using the `json_console_logger`) we started noticing weird behaviour in some of our pipelines. Sometimes pipelines fail with the following exception:\n```dagster._core.errors.DagsterSubprocessError: During celery execution errors occurred in workers:\n[fetch_data]: dagster._serdes.errors.DeserializationError: Output of deserialized json_str was not expected type of tuple. Received type &lt;class 'dict'&gt;.\n\nStack Trace:\nFile \"/usr/local/lib/python3.9/site-packages/dagster_celery/core_execution_loop.py\", line 84, in core_celery_execution_loop\nstep_events = result.get()\nFile \"/usr/local/lib/python3.9/site-packages/celery/result.py\", line 220, in get\nself.maybe_throw(callback=callback)\nFile \"/usr/local/lib/python3.9/site-packages/celery/result.py\", line 336, in maybe_throw\nself.throw(value, self._to_remote_traceback(tb))\nFile \"/usr/local/lib/python3.9/site-packages/celery/result.py\", line 329, in throw\nself.on_ready.throw(*args, **kwargs)\nFile \"/usr/local/lib/python3.9/site-packages/vine/promises.py\", line 234, in throw\nreraise(type(exc), exc, tb)\nFile \"/usr/local/lib/python3.9/site-packages/vine/utils.py\", line 30, in reraise\nraise value\n\n\n  File \"/usr/local/lib/python3.9/site-packages/dagster/_core/execution/api.py\", line 990, in pipeline_execution_iterator\n    for event in pipeline_context.executor.execute(pipeline_context, execution_plan):\n  File \"/usr/local/lib/python3.9/site-packages/dagster_celery/core_execution_loop.py\", line 164, in core_celery_execution_loop\n    raise DagsterSubprocessError(```\nIf I re-run the pipeline with the same configuration, sometimes the pipeline succeeds and sometimes fails again. Most of the runs complete coerrectly so it\u2019s not something easy to reproduce. It would just happen occasionally.\n\nAny idea?", "replies": ["Theres a fix for this going out in the release today!", "Ow that\u2019s cool! Any link to the commit / issue?", "Found it i think: <https://github.com/dagster-io/dagster/pull/13143>"], "is_dagster_empl": [true, false, false], "gpt4_replies_target": "The issue you're experiencing with your pipelines occasionally failing due to a deserialization error has been identified and a fix is being released today, you can find more details on this GitHub link: https://github.com/dagster-io/dagster/pull/13143.", "dagster_empl_first_target": "Theres a fix for this going out in the release today!", "dagster_empl_last_target": "Theres a fix for this going out in the release today!"}, {"question": "I'm experimenting w/ Dagster, and checking the terms. I'm trying to find in the terms of service, something about whether Dagster cloud will delete data if I decide to delete our org's account? This <https://dagster.io/security> talks about how the hybrid architecture means dagster doesn't actually see our data. But - if I'm using the cloud service, that wouldn't seem to apply.", "replies": ["Hi Nate - we\u2019ll delete all data that we have stored for you if you remove your account, in both hybrid and serverless.", "ok, thanks! is that in the policy somewhere, or is Cloud too new, and the policies on the web don't reflect that yet?", "I'll check about the copy on the website, it's possible it needs to be updated to include Serverless as well as Hybrid (Hybrid and Serverless are both considered \"Dagster Cloud\" but Serverless is indeed a newer option)", "ahh, got it. Yeah i definitely meant serverless. Thanks so much! Dagster seems awesome, and the serverless option has made it seem like a doable thing to try out, in a field where there's very little capacity for managing infrastructure. So thanks for making it!"], "is_dagster_empl": [true, false, true, false], "gpt4_replies_target": "If you delete your organization's account, Dagster will delete all data that they have stored for you, regardless of whether you're using the hybrid or serverless option.", "dagster_empl_first_target": "Hi Nate - we\u2019ll delete all data that we have stored for you if you remove your account, in both hybrid and serverless.", "dagster_empl_last_target": "I'll check about the copy on the website, it's possible it needs to be updated to include Serverless as well as Hybrid (Hybrid and Serverless are both considered \"Dagster Cloud\" but Serverless is indeed a newer option)"}, {"question": "hi all, is there a tool that you would use to copy files (between SFTP and S3) without parsing them?\nI'm looking for a stable, no-code way of copying over source files, including e.g. PDFs and images, before feeding them to pipelines", "replies": ["Hi tga, IIUC this isn\u2019t really a dagster question so we may be of limited help here, but this looks like it might help: <https://stackoverflow.com/questions/66316645/get-files-from-sftp-to-s3-aws>", "thanks for the hint, this is indeed only related to Dagster in an ecosystem kind of way"], "is_dagster_empl": [true, false], "gpt4_replies_target": "While this isn't directly related to Dagster, you might find this StackOverflow thread helpful for copying files between SFTP and S3 without parsing them: https://stackoverflow.com/questions/66316645/get-files-from-sftp-to-s3-aws.", "dagster_empl_first_target": "Hi tga, IIUC this isn\u2019t really a dagster question so we may be of limited help here, but this looks like it might help: <https://stackoverflow.com/questions/66316645/get-files-from-sftp-to-s3-aws>", "dagster_empl_last_target": "Hi tga, IIUC this isn\u2019t really a dagster question so we may be of limited help here, but this looks like it might help: <https://stackoverflow.com/questions/66316645/get-files-from-sftp-to-s3-aws>"}, {"question": "Hi Team, just a small question, is there any additional documentation or tutorials that show dagster resources in use?", "replies": ["Hi Ricardo, \u201cadditional\u201d relative to what?\n\nThis is a good starting point in our docs: <https://docs.dagster.io/concepts/resources>", "My bad, I meant additional relative to the documentation, went through it but I am quite new so some things in there eluded me, was hoping to know if there were example projects or some additional docs where I could see resources being implemented", "(went trhough taht one you shared and the one about pythonic resources)", "There are a bunch of examples in the `examples` directory of the OSS dagster repo, you might want to look at examples/project_fuly_featured <https://github.com/dagster-io/dagster/tree/master/examples/project_fully_featured>", "this works for me, thanks you sean!"], "is_dagster_empl": [true, false, false, true, false], "gpt4_replies_target": "Yes, there are additional resources available. You can find a variety of examples in the `examples` directory of the OSS dagster repo, specifically you might want to look at examples/project_fuly_featured.", "dagster_empl_first_target": "Hi Ricardo, \u201cadditional\u201d relative to what?\n\nThis is a good starting point in our docs: <https://docs.dagster.io/concepts/resources>", "dagster_empl_last_target": "There are a bunch of examples in the `examples` directory of the OSS dagster repo, you might want to look at examples/project_fuly_featured <https://github.com/dagster-io/dagster/tree/master/examples/project_fully_featured>"}, {"question": "What level of completion is required for contributions to a dagster plugin to be welcome? We have our own AWS SQS implementation now since it was missing from dagster_aws. If it has (tested of course) basic message fetching, deleting, resource, etc. but does not implement all of the API, is it ok? Am I responsible for developing it further then?", "replies": ["cc <@U01EK4V7BM4> as AWS guru", "It\u2019s hard to say without seeing the PR, but in general, any incremental improvement is welcome. No, it does not become your responsibility to extend and support further because you contributed once, but we certainly welcome additional contributions!", "Thanks Jordan then I will ping you on GH after we make the PR."], "is_dagster_empl": [true, true, false], "gpt4_replies_target": "Any incremental improvement to a dagster plugin is welcome, even if it doesn't implement all of the API. After contributing, you are not obligated to develop it further, but additional contributions are appreciated.", "dagster_empl_first_target": "cc <@U01EK4V7BM4> as AWS guru", "dagster_empl_last_target": "It\u2019s hard to say without seeing the PR, but in general, any incremental improvement is welcome. No, it does not become your responsibility to extend and support further because you contributed once, but we certainly welcome additional contributions!"}, {"question": "Hello!\nIn the v1.2.2 <https://docs.dagster.io/changelog#experimental|changelog>, I see an experimental feature that allows a `required_resource_keys` to be specified for sensors. From what I understood I should use `with_resources` in my repository. As follows:\n```@sensor(\n    job=my_job,\n    required_resource_keys={\"my_resource\"}\n)\ndef my_sensor(context):\n    context.my_resource...\n\n@repository\ndef my_repo():\n    return *with_resources(\n        definitions=[my_sensor],\n        resource_defs={'my_resource': my_resource}\n    )```\nWhen I do so I get this error :\n```'SensorDefinition' object has no attribute 'with_resources'```\nIs there anything I do wrong ?\nThank you by advance and have a nice day,\nLouis", "replies": ["Hmm I\u2019m guessing that new feature only works with the Definitions API: <https://docs.dagster.io/concepts/code-locations#defining-code-locations>\n\n`Definitions` does not require `with_resources`, the sensors will be able to bind any resources passed to the `Definitions` object.", "Oh thanks a lot !"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The error occurs because the new feature only works with the Definitions API and does not require `with_resources`. The sensors will be able to bind any resources passed to the `Definitions` object.", "dagster_empl_first_target": "Hmm I\u2019m guessing that new feature only works with the Definitions API: <https://docs.dagster.io/concepts/code-locations#defining-code-locations>\n\n`Definitions` does not require `with_resources`, the sensors will be able to bind any resources passed to the `Definitions` object.", "dagster_empl_last_target": "Hmm I\u2019m guessing that new feature only works with the Definitions API: <https://docs.dagster.io/concepts/code-locations#defining-code-locations>\n\n`Definitions` does not require `with_resources`, the sensors will be able to bind any resources passed to the `Definitions` object."}, {"question": "Anyone know if when you \"and\" together multiple asset selections in define_asset_job, whether it ensures that the assets are executed in the order specified (left to right)? I want to have a few assets materialized at the end of a bunch of others. But I don't want to create a huge list of \"AssetIn\" non-argument deps. Basically it's a database optimization asset.\n\nMaybe I'm approaching it the wrong way. I want to run some code after all the assets have been materialized in a job.", "replies": ["&gt; Anyone know if when you \u201cand\u201d together multiple asset selections in define_asset_job, whether it ensures that the assets are executed in the order specified (left to right)? \nIt does not-- an `AssetSelection` resolves to a (unordered) set.\n\nIt seems like a <https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors#run-status-sensors|Run status sensor> might be what you\u2019re looking for-- you can point one at your asset job to perform follow-up work if the job completes successfully.", "Aha! Thank you. :blush: "], "is_dagster_empl": [true, false], "gpt4_replies_target": "No, using \"and\" with multiple asset selections in define_asset_job does not ensure that the assets are executed in the specified order. Instead, you might want to use a Run status sensor to perform follow-up work if the job completes successfully.", "dagster_empl_first_target": "&gt; Anyone know if when you \u201cand\u201d together multiple asset selections in define_asset_job, whether it ensures that the assets are executed in the order specified (left to right)? \nIt does not-- an `AssetSelection` resolves to a (unordered) set.\n\nIt seems like a <https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors#run-status-sensors|Run status sensor> might be what you\u2019re looking for-- you can point one at your asset job to perform follow-up work if the job completes successfully.", "dagster_empl_last_target": "&gt; Anyone know if when you \u201cand\u201d together multiple asset selections in define_asset_job, whether it ensures that the assets are executed in the order specified (left to right)? \nIt does not-- an `AssetSelection` resolves to a (unordered) set.\n\nIt seems like a <https://docs.dagster.io/concepts/partitions-schedules-sensors/sensors#run-status-sensors|Run status sensor> might be what you\u2019re looking for-- you can point one at your asset job to perform follow-up work if the job completes successfully."}, {"question": "Hello, it is possible to change the status of a certain run to OK/Green? I'm looking for something like the \"Mark as succeed\" button that there is in Airflow.", "replies": ["Hi Ruben, We don\u2019t currently have this feature, and I don\u2019t think we would implement it since it clashes with how we model runs-- run records are immutable and re-execution generates a new run with a reference to the parent.", "Okey Sean, thanks for the clarification!"], "is_dagster_empl": [true, false], "gpt4_replies_target": "Dagster does not currently support changing the status of a run to OK/Green as run records are immutable and re-execution generates a new run with a reference to the parent.", "dagster_empl_first_target": "Hi Ruben, We don\u2019t currently have this feature, and I don\u2019t think we would implement it since it clashes with how we model runs-- run records are immutable and re-execution generates a new run with a reference to the parent.", "dagster_empl_last_target": "Hi Ruben, We don\u2019t currently have this feature, and I don\u2019t think we would implement it since it clashes with how we model runs-- run records are immutable and re-execution generates a new run with a reference to the parent."}, {"question": "with the default daemon logging we see things like \"[34mInfo [0m\".  -- any way to disable these ANSI codes so they don't appear?", "replies": ["Hey Robert, unfortunately this is not configurable at the moment.\n\nThere\u2019s a discussion I started on standardizing this sort of configuration to meet use cases such as yours: <https://github.com/dagster-io/dagster/discussions/12495>. Feel free to add your additional input to help with prioritization efforts."], "is_dagster_empl": [true], "gpt4_replies_target": "Currently, it's not possible to disable these ANSI codes in the default daemon logging, but there's an ongoing discussion on standardizing this configuration to meet such use cases.", "dagster_empl_first_target": "Hey Robert, unfortunately this is not configurable at the moment.\n\nThere\u2019s a discussion I started on standardizing this sort of configuration to meet use cases such as yours: <https://github.com/dagster-io/dagster/discussions/12495>. Feel free to add your additional input to help with prioritization efforts.", "dagster_empl_last_target": "Hey Robert, unfortunately this is not configurable at the moment.\n\nThere\u2019s a discussion I started on standardizing this sort of configuration to meet use cases such as yours: <https://github.com/dagster-io/dagster/discussions/12495>. Feel free to add your additional input to help with prioritization efforts."}, {"question": "Hi! It's been awhile. I'm looking at using DynamicPartitions for an upstream asset. A downstream asset would consume all the partitions of the upstream asset. Will it do this by default or do I need a PartitionMapping? Thanks!", "replies": ["A related question...Can I have another upstream asset that creates the DynamicPartitions or do I need to use a sensor like in the docs?", "Hi Yang. By default, if an unpartitioned asset is downstream of a partitioned asset, the unpartitioned asset consumes all partitions of the upstream asset.\n\nIf your downstream asset is partitioned, you'll have to specify a partition mapping", "you could add dynamic partitions in another asset e.g.:\n```@asset\ndef adds_partitions(context):\n    context.instance.add_dynamic_partitions(partitions_def_name, partition_keys)\n    ....```\nOne caveat here is that unexpected behaviors may occur if you mutate the dynamic partitions def during run for one of its partitions: i.e. if you added partitions in an upstream dynamic-partitioned asset, and attempted to read all partitions in in a downstream unpartitioned asset in the same run. So I would recommend *not* mutating the partitions def in one of its runs.", "oh great! yes the downstream asset will be unpartitioned.\nBut how exactly will each partition be referenced? I saw somewhere it would be like a dictionary? `upstream_asset['part1']`", "is that right?", "depends on what your IO manager is.\n\nThe default `fs_io_manager` stores outputs per-partition, so returns a `Dict[str, obj]` which maps partition key to the output object.\n\nBut if you had something like the snowflake IO manager which can load ranges of partitions, you would just get back a single object", "oohh I see, I'm using the gcs_pickle_io_manager", "Can Dynamic Partitions be Multipartitions?", "Looks like a no, but I just want to confirm, and I'm wondering if it will be supported in the future. Thanks!", "I just saw in the release notes of 1.2.3 that \"dynamic partitions can now exist as dimensions of multi-partition definitions\" (though that may be slightly different than what you are looking for?)", "Oh??? I think that should be what I'm looking for, thanks!", "Yes I see it, that's awesome. In that case, <@U028M11QNDD>, how would I have the downstream asset take all partitions of the one of the dimensions? What I mean is that the Multipartition would have 1 dimension that is Dynamic, say `name`. And the other one is `year`. Then the downstream asset would be using the partition `year` and combining all the `names`. Does that work automatically, and would the input asset also look like a dict with the `names` as the keys? Thanks!", "Where did you see that <@U04TX6WLT99>? I coulnd't find that in tha changelog.", "By default, if a single dimensioned asset is upstream/downstream of a multipartitioned asset and the single dimension partitions def is a dimension of the multipartitions def, Dagster auto-infers a dependency relationship.", "In terms of what the IO manager returns, it depends on the IO manager. For the default `fs_io_manager`, you would get a dict containing the partition key path to the object. Currently the path orders each dimension in a different folder, if you wanted to get back the multipartition key to the object that would look like this:\n```@asset(partitions_def=DailyPartitionsDefinition(\"2023-01-01\"))\ndef downstream_of_multipartitions_asset(upstream):\n    def _get_key_from_multipartition_path(path: str) -&gt; MultiPartitionKey:\n        dimension_names = sorted(multipartitions_def.partition_dimension_names)\n        keys_ordered_by_dimension = path.split(\"/\")\n        return MultiPartitionKey(\n            {tup[0]: tup[1] for tup in list(zip(dimension_names, keys_ordered_by_dimension))}\n        )\n\n    object_by_key = {_get_key_from_multipartition_path(path): object for path, object in upstream.items()}\n    ...```\n", "Not super ergonomic at the moment, I'd recommend filing an issue to instead return a mapping of multipartition key -&gt; object instead of the filepath dict", "Thanks, that's really helpful! Do you happen to know what it is for `gcs_pickle_io_manager`?", "<@U04U4FF9B8S> <https://github.com/dagster-io/dagster/blob/master/CHANGES.md#new-1>\n\u2022 (experimental) Dynamic partitions definitions can now exist as dimensions of multi-partitions definitions.\n", "Hi <@U028M11QNDD>, sorry I have another question. I'm looking at adding dynamic partitions in an asset instead of sensor. I'm wondering if this still applies for the asset context\n``` context.instance.add_dynamic_partitions```", "yes it does, its the same method on the instance", "How do I get the existing list of partitions? I was trying to do this but it didn't work.\n```    # existing_partitions = sfdr_metric_partitions_def.get_partitions()```", "I believe you can do:\n```context.instance.get_dynamic_partitions(sfdr_metric_partitions_def.name)```", "oh thank you!", "yes, verified", "Hey Yang, reviving this thread as we're thinking about API improvements for adding dynamic partitions within assets.\n\nWhat's your use case for doing something like this? Are there any assets downstream of your asset that adds the dynamic partitions?", "Hi! Oh great. No I can have a separate job that adds the dynamic partitions. Then another job that will materialize them.\nThe use case is I have a configurable scoring system where there are metrics that are configured in a yaml file. Each different metric is a DynamicPartition. Then a downstream asset loads all the metric partitions and formats an output.", "These should be multipartitions where the other partition with the metric is Year", "Ideally I'd like to have partitions only materialize if they're missing in that job.", "Ah ok, thanks! this makes sense to me"], "is_dagster_empl": [false, true, true, false, false, true, false, false, false, false, false, false, false, true, true, true, false, false, false, true, false, false, false, false, true, false, false, false, true], "gpt4_replies_target": "By default, an unpartitioned downstream asset will consume all partitions of an upstream partitioned asset. If the downstream asset is partitioned, a partition mapping is required. The way each partition is referenced depends on the IO manager being used. For the default `fs_io_manager`, outputs are stored per-partition and returned as a dictionary mapping the partition key to the output object. Dynamic partitions can exist as dimensions of multi-partition definitions as of release 1.2.3.", "dagster_empl_first_target": "Hi Yang. By default, if an unpartitioned asset is downstream of a partitioned asset, the unpartitioned asset consumes all partitions of the upstream asset.\n\nIf your downstream asset is partitioned, you'll have to specify a partition mapping", "dagster_empl_last_target": "Ah ok, thanks! this makes sense to me"}, {"question": "With PartitionMapping's can I just get some clarification on what 'Overriding PartitionMapping outside of Dagster is not supported' means?\n\ndoes this mean, please don't implement your own partitionmapping class?", "replies": ["Hi Harrison. Partition mapping method signatures can change depending on internal callsites, so if you did implement your own partition mapping, it's possible that upgrading can lead to unexpected breakages if we changed the method signatures.\n\nIf you don't mind this happening, you can implement your own partition mapping class. Another option is also opening up a PR to add your partition mapping to the dagster library, and we'll maintain it from there.", "thanks. I'm use to my code breaking all the time, so that's not the biggest of issues :slightly_smiling_face:"], "is_dagster_empl": [true, false], "gpt4_replies_target": "The statement 'Overriding PartitionMapping outside of Dagster is not supported' means that while you can implement your own partition mapping class, doing so may lead to unexpected breakages if method signatures change during upgrades, as these are dependent on internal callsites.", "dagster_empl_first_target": "Hi Harrison. Partition mapping method signatures can change depending on internal callsites, so if you did implement your own partition mapping, it's possible that upgrading can lead to unexpected breakages if we changed the method signatures.\n\nIf you don't mind this happening, you can implement your own partition mapping class. Another option is also opening up a PR to add your partition mapping to the dagster library, and we'll maintain it from there.", "dagster_empl_last_target": "Hi Harrison. Partition mapping method signatures can change depending on internal callsites, so if you did implement your own partition mapping, it's possible that upgrading can lead to unexpected breakages if we changed the method signatures.\n\nIf you don't mind this happening, you can implement your own partition mapping class. Another option is also opening up a PR to add your partition mapping to the dagster library, and we'll maintain it from there."}]